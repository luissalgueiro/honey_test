{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c64e8423-696e-40f8-af48-051a8472a5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pytorch_lightning as pl\n",
    "import albumentations\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from torchmetrics import F1\n",
    "from pytorch_toolbelt import losses as L\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3da888e3-d6c7-4355-b766-14eed6f0e693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Eucalyptus sp.', 'Brassica', 'Taraxacum', 'Rosmarinus officinalis', 'Tilia', 'Erica.m', 'Lavandula', 'Cistus sp', 'Pinus', 'Cardus', 'Citrus sp', 'Helianthus annuus']\n"
     ]
    }
   ],
   "source": [
    "root_path = '/mnt/gpid08/datasets/remote_sensing/tmp_from_gpid07/honey/datasets/'\n",
    "print(os.listdir(root_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7763f599-8e7c-490e-835a-8a7066fac749",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_folders=os.listdir(root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ac02025-1995-4c23-8a87-ff529144adbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total amount of files: 8970\n"
     ]
    }
   ],
   "source": [
    "list_files = []\n",
    "for (dirpath, dirnames, filenames) in os.walk(root_path):\n",
    "        list_files += [os.path.join(dirpath, file) for file in filenames]\n",
    "print(f'Total amount of files: {len(list_files)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5aecc888-c3e9-43e3-8d6f-3ac25a810b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Eucalyptus sp.</th>\n",
       "      <th>Brassica</th>\n",
       "      <th>Taraxacum</th>\n",
       "      <th>Rosmarinus officinalis</th>\n",
       "      <th>Tilia</th>\n",
       "      <th>Erica.m</th>\n",
       "      <th>Lavandula</th>\n",
       "      <th>Cistus sp</th>\n",
       "      <th>Pinus</th>\n",
       "      <th>Cardus</th>\n",
       "      <th>Citrus sp</th>\n",
       "      <th>Helianthus annuus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eucalyptus sp.-0-Sample_3_Capture_128.bmp</td>\n",
       "      <td>Brassica-0-Sample_61_Olea_Brassica_Salix_0_28....</td>\n",
       "      <td>Taraxacum-0-Sample_72_Arbutus_Erica_0911211048...</td>\n",
       "      <td>Rosmarinus officinalis-0-Sample_19_Rosmarinus_...</td>\n",
       "      <td>Tilia-0-Sample_49_Tilia_Flor_3105211737_22.588...</td>\n",
       "      <td>Erica.m-4-Sample_72_Arbutus_Erica_0311211548_0...</td>\n",
       "      <td>Lavandula-4-Sample_71_Lavanda_Espliego_1810211...</td>\n",
       "      <td>Cistus sp-4-Sample_62_Thymus_0_19.666667_0.000...</td>\n",
       "      <td>Pinus-1-Sample_77_Pinus_Poaceae_2112211334_0_-...</td>\n",
       "      <td>Cardus-0-Sample_61_Olea_Brassica_Salix_0_22.73...</td>\n",
       "      <td>Citrus sp-0-Sample_67_Citrus_2909211453_0_9.00...</td>\n",
       "      <td>Helianthus annuus-1-Sample_17_sunflower_070921...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Eucalyptus sp.-0-Sample_66_Echium_Lotus_Castan...</td>\n",
       "      <td>Brassica-0-Sample_21_Rapseed_1105211748_3.5294...</td>\n",
       "      <td>Taraxacum-0-Sample_46_Taraxacum_3003211226_15....</td>\n",
       "      <td>Rosmarinus officinalis-0-Sample_19_Rosmarinus_...</td>\n",
       "      <td>Tilia-0-Sample_47_Tilia_0706211343_0_21.205_2....</td>\n",
       "      <td>Vaccinium_m-0-Sample_64_Erica.t_Calluna_010921...</td>\n",
       "      <td>Lavandula-0-Sample_70_Cantueso_2010210942_0_30...</td>\n",
       "      <td>Cistus sp-1-Sample_61_Olea_Brassica_Salix_0_26...</td>\n",
       "      <td>Pinus-1-Sample_77_Pinus_Poaceae_2112211254_0_3...</td>\n",
       "      <td>Cardus-1-Sample_67_Citrus_2909211030_0_41.276_...</td>\n",
       "      <td>Citrus sp-0-Citrus_no_fucsin_34.632_-0.556_0.8...</td>\n",
       "      <td>Helianthus annuus-1-Sample_17_sunflower_0_19.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Eucalyptus sp.-1-Sample_23_Euc_Ech_Coro_8.679_...</td>\n",
       "      <td>Brassica-1-Sample_21_Rapseed_27.596_9.269_0.78...</td>\n",
       "      <td>Taraxacum-1-Sample_67_Citrus_0410211143_0_18.6...</td>\n",
       "      <td>Rosmarinus officinalis-0-Sample_19_Rosmarinus_...</td>\n",
       "      <td>Tilia-0-Sample_47_Tilia_0206211356_0_18.583_-1...</td>\n",
       "      <td>Vaccinium_m-3-Sample_63_Rosmarinus_Prunus_0_23...</td>\n",
       "      <td>Lavandula-0-Sample_70_Cantueso_1410211700_0_5....</td>\n",
       "      <td>Cistus sp-1-Sample_72_Arbutus_Erica_2311211327...</td>\n",
       "      <td>Pinus-2-Sample_77_Pinus_Poaceae_2112211301_0_1...</td>\n",
       "      <td>Cardus-0-Sample_68_Persea_0410210944_0_43.1085...</td>\n",
       "      <td>Citrus sp-1-Sample_67_Citrus_2809211605_0_14.1...</td>\n",
       "      <td>Helianthus annuus-0-Sample_22_imported_Ana_13....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Eucalyptus sp.-3-Sample_3_Capture_168.bmp</td>\n",
       "      <td>Brassica-1-Sample_21_Rapseed_6.537_-2.756_0.71...</td>\n",
       "      <td>Taraxacum-0-Sample_67_Citrus_0410211004_0_23.4...</td>\n",
       "      <td>Rosmarinus officinalis-0-Sample_75_Liliaceae_P...</td>\n",
       "      <td>Tilia-1-Sample_47_Tilia_0706211538_0_13.339_6....</td>\n",
       "      <td>Vaccinium_m-1-Sample_58_Vaccinium_myrtillus_29...</td>\n",
       "      <td>Lavandula-1-Sample_71_Lavanda_Espliego_2110211...</td>\n",
       "      <td>Cistus sp-1-Sample_67_Citrus_0410211017_0_19.4...</td>\n",
       "      <td>Pinus-0-Sample_32_PINUS_2603211549_21.529412_6...</td>\n",
       "      <td>Cardus-0-Sample_68_Persea_0410210943_0_37.5421...</td>\n",
       "      <td>Citrus sp-0-Sample_67_Citrus_0410211045_0_22.2...</td>\n",
       "      <td>Helianthus annuus-1-Sample_22_imported_Ana_050...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Eucalyptus sp.-2-Sample_61_Olea_Brassica_Salix...</td>\n",
       "      <td>Brassica-6-Sample_21_Rapseed_1105211320_1.4117...</td>\n",
       "      <td>Taraxacum-0-Sample_67_Citrus_0410211136_0_22.8...</td>\n",
       "      <td>Rosmarinus officinalis-0-Sample_19_Rosmarinus_...</td>\n",
       "      <td>Tilia-3-Sample_53_Tilia_honey_3105211737_22.94...</td>\n",
       "      <td>Vaccinium_m-0-Sample_64_Erica.t_Calluna_020921...</td>\n",
       "      <td>Lavandula-3-Sample_71_Lavanda_Espliego_2110210...</td>\n",
       "      <td>Cistus sp-1-Sample_62_Thymus_0709211156_0_12.3...</td>\n",
       "      <td>Pinus-1-Sample_17_sunflower_0709211301_0_2.192...</td>\n",
       "      <td>Cardus-1-Sample_17_sunflower_0709211348_0_30.2...</td>\n",
       "      <td>Citrus sp-1-Sample_61_Olea_Brassica_Salix_2007...</td>\n",
       "      <td>Helianthus annuus-1-Sample_44_Cardus_0_20.8257...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Eucalyptus sp.  \\\n",
       "0          Eucalyptus sp.-0-Sample_3_Capture_128.bmp   \n",
       "1  Eucalyptus sp.-0-Sample_66_Echium_Lotus_Castan...   \n",
       "2  Eucalyptus sp.-1-Sample_23_Euc_Ech_Coro_8.679_...   \n",
       "3          Eucalyptus sp.-3-Sample_3_Capture_168.bmp   \n",
       "4  Eucalyptus sp.-2-Sample_61_Olea_Brassica_Salix...   \n",
       "\n",
       "                                            Brassica  \\\n",
       "0  Brassica-0-Sample_61_Olea_Brassica_Salix_0_28....   \n",
       "1  Brassica-0-Sample_21_Rapseed_1105211748_3.5294...   \n",
       "2  Brassica-1-Sample_21_Rapseed_27.596_9.269_0.78...   \n",
       "3  Brassica-1-Sample_21_Rapseed_6.537_-2.756_0.71...   \n",
       "4  Brassica-6-Sample_21_Rapseed_1105211320_1.4117...   \n",
       "\n",
       "                                           Taraxacum  \\\n",
       "0  Taraxacum-0-Sample_72_Arbutus_Erica_0911211048...   \n",
       "1  Taraxacum-0-Sample_46_Taraxacum_3003211226_15....   \n",
       "2  Taraxacum-1-Sample_67_Citrus_0410211143_0_18.6...   \n",
       "3  Taraxacum-0-Sample_67_Citrus_0410211004_0_23.4...   \n",
       "4  Taraxacum-0-Sample_67_Citrus_0410211136_0_22.8...   \n",
       "\n",
       "                              Rosmarinus officinalis  \\\n",
       "0  Rosmarinus officinalis-0-Sample_19_Rosmarinus_...   \n",
       "1  Rosmarinus officinalis-0-Sample_19_Rosmarinus_...   \n",
       "2  Rosmarinus officinalis-0-Sample_19_Rosmarinus_...   \n",
       "3  Rosmarinus officinalis-0-Sample_75_Liliaceae_P...   \n",
       "4  Rosmarinus officinalis-0-Sample_19_Rosmarinus_...   \n",
       "\n",
       "                                               Tilia  \\\n",
       "0  Tilia-0-Sample_49_Tilia_Flor_3105211737_22.588...   \n",
       "1  Tilia-0-Sample_47_Tilia_0706211343_0_21.205_2....   \n",
       "2  Tilia-0-Sample_47_Tilia_0206211356_0_18.583_-1...   \n",
       "3  Tilia-1-Sample_47_Tilia_0706211538_0_13.339_6....   \n",
       "4  Tilia-3-Sample_53_Tilia_honey_3105211737_22.94...   \n",
       "\n",
       "                                             Erica.m  \\\n",
       "0  Erica.m-4-Sample_72_Arbutus_Erica_0311211548_0...   \n",
       "1  Vaccinium_m-0-Sample_64_Erica.t_Calluna_010921...   \n",
       "2  Vaccinium_m-3-Sample_63_Rosmarinus_Prunus_0_23...   \n",
       "3  Vaccinium_m-1-Sample_58_Vaccinium_myrtillus_29...   \n",
       "4  Vaccinium_m-0-Sample_64_Erica.t_Calluna_020921...   \n",
       "\n",
       "                                           Lavandula  \\\n",
       "0  Lavandula-4-Sample_71_Lavanda_Espliego_1810211...   \n",
       "1  Lavandula-0-Sample_70_Cantueso_2010210942_0_30...   \n",
       "2  Lavandula-0-Sample_70_Cantueso_1410211700_0_5....   \n",
       "3  Lavandula-1-Sample_71_Lavanda_Espliego_2110211...   \n",
       "4  Lavandula-3-Sample_71_Lavanda_Espliego_2110210...   \n",
       "\n",
       "                                           Cistus sp  \\\n",
       "0  Cistus sp-4-Sample_62_Thymus_0_19.666667_0.000...   \n",
       "1  Cistus sp-1-Sample_61_Olea_Brassica_Salix_0_26...   \n",
       "2  Cistus sp-1-Sample_72_Arbutus_Erica_2311211327...   \n",
       "3  Cistus sp-1-Sample_67_Citrus_0410211017_0_19.4...   \n",
       "4  Cistus sp-1-Sample_62_Thymus_0709211156_0_12.3...   \n",
       "\n",
       "                                               Pinus  \\\n",
       "0  Pinus-1-Sample_77_Pinus_Poaceae_2112211334_0_-...   \n",
       "1  Pinus-1-Sample_77_Pinus_Poaceae_2112211254_0_3...   \n",
       "2  Pinus-2-Sample_77_Pinus_Poaceae_2112211301_0_1...   \n",
       "3  Pinus-0-Sample_32_PINUS_2603211549_21.529412_6...   \n",
       "4  Pinus-1-Sample_17_sunflower_0709211301_0_2.192...   \n",
       "\n",
       "                                              Cardus  \\\n",
       "0  Cardus-0-Sample_61_Olea_Brassica_Salix_0_22.73...   \n",
       "1  Cardus-1-Sample_67_Citrus_2909211030_0_41.276_...   \n",
       "2  Cardus-0-Sample_68_Persea_0410210944_0_43.1085...   \n",
       "3  Cardus-0-Sample_68_Persea_0410210943_0_37.5421...   \n",
       "4  Cardus-1-Sample_17_sunflower_0709211348_0_30.2...   \n",
       "\n",
       "                                           Citrus sp  \\\n",
       "0  Citrus sp-0-Sample_67_Citrus_2909211453_0_9.00...   \n",
       "1  Citrus sp-0-Citrus_no_fucsin_34.632_-0.556_0.8...   \n",
       "2  Citrus sp-1-Sample_67_Citrus_2809211605_0_14.1...   \n",
       "3  Citrus sp-0-Sample_67_Citrus_0410211045_0_22.2...   \n",
       "4  Citrus sp-1-Sample_61_Olea_Brassica_Salix_2007...   \n",
       "\n",
       "                                   Helianthus annuus  \n",
       "0  Helianthus annuus-1-Sample_17_sunflower_070921...  \n",
       "1  Helianthus annuus-1-Sample_17_sunflower_0_19.5...  \n",
       "2  Helianthus annuus-0-Sample_22_imported_Ana_13....  \n",
       "3  Helianthus annuus-1-Sample_22_imported_Ana_050...  \n",
       "4  Helianthus annuus-1-Sample_44_Cardus_0_20.8257...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "for folder in list_folders:\n",
    "    df_f1 = pd.DataFrame({folder: os.listdir(root_path+folder) })\n",
    "    df = pd.concat([df, df_f1], ignore_index=False, axis=1 )\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a9bbd69-760d-4891-a01e-53fb3262b72e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "aa = len(df) - df.isna().sum(axis=0)\n",
    "plt.bar(aa.index, aa, color='blue')\n",
    "plt.xlabel('Classes')\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('Quantity')\n",
    "plt.title('Imagens per class')\n",
    "\n",
    "for index, value in enumerate(aa):\n",
    "    plt.text(index-0.05, value+10, str(int(value)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a785d94-b703-43fa-8ae9-5c853f8e763d",
   "metadata": {},
   "source": [
    "## Partition of data in train-val-test subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86c72099-87ea-4e41-8dad-9333e286c216",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_files = []\n",
    "labels=[]\n",
    "for folder in list_folders:\n",
    "    list_per_folder = os.listdir(os.path.join(root_path, folder))\n",
    "    for file in list_per_folder:\n",
    "        list_files += [os.path.join(root_path, folder, file) ]\n",
    "        labels += [folder]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "851a59cf-fba0-427f-8d1e-b7cb62313a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/mnt/gpid08/datasets/remote_sensing/tmp_from_g...</td>\n",
       "      <td>Eucalyptus sp.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/mnt/gpid08/datasets/remote_sensing/tmp_from_g...</td>\n",
       "      <td>Eucalyptus sp.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/mnt/gpid08/datasets/remote_sensing/tmp_from_g...</td>\n",
       "      <td>Eucalyptus sp.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/mnt/gpid08/datasets/remote_sensing/tmp_from_g...</td>\n",
       "      <td>Eucalyptus sp.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/mnt/gpid08/datasets/remote_sensing/tmp_from_g...</td>\n",
       "      <td>Eucalyptus sp.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name          labels\n",
       "0  /mnt/gpid08/datasets/remote_sensing/tmp_from_g...  Eucalyptus sp.\n",
       "1  /mnt/gpid08/datasets/remote_sensing/tmp_from_g...  Eucalyptus sp.\n",
       "2  /mnt/gpid08/datasets/remote_sensing/tmp_from_g...  Eucalyptus sp.\n",
       "3  /mnt/gpid08/datasets/remote_sensing/tmp_from_g...  Eucalyptus sp.\n",
       "4  /mnt/gpid08/datasets/remote_sensing/tmp_from_g...  Eucalyptus sp."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\"name\":list_files,\"labels\":labels})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e218a064-fc34-4480-bb2b-757a7922fe0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8970 7669 897 404\n"
     ]
    }
   ],
   "source": [
    "## Train Test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(list_files, labels, test_size=0.1, random_state=100, stratify=labels)\n",
    "\n",
    "## Train val split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.05, random_state=100, stratify=y_train)\n",
    "\n",
    "df_train = pd.DataFrame({\"name\":x_train,\"labels\":y_train})\n",
    "df_test  = pd.DataFrame({\"name\":x_test,\"labels\":y_test})\n",
    "df_val  = pd.DataFrame({\"name\":x_val,\"labels\":y_val})\n",
    "\n",
    "\n",
    "print(len(df), len(df_train), len(df_test), len(df_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8c5033-1c8c-4cc5-9337-0c42f29d1d22",
   "metadata": {},
   "source": [
    "## Faltan histogramas!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9253ca4f-e4fe-43c6-a039-e7c3967202a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Brassica</th>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cardus</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cistus sp</th>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Citrus sp</th>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Erica.m</th>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eucalyptus sp.</th>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Helianthus annuus</th>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lavandula</th>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pinus</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rosmarinus officinalis</th>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Taraxacum</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tilia</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        name\n",
       "labels                      \n",
       "Brassica                  63\n",
       "Cardus                     9\n",
       "Cistus sp                 31\n",
       "Citrus sp                 24\n",
       "Erica.m                   83\n",
       "Eucalyptus sp.            42\n",
       "Helianthus annuus         41\n",
       "Lavandula                 34\n",
       "Pinus                     13\n",
       "Rosmarinus officinalis    23\n",
       "Taraxacum                 11\n",
       "Tilia                     30"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.groupby('labels').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248cd1e3-1f73-4637-bd9b-1f9c27e2dcc4",
   "metadata": {},
   "source": [
    "# Baseline using timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8811130c-4c6d-47bf-99cb-e47b3ea091a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc95a892-f826-43dd-be2a-c680c50b1710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://juansensio.com/blog/062_multihead_attention\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, mode, df):\n",
    "        self.mode = mode\n",
    "        self.df = df #pd.read_csv(\"/content/drive/MyDrive/melanoma/compartido/isic/isic_2017_train.csv\")\n",
    "\n",
    "        self.mean_img = (0.485, 0.456, 0.406 )\n",
    "        self.std_img = (0.229, 0.224, 0.225)\n",
    "        self.classes = ['Pinus','Erica.m', 'Cistus sp', 'Lavandula', 'Citrus sp', 'Helianthus annuus',\n",
    "                        'Eucalyptus sp.', 'Rosmarinus officinalis', 'Brassica', 'Cardus', 'Tilia', 'Taraxacum']\n",
    "\n",
    "    def __crop_padding(self,img):\n",
    "        ## convert to gray\n",
    "        img_gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        ## set threshold for 0\n",
    "        _,thresh = cv2.threshold(img_gray,10,255,cv2.THRESH_BINARY)\n",
    "        ## find contours\n",
    "        contours, hierarchy = cv2.findContours(thresh,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cnt = contours[0]\n",
    "        x,y,w,h = cv2.boundingRect(cnt)\n",
    "        crop = img[y:y+h,x:x+w,:]\n",
    "        return crop\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        name_img = self.df['name'].iloc[index]\n",
    "        label    = self.df['labels'].iloc[index]\n",
    "        ## READ IMAGE\n",
    "        image = plt.imread(name_img)\n",
    "        image = self.__crop_padding(image)\n",
    "        target = torch.tensor(self.classes.index(label))\n",
    "        # print(f'Image shape: {image.shape} \\t Target:{target}')\n",
    "        if self.mode=='train':\n",
    "            train_augm = albumentations.Compose(\n",
    "              [\n",
    "               albumentations.Resize(height=320,width=320),\n",
    "               albumentations.Normalize(self.mean_img, self.std_img, max_pixel_value=255.0, always_apply=True),\n",
    "              #  albumentations.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15),\n",
    "              #  albumentations.Flip(p=0.5)\n",
    "              ]\n",
    "            )\n",
    "            transformed = train_augm(image=image)\n",
    "            image=transformed['image']\n",
    "        else:\n",
    "            valid_augm = albumentations.Compose(\n",
    "              [\n",
    "               albumentations.Resize(height=320,width=320),\n",
    "               albumentations.Normalize(self.mean_img, self.std_img, max_pixel_value=255.0, always_apply=True)\n",
    "              ]\n",
    "            )\n",
    "            transformed = valid_augm(image=image)\n",
    "            image=transformed['image']\n",
    "\n",
    "        image = torch.from_numpy(image.transpose()).float()\n",
    "        target_oh = torch.nn.functional.one_hot(target, num_classes=12).float()\n",
    "        data = {\"image\":image,\n",
    "                \"target_oh\":target_oh,\n",
    "                'target':target,\n",
    "                'class_name':label } \n",
    "        # print(f'Image shape: {image.shape} \\t Target:{target}')\n",
    "\n",
    "        return data\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "class HoneyDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size: int = 4, Dataset = Dataset):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.Dataset = Dataset\n",
    "        self.train_ds =  self.Dataset(mode='train',df= df_train)\n",
    "        self.val_ds   =  self.Dataset(mode='val', df= df_val)\n",
    "        self.test_ds   =  self.Dataset(mode='test', df= df_test)\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_ds,\n",
    "                          batch_size=32,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0,\n",
    "                          pin_memory=True,\n",
    "                          drop_last=True,\n",
    "                          # sampler=sampler\n",
    "                          )\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_ds, batch_size=8, shuffle=False, num_workers=0, pin_memory=True, drop_last=True )\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_ds, batch_size=1, shuffle=False, num_workers=0, pin_memory=True, drop_last=False )\n",
    "    \n",
    "dm = HoneyDataModule(Dataset=Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf5c4f06-27da-4b2d-9137-456c5cc5903c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 320, 320]) torch.Size([32, 12])\n",
      "tensor([ 2,  2,  0,  6,  3,  2,  1,  9,  6, 10,  6,  6, 11,  3,  1,  3,  6, 10,\n",
      "         5,  8,  6, 11,  1,  8,  3,  5,  3,  8,  1,  3,  1,  1])\n",
      "['Cistus sp', 'Cistus sp', 'Pinus', 'Eucalyptus sp.', 'Lavandula', 'Cistus sp', 'Erica.m', 'Cardus', 'Eucalyptus sp.', 'Tilia', 'Eucalyptus sp.', 'Eucalyptus sp.', 'Taraxacum', 'Lavandula', 'Erica.m', 'Lavandula', 'Eucalyptus sp.', 'Tilia', 'Helianthus annuus', 'Brassica', 'Eucalyptus sp.', 'Taraxacum', 'Erica.m', 'Brassica', 'Lavandula', 'Helianthus annuus', 'Lavandula', 'Brassica', 'Erica.m', 'Lavandula', 'Erica.m', 'Erica.m']\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(dm.train_dataloader()))\n",
    "print(data['image'].shape, data['target_oh'].shape)\n",
    "print(data['target'])\n",
    "print(data['class_name'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4987537-4408-4c2a-bed5-d13e05a74b9f",
   "metadata": {},
   "source": [
    "## ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "586ac34a-b432-4483-92b7-5461926332b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model = timm.create_model('resnet50',pretrained='True',num_classes=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b504d75-fa96-4f97-99a8-97229365caa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from torch.optim.lr_scheduler import StepLR, MultiStepLR, CosineAnnealingWarmRestarts, ReduceLROnPlateau\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint \n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2fa47bc-8a16-4375-a038-7a641b946b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_epoch_loss, val_epoch_acc = [], []\n",
    "train_epoch_loss, train_epoch_acc = [], []\n",
    "\n",
    "class LitModel(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__( )\n",
    "        # self.save_hyperparameters()\n",
    "        self.model = model\n",
    "        self.f1score = F1(num_classes=12, average='weighted')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            y_hat = self(x)\n",
    "            return torch.argmax(y_hat, axis=1)\n",
    "    def compute_loss_and_metrics(self, batch):\n",
    "        x, y = batch['image'], batch['target']\n",
    "        # print(f'X: {x.shape} \\t Y: {y.shape}')\n",
    "        y_hat = self(x)\n",
    "        # print(f'Output: {y_hat.shape}')\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        # acc = (torch.argmax(y_hat, axis=1) == y).sum().item() / y.shape[0]\n",
    "        \n",
    "        # y1 = y.detach().cpu().numpy()\n",
    "        # # print(y1.shape)\n",
    "        # y_hat1 = torch.argmax(y_hat, axis=1)\n",
    "        # y_hat1 = y_hat1.detach().cpu().numpy()\n",
    "        # # print(y_hat1.shape)\n",
    "        # f1w = f1_score(y1, y_hat1, average='weighted')\n",
    "        f1w = self.f1score(torch.argmax(y_hat, axis=1), y)\n",
    "        \n",
    "        return loss, f1w\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, f1w = self.compute_loss_and_metrics(batch)\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_F1w', f1w, prog_bar=True)\n",
    "        #print(f'Training_step: loss> {loss} acc:{acc}')\n",
    "        return {'loss':loss,'f1w':torch.tensor(f1w)}\n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_train_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        avg_train_f1w  = torch.stack([x['f1w'] for x in outputs]).mean()\n",
    "        train_epoch_loss.append(avg_train_loss.item())\n",
    "        train_epoch_acc.append(avg_train_f1w.item())\n",
    "        #print(f'Epoch {self.current_epoch} TrainLOSS:{avg_train_loss} TrainACC:{avg_train_acc}  ')\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, f1w = self.compute_loss_and_metrics(batch)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_f1w', f1w, prog_bar=True)\n",
    "        return {'val_loss': torch.tensor(loss.item()), 'val_f1w': torch.tensor(f1w)}\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_val_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        avg_val_f1w  = torch.stack([x['val_f1w'] for x in outputs]).mean()\n",
    "        self.log('EarlyStop_Log', avg_val_loss.detach(), on_epoch=True, sync_dist=True)\n",
    "        self.log('avg_val_f1w', avg_val_f1w.detach(), on_epoch=True, sync_dist=True)\n",
    "        val_epoch_loss.append(avg_val_loss.item())\n",
    "        val_epoch_acc.append(avg_val_f1w.item())\n",
    "        #print(f'VAL-Epoch {self.current_epoch} LOSS:{avg_val_loss} ACC:{avg_val_acc} ')\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.0001)\n",
    "        # lr_scheduler = {'scheduler': MultiStepLR(optimizer, milestones=[10,20,30,40], gamma=0.5,),'interval': 'epoch','frequency':1}\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, \n",
    "                                                                            T_0=10,\n",
    "                                                                            T_mult=1,\n",
    "                                                                            eta_min=1e-7,\n",
    "                                                                            verbose=True,\n",
    "                                                                            )\n",
    "\n",
    "        return [optimizer], [lr_scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1a0bfd7-e403-46a7-ba7a-0f7bbdc1b280",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model  = LitModel(model=resnet_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cda3bd2f-e762-48f7-87e7-a27b9b8bf57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a0dfe9f-4cb3-470e-a205-3e7dae8dd5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(dirpath='/mnt/gpid08/datasets/remote_sensing/tmp_from_gpid07/honey/results/', \n",
    "                                      filename='Base-{epoch}-{val_loss:.2f}-{avg_val_f1w:.2f}',\n",
    "                                      monitor='avg_val_f1w',\n",
    "                                      verbose=True,\n",
    "                                      save_last=None,\n",
    "                                      save_top_k=1,\n",
    "                                      save_weights_only=False,\n",
    "                                      mode='max',\n",
    "                                      auto_insert_metric_name=True,\n",
    "                                      )\n",
    "\n",
    "\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor='EarlyStop_Log',\\\n",
    "                                    min_delta=0.00,\\\n",
    "                                    patience=5,\\\n",
    "                                    verbose=False,\\\n",
    "                                    mode='min')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51d6f660-237d-44d1-8ecd-e4fd3835c09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 GPUS\n"
     ]
    }
   ],
   "source": [
    "gpu = 1 if torch.cuda.is_available() else 0\n",
    "print(f'Using {gpu} GPUS')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31e8bef8-a4a4-46a6-8f87-1749c479905d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Set SLURM handle signals.\n",
      "\n",
      "  | Name    | Type   | Params\n",
      "-----------------------------------\n",
      "0 | model   | ResNet | 23.5 M\n",
      "1 | f1score | F1     | 0     \n",
      "-----------------------------------\n",
      "23.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.130    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     0: adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 0:  82%|████████▏ | 238/289 [01:57<00:25,  2.02it/s, loss=0.153, v_num=1666580, train_F1w=1.000]Epoch     1: adjusting learning rate of group 0 to 9.7555e-05.\n",
      "Epoch 0:  83%|████████▎ | 239/289 [01:57<00:24,  2.03it/s, loss=0.154, v_num=1666580, train_F1w=0.901]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  83%|████████▎ | 241/289 [01:58<00:23,  2.04it/s, loss=0.154, v_num=1666580, train_F1w=0.901]\n",
      "Epoch 0:  84%|████████▍ | 243/289 [01:58<00:22,  2.06it/s, loss=0.154, v_num=1666580, train_F1w=0.901]\n",
      "Epoch 0:  85%|████████▍ | 245/289 [01:58<00:21,  2.07it/s, loss=0.154, v_num=1666580, train_F1w=0.901]\n",
      "Epoch 0:  85%|████████▌ | 247/289 [01:58<00:20,  2.08it/s, loss=0.154, v_num=1666580, train_F1w=0.901]\n",
      "Epoch 0:  86%|████████▌ | 249/289 [01:58<00:19,  2.10it/s, loss=0.154, v_num=1666580, train_F1w=0.901]\n",
      "Epoch 0:  87%|████████▋ | 251/289 [01:58<00:17,  2.11it/s, loss=0.154, v_num=1666580, train_F1w=0.901]\n",
      "Epoch 0:  88%|████████▊ | 253/289 [01:58<00:16,  2.13it/s, loss=0.154, v_num=1666580, train_F1w=0.901]\n",
      "Epoch 0:  88%|████████▊ | 255/289 [01:59<00:15,  2.14it/s, loss=0.154, v_num=1666580, train_F1w=0.901]\n",
      "Epoch 0:  89%|████████▉ | 257/289 [01:59<00:14,  2.15it/s, loss=0.154, v_num=1666580, train_F1w=0.901]\n",
      "Epoch 0:  90%|████████▉ | 259/289 [01:59<00:13,  2.17it/s, loss=0.154, v_num=1666580, train_F1w=0.901]\n",
      "Epoch 0:  90%|█████████ | 261/289 [01:59<00:12,  2.18it/s, loss=0.154, v_num=1666580, train_F1w=0.901]\n",
      "Epoch 0:  91%|█████████ | 263/289 [01:59<00:11,  2.20it/s, loss=0.154, v_num=1666580, train_F1w=0.901]\n",
      "Epoch 0:  92%|█████████▏| 265/289 [01:59<00:10,  2.21it/s, loss=0.154, v_num=1666580, train_F1w=0.901]\n",
      "Epoch 0:  92%|█████████▏| 267/289 [02:00<00:09,  2.22it/s, loss=0.154, v_num=1666580, train_F1w=0.901]\n",
      "Epoch 0:  93%|█████████▎| 269/289 [02:00<00:08,  2.24it/s, loss=0.154, v_num=1666580, train_F1w=0.901]\n",
      "Epoch 0:  94%|█████████▍| 271/289 [02:00<00:07,  2.25it/s, loss=0.154, v_num=1666580, train_F1w=0.901]\n",
      "Epoch 0:  94%|█████████▍| 273/289 [02:00<00:07,  2.26it/s, loss=0.154, v_num=1666580, train_F1w=0.901]\n",
      "Epoch 0:  95%|█████████▌| 275/289 [02:00<00:06,  2.28it/s, loss=0.154, v_num=1666580, train_F1w=0.901]\n",
      "Epoch 0:  96%|█████████▌| 277/289 [02:00<00:05,  2.29it/s, loss=0.154, v_num=1666580, train_F1w=0.901]\n",
      "Epoch 0:  97%|█████████▋| 279/289 [02:01<00:04,  2.31it/s, loss=0.154, v_num=1666580, train_F1w=0.901]\n",
      "Epoch 0:  97%|█████████▋| 281/289 [02:01<00:03,  2.32it/s, loss=0.154, v_num=1666580, train_F1w=0.901]\n",
      "Epoch 0:  98%|█████████▊| 283/289 [02:01<00:02,  2.33it/s, loss=0.154, v_num=1666580, train_F1w=0.901]\n",
      "Epoch 0:  99%|█████████▊| 285/289 [02:01<00:01,  2.35it/s, loss=0.154, v_num=1666580, train_F1w=0.901]\n",
      "Epoch 0:  99%|█████████▉| 287/289 [02:01<00:00,  2.36it/s, loss=0.154, v_num=1666580, train_F1w=0.901]\n",
      "Epoch 0: 100%|██████████| 289/289 [02:01<00:00,  2.37it/s, loss=0.154, v_num=1666580, train_F1w=0.901, val_loss=0.145, val_f1w=0.946]\n",
      "Epoch 0: 100%|██████████| 289/289 [02:01<00:00,  2.37it/s, loss=0.154, v_num=1666580, train_F1w=0.901, val_loss=0.145, val_f1w=0.946]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 238: avg_val_f1w reached 0.94600 (best 0.94600), saving model to \"/mnt/gpid08/datasets/remote_sensing/tmp_from_gpid07/honey/results/Base-epoch=0-val_loss=0.14-avg_val_f1w=0.95.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  82%|████████▏ | 238/289 [01:55<00:24,  2.06it/s, loss=0.102, v_num=1666580, train_F1w=1.000, val_loss=0.145, val_f1w=0.946] Epoch     2: adjusting learning rate of group 0 to 9.0460e-05.\n",
      "Epoch 1:  83%|████████▎ | 240/289 [01:55<00:23,  2.08it/s, loss=0.0974, v_num=1666580, train_F1w=1.000, val_loss=0.145, val_f1w=0.946]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  84%|████████▎ | 242/289 [01:55<00:22,  2.09it/s, loss=0.0974, v_num=1666580, train_F1w=1.000, val_loss=0.145, val_f1w=0.946]\n",
      "Epoch 1:  84%|████████▍ | 244/289 [01:56<00:21,  2.10it/s, loss=0.0974, v_num=1666580, train_F1w=1.000, val_loss=0.145, val_f1w=0.946]\n",
      "Epoch 1:  85%|████████▌ | 246/289 [01:56<00:20,  2.12it/s, loss=0.0974, v_num=1666580, train_F1w=1.000, val_loss=0.145, val_f1w=0.946]\n",
      "Epoch 1:  86%|████████▌ | 248/289 [01:56<00:19,  2.13it/s, loss=0.0974, v_num=1666580, train_F1w=1.000, val_loss=0.145, val_f1w=0.946]\n",
      "Epoch 1:  87%|████████▋ | 250/289 [01:56<00:18,  2.15it/s, loss=0.0974, v_num=1666580, train_F1w=1.000, val_loss=0.145, val_f1w=0.946]\n",
      "Epoch 1:  87%|████████▋ | 252/289 [01:56<00:17,  2.16it/s, loss=0.0974, v_num=1666580, train_F1w=1.000, val_loss=0.145, val_f1w=0.946]\n",
      "Epoch 1:  88%|████████▊ | 254/289 [01:56<00:16,  2.17it/s, loss=0.0974, v_num=1666580, train_F1w=1.000, val_loss=0.145, val_f1w=0.946]\n",
      "Epoch 1:  89%|████████▊ | 256/289 [01:56<00:15,  2.19it/s, loss=0.0974, v_num=1666580, train_F1w=1.000, val_loss=0.145, val_f1w=0.946]\n",
      "Epoch 1:  89%|████████▉ | 258/289 [01:57<00:14,  2.20it/s, loss=0.0974, v_num=1666580, train_F1w=1.000, val_loss=0.145, val_f1w=0.946]\n",
      "Epoch 1:  90%|████████▉ | 260/289 [01:57<00:13,  2.22it/s, loss=0.0974, v_num=1666580, train_F1w=1.000, val_loss=0.145, val_f1w=0.946]\n",
      "Epoch 1:  91%|█████████ | 262/289 [01:57<00:12,  2.23it/s, loss=0.0974, v_num=1666580, train_F1w=1.000, val_loss=0.145, val_f1w=0.946]\n",
      "Epoch 1:  91%|█████████▏| 264/289 [01:57<00:11,  2.25it/s, loss=0.0974, v_num=1666580, train_F1w=1.000, val_loss=0.145, val_f1w=0.946]\n",
      "Epoch 1:  92%|█████████▏| 266/289 [01:57<00:10,  2.26it/s, loss=0.0974, v_num=1666580, train_F1w=1.000, val_loss=0.145, val_f1w=0.946]\n",
      "Epoch 1:  93%|█████████▎| 268/289 [01:57<00:09,  2.27it/s, loss=0.0974, v_num=1666580, train_F1w=1.000, val_loss=0.145, val_f1w=0.946]\n",
      "Epoch 1:  93%|█████████▎| 270/289 [01:58<00:08,  2.29it/s, loss=0.0974, v_num=1666580, train_F1w=1.000, val_loss=0.145, val_f1w=0.946]\n",
      "Epoch 1:  94%|█████████▍| 272/289 [01:58<00:07,  2.30it/s, loss=0.0974, v_num=1666580, train_F1w=1.000, val_loss=0.145, val_f1w=0.946]\n",
      "Epoch 1:  95%|█████████▍| 274/289 [01:58<00:06,  2.32it/s, loss=0.0974, v_num=1666580, train_F1w=1.000, val_loss=0.145, val_f1w=0.946]\n",
      "Epoch 1:  96%|█████████▌| 276/289 [01:58<00:05,  2.33it/s, loss=0.0974, v_num=1666580, train_F1w=1.000, val_loss=0.145, val_f1w=0.946]\n",
      "Epoch 1:  96%|█████████▌| 278/289 [01:58<00:04,  2.34it/s, loss=0.0974, v_num=1666580, train_F1w=1.000, val_loss=0.145, val_f1w=0.946]\n",
      "Epoch 1:  97%|█████████▋| 280/289 [01:58<00:03,  2.36it/s, loss=0.0974, v_num=1666580, train_F1w=1.000, val_loss=0.145, val_f1w=0.946]\n",
      "Epoch 1:  98%|█████████▊| 282/289 [01:58<00:02,  2.37it/s, loss=0.0974, v_num=1666580, train_F1w=1.000, val_loss=0.145, val_f1w=0.946]\n",
      "Epoch 1:  98%|█████████▊| 284/289 [01:59<00:02,  2.38it/s, loss=0.0974, v_num=1666580, train_F1w=1.000, val_loss=0.145, val_f1w=0.946]\n",
      "Epoch 1:  99%|█████████▉| 286/289 [01:59<00:01,  2.40it/s, loss=0.0974, v_num=1666580, train_F1w=1.000, val_loss=0.145, val_f1w=0.946]\n",
      "Epoch 1: 100%|█████████▉| 288/289 [01:59<00:00,  2.41it/s, loss=0.0974, v_num=1666580, train_F1w=1.000, val_loss=0.145, val_f1w=0.946]\n",
      "Epoch 1: 100%|██████████| 289/289 [01:59<00:00,  2.42it/s, loss=0.0974, v_num=1666580, train_F1w=1.000, val_loss=0.0772, val_f1w=0.974]\n",
      "Epoch 1: 100%|██████████| 289/289 [01:59<00:00,  2.42it/s, loss=0.0974, v_num=1666580, train_F1w=1.000, val_loss=0.0772, val_f1w=0.974]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 477: avg_val_f1w reached 0.97417 (best 0.97417), saving model to \"/mnt/gpid08/datasets/remote_sensing/tmp_from_gpid07/honey/results/Base-epoch=1-val_loss=0.08-avg_val_f1w=0.97.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  82%|████████▏ | 238/289 [01:55<00:24,  2.05it/s, loss=0.059, v_num=1666580, train_F1w=1.000, val_loss=0.0772, val_f1w=0.974] Epoch     3: adjusting learning rate of group 0 to 7.9410e-05.\n",
      "Epoch 2:  83%|████████▎ | 240/289 [01:56<00:23,  2.07it/s, loss=0.0579, v_num=1666580, train_F1w=0.966, val_loss=0.0772, val_f1w=0.974]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  84%|████████▎ | 242/289 [01:56<00:22,  2.08it/s, loss=0.0579, v_num=1666580, train_F1w=0.966, val_loss=0.0772, val_f1w=0.974]\n",
      "Epoch 2:  84%|████████▍ | 244/289 [01:56<00:21,  2.09it/s, loss=0.0579, v_num=1666580, train_F1w=0.966, val_loss=0.0772, val_f1w=0.974]\n",
      "Epoch 2:  85%|████████▌ | 246/289 [01:56<00:20,  2.11it/s, loss=0.0579, v_num=1666580, train_F1w=0.966, val_loss=0.0772, val_f1w=0.974]\n",
      "Epoch 2:  86%|████████▌ | 248/289 [01:56<00:19,  2.12it/s, loss=0.0579, v_num=1666580, train_F1w=0.966, val_loss=0.0772, val_f1w=0.974]\n",
      "Epoch 2:  87%|████████▋ | 250/289 [01:57<00:18,  2.14it/s, loss=0.0579, v_num=1666580, train_F1w=0.966, val_loss=0.0772, val_f1w=0.974]\n",
      "Epoch 2:  87%|████████▋ | 252/289 [01:57<00:17,  2.15it/s, loss=0.0579, v_num=1666580, train_F1w=0.966, val_loss=0.0772, val_f1w=0.974]\n",
      "Epoch 2:  88%|████████▊ | 254/289 [01:57<00:16,  2.16it/s, loss=0.0579, v_num=1666580, train_F1w=0.966, val_loss=0.0772, val_f1w=0.974]\n",
      "Epoch 2:  89%|████████▊ | 256/289 [01:57<00:15,  2.18it/s, loss=0.0579, v_num=1666580, train_F1w=0.966, val_loss=0.0772, val_f1w=0.974]\n",
      "Epoch 2:  89%|████████▉ | 258/289 [01:57<00:14,  2.19it/s, loss=0.0579, v_num=1666580, train_F1w=0.966, val_loss=0.0772, val_f1w=0.974]\n",
      "Epoch 2:  90%|████████▉ | 260/289 [01:57<00:13,  2.21it/s, loss=0.0579, v_num=1666580, train_F1w=0.966, val_loss=0.0772, val_f1w=0.974]\n",
      "Epoch 2:  91%|█████████ | 262/289 [01:57<00:12,  2.22it/s, loss=0.0579, v_num=1666580, train_F1w=0.966, val_loss=0.0772, val_f1w=0.974]\n",
      "Epoch 2:  91%|█████████▏| 264/289 [01:58<00:11,  2.24it/s, loss=0.0579, v_num=1666580, train_F1w=0.966, val_loss=0.0772, val_f1w=0.974]\n",
      "Epoch 2:  92%|█████████▏| 266/289 [01:58<00:10,  2.25it/s, loss=0.0579, v_num=1666580, train_F1w=0.966, val_loss=0.0772, val_f1w=0.974]\n",
      "Epoch 2:  93%|█████████▎| 268/289 [01:58<00:09,  2.26it/s, loss=0.0579, v_num=1666580, train_F1w=0.966, val_loss=0.0772, val_f1w=0.974]\n",
      "Epoch 2:  93%|█████████▎| 270/289 [01:58<00:08,  2.28it/s, loss=0.0579, v_num=1666580, train_F1w=0.966, val_loss=0.0772, val_f1w=0.974]\n",
      "Epoch 2:  94%|█████████▍| 272/289 [01:58<00:07,  2.29it/s, loss=0.0579, v_num=1666580, train_F1w=0.966, val_loss=0.0772, val_f1w=0.974]\n",
      "Epoch 2:  95%|█████████▍| 274/289 [01:58<00:06,  2.30it/s, loss=0.0579, v_num=1666580, train_F1w=0.966, val_loss=0.0772, val_f1w=0.974]\n",
      "Epoch 2:  96%|█████████▌| 276/289 [01:59<00:05,  2.32it/s, loss=0.0579, v_num=1666580, train_F1w=0.966, val_loss=0.0772, val_f1w=0.974]\n",
      "Epoch 2:  96%|█████████▌| 278/289 [01:59<00:04,  2.33it/s, loss=0.0579, v_num=1666580, train_F1w=0.966, val_loss=0.0772, val_f1w=0.974]\n",
      "Epoch 2:  97%|█████████▋| 280/289 [01:59<00:03,  2.35it/s, loss=0.0579, v_num=1666580, train_F1w=0.966, val_loss=0.0772, val_f1w=0.974]\n",
      "Epoch 2:  98%|█████████▊| 282/289 [01:59<00:02,  2.36it/s, loss=0.0579, v_num=1666580, train_F1w=0.966, val_loss=0.0772, val_f1w=0.974]\n",
      "Epoch 2:  98%|█████████▊| 284/289 [01:59<00:02,  2.37it/s, loss=0.0579, v_num=1666580, train_F1w=0.966, val_loss=0.0772, val_f1w=0.974]\n",
      "Epoch 2:  99%|█████████▉| 286/289 [01:59<00:01,  2.39it/s, loss=0.0579, v_num=1666580, train_F1w=0.966, val_loss=0.0772, val_f1w=0.974]\n",
      "Epoch 2: 100%|█████████▉| 288/289 [02:00<00:00,  2.40it/s, loss=0.0579, v_num=1666580, train_F1w=0.966, val_loss=0.0772, val_f1w=0.974]\n",
      "Epoch 2: 100%|██████████| 289/289 [02:00<00:00,  2.41it/s, loss=0.0579, v_num=1666580, train_F1w=0.966, val_loss=0.0792, val_f1w=0.978]\n",
      "Epoch 2: 100%|██████████| 289/289 [02:00<00:00,  2.41it/s, loss=0.0579, v_num=1666580, train_F1w=0.966, val_loss=0.0792, val_f1w=0.978]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 716: avg_val_f1w reached 0.97817 (best 0.97817), saving model to \"/mnt/gpid08/datasets/remote_sensing/tmp_from_gpid07/honey/results/Base-epoch=2-val_loss=0.08-avg_val_f1w=0.98.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  82%|████████▏ | 238/289 [01:55<00:24,  2.06it/s, loss=0.0268, v_num=1666580, train_F1w=1.000, val_loss=0.0792, val_f1w=0.978] Epoch     4: adjusting learning rate of group 0 to 6.5485e-05.\n",
      "Epoch 3:  83%|████████▎ | 240/289 [01:56<00:23,  2.07it/s, loss=0.0275, v_num=1666580, train_F1w=1.000, val_loss=0.0792, val_f1w=0.978]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  84%|████████▎ | 242/289 [01:56<00:22,  2.08it/s, loss=0.0275, v_num=1666580, train_F1w=1.000, val_loss=0.0792, val_f1w=0.978]\n",
      "Epoch 3:  84%|████████▍ | 244/289 [01:56<00:21,  2.10it/s, loss=0.0275, v_num=1666580, train_F1w=1.000, val_loss=0.0792, val_f1w=0.978]\n",
      "Epoch 3:  85%|████████▌ | 246/289 [01:56<00:20,  2.11it/s, loss=0.0275, v_num=1666580, train_F1w=1.000, val_loss=0.0792, val_f1w=0.978]\n",
      "Epoch 3:  86%|████████▌ | 248/289 [01:56<00:19,  2.13it/s, loss=0.0275, v_num=1666580, train_F1w=1.000, val_loss=0.0792, val_f1w=0.978]\n",
      "Epoch 3:  87%|████████▋ | 250/289 [01:56<00:18,  2.14it/s, loss=0.0275, v_num=1666580, train_F1w=1.000, val_loss=0.0792, val_f1w=0.978]\n",
      "Epoch 3:  87%|████████▋ | 252/289 [01:56<00:17,  2.15it/s, loss=0.0275, v_num=1666580, train_F1w=1.000, val_loss=0.0792, val_f1w=0.978]\n",
      "Epoch 3:  88%|████████▊ | 254/289 [01:57<00:16,  2.17it/s, loss=0.0275, v_num=1666580, train_F1w=1.000, val_loss=0.0792, val_f1w=0.978]\n",
      "Epoch 3:  89%|████████▊ | 256/289 [01:57<00:15,  2.18it/s, loss=0.0275, v_num=1666580, train_F1w=1.000, val_loss=0.0792, val_f1w=0.978]\n",
      "Epoch 3:  89%|████████▉ | 258/289 [01:57<00:14,  2.20it/s, loss=0.0275, v_num=1666580, train_F1w=1.000, val_loss=0.0792, val_f1w=0.978]\n",
      "Epoch 3:  90%|████████▉ | 260/289 [01:57<00:13,  2.21it/s, loss=0.0275, v_num=1666580, train_F1w=1.000, val_loss=0.0792, val_f1w=0.978]\n",
      "Epoch 3:  91%|█████████ | 262/289 [01:57<00:12,  2.22it/s, loss=0.0275, v_num=1666580, train_F1w=1.000, val_loss=0.0792, val_f1w=0.978]\n",
      "Epoch 3:  91%|█████████▏| 264/289 [01:57<00:11,  2.24it/s, loss=0.0275, v_num=1666580, train_F1w=1.000, val_loss=0.0792, val_f1w=0.978]\n",
      "Epoch 3:  92%|█████████▏| 266/289 [01:58<00:10,  2.25it/s, loss=0.0275, v_num=1666580, train_F1w=1.000, val_loss=0.0792, val_f1w=0.978]\n",
      "Epoch 3:  93%|█████████▎| 268/289 [01:58<00:09,  2.27it/s, loss=0.0275, v_num=1666580, train_F1w=1.000, val_loss=0.0792, val_f1w=0.978]\n",
      "Epoch 3:  93%|█████████▎| 270/289 [01:58<00:08,  2.28it/s, loss=0.0275, v_num=1666580, train_F1w=1.000, val_loss=0.0792, val_f1w=0.978]\n",
      "Epoch 3:  94%|█████████▍| 272/289 [01:58<00:07,  2.29it/s, loss=0.0275, v_num=1666580, train_F1w=1.000, val_loss=0.0792, val_f1w=0.978]\n",
      "Epoch 3:  95%|█████████▍| 274/289 [01:58<00:06,  2.31it/s, loss=0.0275, v_num=1666580, train_F1w=1.000, val_loss=0.0792, val_f1w=0.978]\n",
      "Epoch 3:  96%|█████████▌| 276/289 [01:58<00:05,  2.32it/s, loss=0.0275, v_num=1666580, train_F1w=1.000, val_loss=0.0792, val_f1w=0.978]\n",
      "Epoch 3:  96%|█████████▌| 278/289 [01:59<00:04,  2.34it/s, loss=0.0275, v_num=1666580, train_F1w=1.000, val_loss=0.0792, val_f1w=0.978]\n",
      "Epoch 3:  97%|█████████▋| 280/289 [01:59<00:03,  2.35it/s, loss=0.0275, v_num=1666580, train_F1w=1.000, val_loss=0.0792, val_f1w=0.978]\n",
      "Epoch 3:  98%|█████████▊| 282/289 [01:59<00:02,  2.36it/s, loss=0.0275, v_num=1666580, train_F1w=1.000, val_loss=0.0792, val_f1w=0.978]\n",
      "Epoch 3:  98%|█████████▊| 284/289 [01:59<00:02,  2.38it/s, loss=0.0275, v_num=1666580, train_F1w=1.000, val_loss=0.0792, val_f1w=0.978]\n",
      "Epoch 3:  99%|█████████▉| 286/289 [01:59<00:01,  2.39it/s, loss=0.0275, v_num=1666580, train_F1w=1.000, val_loss=0.0792, val_f1w=0.978]\n",
      "Epoch 3: 100%|█████████▉| 288/289 [01:59<00:00,  2.40it/s, loss=0.0275, v_num=1666580, train_F1w=1.000, val_loss=0.0792, val_f1w=0.978]\n",
      "Epoch 3: 100%|██████████| 289/289 [01:59<00:00,  2.41it/s, loss=0.0275, v_num=1666580, train_F1w=1.000, val_loss=0.0608, val_f1w=0.978]\n",
      "Epoch 3: 100%|██████████| 289/289 [01:59<00:00,  2.41it/s, loss=0.0275, v_num=1666580, train_F1w=1.000, val_loss=0.0608, val_f1w=0.978]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 955: avg_val_f1w reached 0.97840 (best 0.97840), saving model to \"/mnt/gpid08/datasets/remote_sensing/tmp_from_gpid07/honey/results/Base-epoch=3-val_loss=0.06-avg_val_f1w=0.98.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:  82%|████████▏ | 238/289 [01:56<00:24,  2.05it/s, loss=0.0156, v_num=1666580, train_F1w=1.000, val_loss=0.0608, val_f1w=0.978] Epoch     5: adjusting learning rate of group 0 to 5.0050e-05.\n",
      "Epoch 4:  83%|████████▎ | 240/289 [01:56<00:23,  2.06it/s, loss=0.0156, v_num=1666580, train_F1w=1.000, val_loss=0.0608, val_f1w=0.978]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:  84%|████████▎ | 242/289 [01:56<00:22,  2.08it/s, loss=0.0156, v_num=1666580, train_F1w=1.000, val_loss=0.0608, val_f1w=0.978]\n",
      "Epoch 4:  84%|████████▍ | 244/289 [01:56<00:21,  2.09it/s, loss=0.0156, v_num=1666580, train_F1w=1.000, val_loss=0.0608, val_f1w=0.978]\n",
      "Epoch 4:  85%|████████▌ | 246/289 [01:56<00:20,  2.11it/s, loss=0.0156, v_num=1666580, train_F1w=1.000, val_loss=0.0608, val_f1w=0.978]\n",
      "Epoch 4:  86%|████████▌ | 248/289 [01:56<00:19,  2.12it/s, loss=0.0156, v_num=1666580, train_F1w=1.000, val_loss=0.0608, val_f1w=0.978]\n",
      "Epoch 4:  87%|████████▋ | 250/289 [01:57<00:18,  2.13it/s, loss=0.0156, v_num=1666580, train_F1w=1.000, val_loss=0.0608, val_f1w=0.978]\n",
      "Epoch 4:  87%|████████▋ | 252/289 [01:57<00:17,  2.15it/s, loss=0.0156, v_num=1666580, train_F1w=1.000, val_loss=0.0608, val_f1w=0.978]\n",
      "Epoch 4:  88%|████████▊ | 254/289 [01:57<00:16,  2.16it/s, loss=0.0156, v_num=1666580, train_F1w=1.000, val_loss=0.0608, val_f1w=0.978]\n",
      "Epoch 4:  89%|████████▊ | 256/289 [01:57<00:15,  2.18it/s, loss=0.0156, v_num=1666580, train_F1w=1.000, val_loss=0.0608, val_f1w=0.978]\n",
      "Epoch 4:  89%|████████▉ | 258/289 [01:57<00:14,  2.19it/s, loss=0.0156, v_num=1666580, train_F1w=1.000, val_loss=0.0608, val_f1w=0.978]\n",
      "Epoch 4:  90%|████████▉ | 260/289 [01:57<00:13,  2.21it/s, loss=0.0156, v_num=1666580, train_F1w=1.000, val_loss=0.0608, val_f1w=0.978]\n",
      "Epoch 4:  91%|█████████ | 262/289 [01:58<00:12,  2.22it/s, loss=0.0156, v_num=1666580, train_F1w=1.000, val_loss=0.0608, val_f1w=0.978]\n",
      "Epoch 4:  91%|█████████▏| 264/289 [01:58<00:11,  2.23it/s, loss=0.0156, v_num=1666580, train_F1w=1.000, val_loss=0.0608, val_f1w=0.978]\n",
      "Epoch 4:  92%|█████████▏| 266/289 [01:58<00:10,  2.25it/s, loss=0.0156, v_num=1666580, train_F1w=1.000, val_loss=0.0608, val_f1w=0.978]\n",
      "Epoch 4:  93%|█████████▎| 268/289 [01:58<00:09,  2.26it/s, loss=0.0156, v_num=1666580, train_F1w=1.000, val_loss=0.0608, val_f1w=0.978]\n",
      "Epoch 4:  93%|█████████▎| 270/289 [01:58<00:08,  2.28it/s, loss=0.0156, v_num=1666580, train_F1w=1.000, val_loss=0.0608, val_f1w=0.978]\n",
      "Epoch 4:  94%|█████████▍| 272/289 [01:58<00:07,  2.29it/s, loss=0.0156, v_num=1666580, train_F1w=1.000, val_loss=0.0608, val_f1w=0.978]\n",
      "Epoch 4:  95%|█████████▍| 274/289 [01:58<00:06,  2.30it/s, loss=0.0156, v_num=1666580, train_F1w=1.000, val_loss=0.0608, val_f1w=0.978]\n",
      "Epoch 4:  96%|█████████▌| 276/289 [01:59<00:05,  2.32it/s, loss=0.0156, v_num=1666580, train_F1w=1.000, val_loss=0.0608, val_f1w=0.978]\n",
      "Epoch 4:  96%|█████████▌| 278/289 [01:59<00:04,  2.33it/s, loss=0.0156, v_num=1666580, train_F1w=1.000, val_loss=0.0608, val_f1w=0.978]\n",
      "Epoch 4:  97%|█████████▋| 280/289 [01:59<00:03,  2.34it/s, loss=0.0156, v_num=1666580, train_F1w=1.000, val_loss=0.0608, val_f1w=0.978]\n",
      "Epoch 4:  98%|█████████▊| 282/289 [01:59<00:02,  2.36it/s, loss=0.0156, v_num=1666580, train_F1w=1.000, val_loss=0.0608, val_f1w=0.978]\n",
      "Epoch 4:  98%|█████████▊| 284/289 [01:59<00:02,  2.37it/s, loss=0.0156, v_num=1666580, train_F1w=1.000, val_loss=0.0608, val_f1w=0.978]\n",
      "Epoch 4:  99%|█████████▉| 286/289 [01:59<00:01,  2.38it/s, loss=0.0156, v_num=1666580, train_F1w=1.000, val_loss=0.0608, val_f1w=0.978]\n",
      "Epoch 4: 100%|█████████▉| 288/289 [02:00<00:00,  2.40it/s, loss=0.0156, v_num=1666580, train_F1w=1.000, val_loss=0.0608, val_f1w=0.978]\n",
      "Epoch 4: 100%|██████████| 289/289 [02:00<00:00,  2.40it/s, loss=0.0156, v_num=1666580, train_F1w=1.000, val_loss=0.0973, val_f1w=0.973]\n",
      "Epoch 4: 100%|██████████| 289/289 [02:00<00:00,  2.40it/s, loss=0.0156, v_num=1666580, train_F1w=1.000, val_loss=0.0973, val_f1w=0.973]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 1194: avg_val_f1w was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:  82%|████████▏ | 238/289 [01:56<00:25,  2.04it/s, loss=0.00738, v_num=1666580, train_F1w=1.000, val_loss=0.0973, val_f1w=0.973]Epoch     6: adjusting learning rate of group 0 to 3.4615e-05.\n",
      "Epoch 5:  83%|████████▎ | 240/289 [01:57<00:23,  2.05it/s, loss=0.00726, v_num=1666580, train_F1w=1.000, val_loss=0.0973, val_f1w=0.973]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5:  84%|████████▎ | 242/289 [01:57<00:22,  2.06it/s, loss=0.00726, v_num=1666580, train_F1w=1.000, val_loss=0.0973, val_f1w=0.973]\n",
      "Epoch 5:  84%|████████▍ | 244/289 [01:57<00:21,  2.08it/s, loss=0.00726, v_num=1666580, train_F1w=1.000, val_loss=0.0973, val_f1w=0.973]\n",
      "Epoch 5:  85%|████████▌ | 246/289 [01:57<00:20,  2.09it/s, loss=0.00726, v_num=1666580, train_F1w=1.000, val_loss=0.0973, val_f1w=0.973]\n",
      "Epoch 5:  86%|████████▌ | 248/289 [01:57<00:19,  2.10it/s, loss=0.00726, v_num=1666580, train_F1w=1.000, val_loss=0.0973, val_f1w=0.973]\n",
      "Epoch 5:  87%|████████▋ | 250/289 [01:58<00:18,  2.12it/s, loss=0.00726, v_num=1666580, train_F1w=1.000, val_loss=0.0973, val_f1w=0.973]\n",
      "Epoch 5:  87%|████████▋ | 252/289 [01:58<00:17,  2.13it/s, loss=0.00726, v_num=1666580, train_F1w=1.000, val_loss=0.0973, val_f1w=0.973]\n",
      "Epoch 5:  88%|████████▊ | 254/289 [01:58<00:16,  2.15it/s, loss=0.00726, v_num=1666580, train_F1w=1.000, val_loss=0.0973, val_f1w=0.973]\n",
      "Epoch 5:  89%|████████▊ | 256/289 [01:58<00:15,  2.16it/s, loss=0.00726, v_num=1666580, train_F1w=1.000, val_loss=0.0973, val_f1w=0.973]\n",
      "Epoch 5:  89%|████████▉ | 258/289 [01:58<00:14,  2.17it/s, loss=0.00726, v_num=1666580, train_F1w=1.000, val_loss=0.0973, val_f1w=0.973]\n",
      "Epoch 5:  90%|████████▉ | 260/289 [01:58<00:13,  2.19it/s, loss=0.00726, v_num=1666580, train_F1w=1.000, val_loss=0.0973, val_f1w=0.973]\n",
      "Epoch 5:  91%|█████████ | 262/289 [01:58<00:12,  2.20it/s, loss=0.00726, v_num=1666580, train_F1w=1.000, val_loss=0.0973, val_f1w=0.973]\n",
      "Epoch 5:  91%|█████████▏| 264/289 [01:59<00:11,  2.22it/s, loss=0.00726, v_num=1666580, train_F1w=1.000, val_loss=0.0973, val_f1w=0.973]\n",
      "Epoch 5:  92%|█████████▏| 266/289 [01:59<00:10,  2.23it/s, loss=0.00726, v_num=1666580, train_F1w=1.000, val_loss=0.0973, val_f1w=0.973]\n",
      "Epoch 5:  93%|█████████▎| 268/289 [01:59<00:09,  2.24it/s, loss=0.00726, v_num=1666580, train_F1w=1.000, val_loss=0.0973, val_f1w=0.973]\n",
      "Epoch 5:  93%|█████████▎| 270/289 [01:59<00:08,  2.26it/s, loss=0.00726, v_num=1666580, train_F1w=1.000, val_loss=0.0973, val_f1w=0.973]\n",
      "Epoch 5:  94%|█████████▍| 272/289 [01:59<00:07,  2.27it/s, loss=0.00726, v_num=1666580, train_F1w=1.000, val_loss=0.0973, val_f1w=0.973]\n",
      "Epoch 5:  95%|█████████▍| 274/289 [01:59<00:06,  2.29it/s, loss=0.00726, v_num=1666580, train_F1w=1.000, val_loss=0.0973, val_f1w=0.973]\n",
      "Epoch 5:  96%|█████████▌| 276/289 [02:00<00:05,  2.30it/s, loss=0.00726, v_num=1666580, train_F1w=1.000, val_loss=0.0973, val_f1w=0.973]\n",
      "Epoch 5:  96%|█████████▌| 278/289 [02:00<00:04,  2.31it/s, loss=0.00726, v_num=1666580, train_F1w=1.000, val_loss=0.0973, val_f1w=0.973]\n",
      "Epoch 5:  97%|█████████▋| 280/289 [02:00<00:03,  2.33it/s, loss=0.00726, v_num=1666580, train_F1w=1.000, val_loss=0.0973, val_f1w=0.973]\n",
      "Epoch 5:  98%|█████████▊| 282/289 [02:00<00:02,  2.34it/s, loss=0.00726, v_num=1666580, train_F1w=1.000, val_loss=0.0973, val_f1w=0.973]\n",
      "Epoch 5:  98%|█████████▊| 284/289 [02:00<00:02,  2.35it/s, loss=0.00726, v_num=1666580, train_F1w=1.000, val_loss=0.0973, val_f1w=0.973]\n",
      "Epoch 5:  99%|█████████▉| 286/289 [02:00<00:01,  2.37it/s, loss=0.00726, v_num=1666580, train_F1w=1.000, val_loss=0.0973, val_f1w=0.973]\n",
      "Epoch 5: 100%|█████████▉| 288/289 [02:01<00:00,  2.38it/s, loss=0.00726, v_num=1666580, train_F1w=1.000, val_loss=0.0973, val_f1w=0.973]\n",
      "Epoch 5: 100%|██████████| 289/289 [02:01<00:00,  2.39it/s, loss=0.00726, v_num=1666580, train_F1w=1.000, val_loss=0.057, val_f1w=0.986] \n",
      "Epoch 5: 100%|██████████| 289/289 [02:01<00:00,  2.39it/s, loss=0.00726, v_num=1666580, train_F1w=1.000, val_loss=0.057, val_f1w=0.986]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 1433: avg_val_f1w reached 0.98567 (best 0.98567), saving model to \"/mnt/gpid08/datasets/remote_sensing/tmp_from_gpid07/honey/results/Base-epoch=5-val_loss=0.06-avg_val_f1w=0.99.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:  82%|████████▏ | 238/289 [01:56<00:24,  2.04it/s, loss=0.0041, v_num=1666580, train_F1w=1.000, val_loss=0.057, val_f1w=0.986] Epoch     7: adjusting learning rate of group 0 to 2.0690e-05.\n",
      "Epoch 6:  83%|████████▎ | 240/289 [01:56<00:23,  2.06it/s, loss=0.00417, v_num=1666580, train_F1w=1.000, val_loss=0.057, val_f1w=0.986]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6:  84%|████████▎ | 242/289 [01:56<00:22,  2.07it/s, loss=0.00417, v_num=1666580, train_F1w=1.000, val_loss=0.057, val_f1w=0.986]\n",
      "Epoch 6:  84%|████████▍ | 244/289 [01:57<00:21,  2.08it/s, loss=0.00417, v_num=1666580, train_F1w=1.000, val_loss=0.057, val_f1w=0.986]\n",
      "Epoch 6:  85%|████████▌ | 246/289 [01:57<00:20,  2.10it/s, loss=0.00417, v_num=1666580, train_F1w=1.000, val_loss=0.057, val_f1w=0.986]\n",
      "Epoch 6:  86%|████████▌ | 248/289 [01:57<00:19,  2.11it/s, loss=0.00417, v_num=1666580, train_F1w=1.000, val_loss=0.057, val_f1w=0.986]\n",
      "Epoch 6:  87%|████████▋ | 250/289 [01:57<00:18,  2.13it/s, loss=0.00417, v_num=1666580, train_F1w=1.000, val_loss=0.057, val_f1w=0.986]\n",
      "Epoch 6:  87%|████████▋ | 252/289 [01:57<00:17,  2.14it/s, loss=0.00417, v_num=1666580, train_F1w=1.000, val_loss=0.057, val_f1w=0.986]\n",
      "Epoch 6:  88%|████████▊ | 254/289 [01:57<00:16,  2.16it/s, loss=0.00417, v_num=1666580, train_F1w=1.000, val_loss=0.057, val_f1w=0.986]\n",
      "Epoch 6:  89%|████████▊ | 256/289 [01:57<00:15,  2.17it/s, loss=0.00417, v_num=1666580, train_F1w=1.000, val_loss=0.057, val_f1w=0.986]\n",
      "Epoch 6:  89%|████████▉ | 258/289 [01:58<00:14,  2.18it/s, loss=0.00417, v_num=1666580, train_F1w=1.000, val_loss=0.057, val_f1w=0.986]\n",
      "Epoch 6:  90%|████████▉ | 260/289 [01:58<00:13,  2.20it/s, loss=0.00417, v_num=1666580, train_F1w=1.000, val_loss=0.057, val_f1w=0.986]\n",
      "Epoch 6:  91%|█████████ | 262/289 [01:58<00:12,  2.21it/s, loss=0.00417, v_num=1666580, train_F1w=1.000, val_loss=0.057, val_f1w=0.986]\n",
      "Epoch 6:  91%|█████████▏| 264/289 [01:58<00:11,  2.23it/s, loss=0.00417, v_num=1666580, train_F1w=1.000, val_loss=0.057, val_f1w=0.986]\n",
      "Epoch 6:  92%|█████████▏| 266/289 [01:58<00:10,  2.24it/s, loss=0.00417, v_num=1666580, train_F1w=1.000, val_loss=0.057, val_f1w=0.986]\n",
      "Epoch 6:  93%|█████████▎| 268/289 [01:58<00:09,  2.25it/s, loss=0.00417, v_num=1666580, train_F1w=1.000, val_loss=0.057, val_f1w=0.986]\n",
      "Epoch 6:  93%|█████████▎| 270/289 [01:59<00:08,  2.27it/s, loss=0.00417, v_num=1666580, train_F1w=1.000, val_loss=0.057, val_f1w=0.986]\n",
      "Epoch 6:  94%|█████████▍| 272/289 [01:59<00:07,  2.28it/s, loss=0.00417, v_num=1666580, train_F1w=1.000, val_loss=0.057, val_f1w=0.986]\n",
      "Epoch 6:  95%|█████████▍| 274/289 [01:59<00:06,  2.30it/s, loss=0.00417, v_num=1666580, train_F1w=1.000, val_loss=0.057, val_f1w=0.986]\n",
      "Epoch 6:  96%|█████████▌| 276/289 [01:59<00:05,  2.31it/s, loss=0.00417, v_num=1666580, train_F1w=1.000, val_loss=0.057, val_f1w=0.986]\n",
      "Epoch 6:  96%|█████████▌| 278/289 [01:59<00:04,  2.32it/s, loss=0.00417, v_num=1666580, train_F1w=1.000, val_loss=0.057, val_f1w=0.986]\n",
      "Epoch 6:  97%|█████████▋| 280/289 [01:59<00:03,  2.34it/s, loss=0.00417, v_num=1666580, train_F1w=1.000, val_loss=0.057, val_f1w=0.986]\n",
      "Epoch 6:  98%|█████████▊| 282/289 [01:59<00:02,  2.35it/s, loss=0.00417, v_num=1666580, train_F1w=1.000, val_loss=0.057, val_f1w=0.986]\n",
      "Epoch 6:  98%|█████████▊| 284/289 [02:00<00:02,  2.36it/s, loss=0.00417, v_num=1666580, train_F1w=1.000, val_loss=0.057, val_f1w=0.986]\n",
      "Epoch 6:  99%|█████████▉| 286/289 [02:00<00:01,  2.38it/s, loss=0.00417, v_num=1666580, train_F1w=1.000, val_loss=0.057, val_f1w=0.986]\n",
      "Epoch 6: 100%|█████████▉| 288/289 [02:00<00:00,  2.39it/s, loss=0.00417, v_num=1666580, train_F1w=1.000, val_loss=0.057, val_f1w=0.986]\n",
      "Epoch 6: 100%|██████████| 289/289 [02:00<00:00,  2.40it/s, loss=0.00417, v_num=1666580, train_F1w=1.000, val_loss=0.0653, val_f1w=0.982]\n",
      "Epoch 6: 100%|██████████| 289/289 [02:00<00:00,  2.40it/s, loss=0.00417, v_num=1666580, train_F1w=1.000, val_loss=0.0653, val_f1w=0.982]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 1672: avg_val_f1w was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:  82%|████████▏ | 238/289 [01:56<00:24,  2.04it/s, loss=0.00319, v_num=1666580, train_F1w=1.000, val_loss=0.0653, val_f1w=0.982]Epoch     8: adjusting learning rate of group 0 to 9.6396e-06.\n",
      "Epoch 7:  83%|████████▎ | 240/289 [01:56<00:23,  2.05it/s, loss=0.00321, v_num=1666580, train_F1w=1.000, val_loss=0.0653, val_f1w=0.982]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7:  84%|████████▎ | 242/289 [01:57<00:22,  2.07it/s, loss=0.00321, v_num=1666580, train_F1w=1.000, val_loss=0.0653, val_f1w=0.982]\n",
      "Epoch 7:  84%|████████▍ | 244/289 [01:57<00:21,  2.08it/s, loss=0.00321, v_num=1666580, train_F1w=1.000, val_loss=0.0653, val_f1w=0.982]\n",
      "Epoch 7:  85%|████████▌ | 246/289 [01:57<00:20,  2.10it/s, loss=0.00321, v_num=1666580, train_F1w=1.000, val_loss=0.0653, val_f1w=0.982]\n",
      "Epoch 7:  86%|████████▌ | 248/289 [01:57<00:19,  2.11it/s, loss=0.00321, v_num=1666580, train_F1w=1.000, val_loss=0.0653, val_f1w=0.982]\n",
      "Epoch 7:  87%|████████▋ | 250/289 [01:57<00:18,  2.13it/s, loss=0.00321, v_num=1666580, train_F1w=1.000, val_loss=0.0653, val_f1w=0.982]\n",
      "Epoch 7:  87%|████████▋ | 252/289 [01:57<00:17,  2.14it/s, loss=0.00321, v_num=1666580, train_F1w=1.000, val_loss=0.0653, val_f1w=0.982]\n",
      "Epoch 7:  88%|████████▊ | 254/289 [01:57<00:16,  2.15it/s, loss=0.00321, v_num=1666580, train_F1w=1.000, val_loss=0.0653, val_f1w=0.982]\n",
      "Epoch 7:  89%|████████▊ | 256/289 [01:58<00:15,  2.17it/s, loss=0.00321, v_num=1666580, train_F1w=1.000, val_loss=0.0653, val_f1w=0.982]\n",
      "Epoch 7:  89%|████████▉ | 258/289 [01:58<00:14,  2.18it/s, loss=0.00321, v_num=1666580, train_F1w=1.000, val_loss=0.0653, val_f1w=0.982]\n",
      "Epoch 7:  90%|████████▉ | 260/289 [01:58<00:13,  2.20it/s, loss=0.00321, v_num=1666580, train_F1w=1.000, val_loss=0.0653, val_f1w=0.982]\n",
      "Epoch 7:  91%|█████████ | 262/289 [01:58<00:12,  2.21it/s, loss=0.00321, v_num=1666580, train_F1w=1.000, val_loss=0.0653, val_f1w=0.982]\n",
      "Epoch 7:  91%|█████████▏| 264/289 [01:58<00:11,  2.22it/s, loss=0.00321, v_num=1666580, train_F1w=1.000, val_loss=0.0653, val_f1w=0.982]\n",
      "Epoch 7:  92%|█████████▏| 266/289 [01:58<00:10,  2.24it/s, loss=0.00321, v_num=1666580, train_F1w=1.000, val_loss=0.0653, val_f1w=0.982]\n",
      "Epoch 7:  93%|█████████▎| 268/289 [01:59<00:09,  2.25it/s, loss=0.00321, v_num=1666580, train_F1w=1.000, val_loss=0.0653, val_f1w=0.982]\n",
      "Epoch 7:  93%|█████████▎| 270/289 [01:59<00:08,  2.27it/s, loss=0.00321, v_num=1666580, train_F1w=1.000, val_loss=0.0653, val_f1w=0.982]\n",
      "Epoch 7:  94%|█████████▍| 272/289 [01:59<00:07,  2.28it/s, loss=0.00321, v_num=1666580, train_F1w=1.000, val_loss=0.0653, val_f1w=0.982]\n",
      "Epoch 7:  95%|█████████▍| 274/289 [01:59<00:06,  2.29it/s, loss=0.00321, v_num=1666580, train_F1w=1.000, val_loss=0.0653, val_f1w=0.982]\n",
      "Epoch 7:  96%|█████████▌| 276/289 [01:59<00:05,  2.31it/s, loss=0.00321, v_num=1666580, train_F1w=1.000, val_loss=0.0653, val_f1w=0.982]\n",
      "Epoch 7:  96%|█████████▌| 278/289 [01:59<00:04,  2.32it/s, loss=0.00321, v_num=1666580, train_F1w=1.000, val_loss=0.0653, val_f1w=0.982]\n",
      "Epoch 7:  97%|█████████▋| 280/289 [01:59<00:03,  2.33it/s, loss=0.00321, v_num=1666580, train_F1w=1.000, val_loss=0.0653, val_f1w=0.982]\n",
      "Epoch 7:  98%|█████████▊| 282/289 [02:00<00:02,  2.35it/s, loss=0.00321, v_num=1666580, train_F1w=1.000, val_loss=0.0653, val_f1w=0.982]\n",
      "Epoch 7:  98%|█████████▊| 284/289 [02:00<00:02,  2.36it/s, loss=0.00321, v_num=1666580, train_F1w=1.000, val_loss=0.0653, val_f1w=0.982]\n",
      "Epoch 7:  99%|█████████▉| 286/289 [02:00<00:01,  2.37it/s, loss=0.00321, v_num=1666580, train_F1w=1.000, val_loss=0.0653, val_f1w=0.982]\n",
      "Epoch 7: 100%|█████████▉| 288/289 [02:00<00:00,  2.39it/s, loss=0.00321, v_num=1666580, train_F1w=1.000, val_loss=0.0653, val_f1w=0.982]\n",
      "Epoch 7: 100%|██████████| 289/289 [02:00<00:00,  2.39it/s, loss=0.00321, v_num=1666580, train_F1w=1.000, val_loss=0.0587, val_f1w=0.982]\n",
      "Epoch 7: 100%|██████████| 289/289 [02:00<00:00,  2.39it/s, loss=0.00321, v_num=1666580, train_F1w=1.000, val_loss=0.0587, val_f1w=0.982]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 1911: avg_val_f1w was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:  82%|████████▏ | 238/289 [01:56<00:25,  2.04it/s, loss=0.00123, v_num=1666580, train_F1w=1.000, val_loss=0.0587, val_f1w=0.982]Epoch     9: adjusting learning rate of group 0 to 2.5447e-06.\n",
      "Epoch 8:  83%|████████▎ | 240/289 [01:57<00:23,  2.05it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0587, val_f1w=0.982]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8:  84%|████████▎ | 242/289 [01:57<00:22,  2.06it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0587, val_f1w=0.982]\n",
      "Epoch 8:  84%|████████▍ | 244/289 [01:57<00:21,  2.08it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0587, val_f1w=0.982]\n",
      "Epoch 8:  85%|████████▌ | 246/289 [01:57<00:20,  2.09it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0587, val_f1w=0.982]\n",
      "Epoch 8:  86%|████████▌ | 248/289 [01:57<00:19,  2.11it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0587, val_f1w=0.982]\n",
      "Epoch 8:  87%|████████▋ | 250/289 [01:57<00:18,  2.12it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0587, val_f1w=0.982]\n",
      "Epoch 8:  87%|████████▋ | 252/289 [01:57<00:17,  2.14it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0587, val_f1w=0.982]\n",
      "Epoch 8:  88%|████████▊ | 254/289 [01:58<00:16,  2.15it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0587, val_f1w=0.982]\n",
      "Epoch 8:  89%|████████▊ | 256/289 [01:58<00:15,  2.17it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0587, val_f1w=0.982]\n",
      "Epoch 8:  89%|████████▉ | 258/289 [01:58<00:14,  2.18it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0587, val_f1w=0.982]\n",
      "Epoch 8:  90%|████████▉ | 260/289 [01:58<00:13,  2.19it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0587, val_f1w=0.982]\n",
      "Epoch 8:  91%|█████████ | 262/289 [01:58<00:12,  2.21it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0587, val_f1w=0.982]\n",
      "Epoch 8:  91%|█████████▏| 264/289 [01:58<00:11,  2.22it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0587, val_f1w=0.982]\n",
      "Epoch 8:  92%|█████████▏| 266/289 [01:59<00:10,  2.24it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0587, val_f1w=0.982]\n",
      "Epoch 8:  93%|█████████▎| 268/289 [01:59<00:09,  2.25it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0587, val_f1w=0.982]\n",
      "Epoch 8:  93%|█████████▎| 270/289 [01:59<00:08,  2.26it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0587, val_f1w=0.982]\n",
      "Epoch 8:  94%|█████████▍| 272/289 [01:59<00:07,  2.28it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0587, val_f1w=0.982]\n",
      "Epoch 8:  95%|█████████▍| 274/289 [01:59<00:06,  2.29it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0587, val_f1w=0.982]\n",
      "Epoch 8:  96%|█████████▌| 276/289 [01:59<00:05,  2.30it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0587, val_f1w=0.982]\n",
      "Epoch 8:  96%|█████████▌| 278/289 [01:59<00:04,  2.32it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0587, val_f1w=0.982]\n",
      "Epoch 8:  97%|█████████▋| 280/289 [02:00<00:03,  2.33it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0587, val_f1w=0.982]\n",
      "Epoch 8:  98%|█████████▊| 282/289 [02:00<00:02,  2.34it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0587, val_f1w=0.982]\n",
      "Epoch 8:  98%|█████████▊| 284/289 [02:00<00:02,  2.36it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0587, val_f1w=0.982]\n",
      "Epoch 8:  99%|█████████▉| 286/289 [02:00<00:01,  2.37it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0587, val_f1w=0.982]\n",
      "Epoch 8: 100%|█████████▉| 288/289 [02:00<00:00,  2.39it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0587, val_f1w=0.982]\n",
      "Epoch 8: 100%|██████████| 289/289 [02:00<00:00,  2.39it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0582, val_f1w=0.985]\n",
      "Epoch 8: 100%|██████████| 289/289 [02:00<00:00,  2.39it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0582, val_f1w=0.985]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 2150: avg_val_f1w was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:  82%|████████▏ | 238/289 [01:56<00:25,  2.04it/s, loss=0.00176, v_num=1666580, train_F1w=1.000, val_loss=0.0582, val_f1w=0.985] Epoch    10: adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 9:  83%|████████▎ | 240/289 [01:57<00:23,  2.05it/s, loss=0.00173, v_num=1666580, train_F1w=1.000, val_loss=0.0582, val_f1w=0.985]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9:  84%|████████▎ | 242/289 [01:57<00:22,  2.06it/s, loss=0.00173, v_num=1666580, train_F1w=1.000, val_loss=0.0582, val_f1w=0.985]\n",
      "Epoch 9:  84%|████████▍ | 244/289 [01:57<00:21,  2.08it/s, loss=0.00173, v_num=1666580, train_F1w=1.000, val_loss=0.0582, val_f1w=0.985]\n",
      "Epoch 9:  85%|████████▌ | 246/289 [01:57<00:20,  2.09it/s, loss=0.00173, v_num=1666580, train_F1w=1.000, val_loss=0.0582, val_f1w=0.985]\n",
      "Epoch 9:  86%|████████▌ | 248/289 [01:57<00:19,  2.10it/s, loss=0.00173, v_num=1666580, train_F1w=1.000, val_loss=0.0582, val_f1w=0.985]\n",
      "Epoch 9:  87%|████████▋ | 250/289 [01:58<00:18,  2.12it/s, loss=0.00173, v_num=1666580, train_F1w=1.000, val_loss=0.0582, val_f1w=0.985]\n",
      "Epoch 9:  87%|████████▋ | 252/289 [01:58<00:17,  2.13it/s, loss=0.00173, v_num=1666580, train_F1w=1.000, val_loss=0.0582, val_f1w=0.985]\n",
      "Epoch 9:  88%|████████▊ | 254/289 [01:58<00:16,  2.15it/s, loss=0.00173, v_num=1666580, train_F1w=1.000, val_loss=0.0582, val_f1w=0.985]\n",
      "Epoch 9:  89%|████████▊ | 256/289 [01:58<00:15,  2.16it/s, loss=0.00173, v_num=1666580, train_F1w=1.000, val_loss=0.0582, val_f1w=0.985]\n",
      "Epoch 9:  89%|████████▉ | 258/289 [01:58<00:14,  2.17it/s, loss=0.00173, v_num=1666580, train_F1w=1.000, val_loss=0.0582, val_f1w=0.985]\n",
      "Epoch 9:  90%|████████▉ | 260/289 [01:58<00:13,  2.19it/s, loss=0.00173, v_num=1666580, train_F1w=1.000, val_loss=0.0582, val_f1w=0.985]\n",
      "Epoch 9:  91%|█████████ | 262/289 [01:58<00:12,  2.20it/s, loss=0.00173, v_num=1666580, train_F1w=1.000, val_loss=0.0582, val_f1w=0.985]\n",
      "Epoch 9:  91%|█████████▏| 264/289 [01:59<00:11,  2.22it/s, loss=0.00173, v_num=1666580, train_F1w=1.000, val_loss=0.0582, val_f1w=0.985]\n",
      "Epoch 9:  92%|█████████▏| 266/289 [01:59<00:10,  2.23it/s, loss=0.00173, v_num=1666580, train_F1w=1.000, val_loss=0.0582, val_f1w=0.985]\n",
      "Epoch 9:  93%|█████████▎| 268/289 [01:59<00:09,  2.24it/s, loss=0.00173, v_num=1666580, train_F1w=1.000, val_loss=0.0582, val_f1w=0.985]\n",
      "Epoch 9:  93%|█████████▎| 270/289 [01:59<00:08,  2.26it/s, loss=0.00173, v_num=1666580, train_F1w=1.000, val_loss=0.0582, val_f1w=0.985]\n",
      "Epoch 9:  94%|█████████▍| 272/289 [01:59<00:07,  2.27it/s, loss=0.00173, v_num=1666580, train_F1w=1.000, val_loss=0.0582, val_f1w=0.985]\n",
      "Epoch 9:  95%|█████████▍| 274/289 [01:59<00:06,  2.29it/s, loss=0.00173, v_num=1666580, train_F1w=1.000, val_loss=0.0582, val_f1w=0.985]\n",
      "Epoch 9:  96%|█████████▌| 276/289 [02:00<00:05,  2.30it/s, loss=0.00173, v_num=1666580, train_F1w=1.000, val_loss=0.0582, val_f1w=0.985]\n",
      "Epoch 9:  96%|█████████▌| 278/289 [02:00<00:04,  2.31it/s, loss=0.00173, v_num=1666580, train_F1w=1.000, val_loss=0.0582, val_f1w=0.985]\n",
      "Epoch 9:  97%|█████████▋| 280/289 [02:00<00:03,  2.33it/s, loss=0.00173, v_num=1666580, train_F1w=1.000, val_loss=0.0582, val_f1w=0.985]\n",
      "Epoch 9:  98%|█████████▊| 282/289 [02:00<00:02,  2.34it/s, loss=0.00173, v_num=1666580, train_F1w=1.000, val_loss=0.0582, val_f1w=0.985]\n",
      "Epoch 9:  98%|█████████▊| 284/289 [02:00<00:02,  2.35it/s, loss=0.00173, v_num=1666580, train_F1w=1.000, val_loss=0.0582, val_f1w=0.985]\n",
      "Epoch 9:  99%|█████████▉| 286/289 [02:00<00:01,  2.37it/s, loss=0.00173, v_num=1666580, train_F1w=1.000, val_loss=0.0582, val_f1w=0.985]\n",
      "Epoch 9: 100%|█████████▉| 288/289 [02:01<00:00,  2.38it/s, loss=0.00173, v_num=1666580, train_F1w=1.000, val_loss=0.0582, val_f1w=0.985]\n",
      "Epoch 9: 100%|██████████| 289/289 [02:01<00:00,  2.39it/s, loss=0.00173, v_num=1666580, train_F1w=1.000, val_loss=0.0573, val_f1w=0.983]\n",
      "Epoch 9: 100%|██████████| 289/289 [02:01<00:00,  2.39it/s, loss=0.00173, v_num=1666580, train_F1w=1.000, val_loss=0.0573, val_f1w=0.983]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 2389: avg_val_f1w was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:  82%|████████▏ | 238/289 [01:57<00:25,  2.03it/s, loss=0.0941, v_num=1666580, train_F1w=0.969, val_loss=0.0573, val_f1w=0.983] Epoch    11: adjusting learning rate of group 0 to 9.7555e-05.\n",
      "Epoch 10:  83%|████████▎ | 240/289 [01:57<00:23,  2.04it/s, loss=0.0952, v_num=1666580, train_F1w=1.000, val_loss=0.0573, val_f1w=0.983]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 10:  84%|████████▎ | 242/289 [01:57<00:22,  2.06it/s, loss=0.0952, v_num=1666580, train_F1w=1.000, val_loss=0.0573, val_f1w=0.983]\n",
      "Epoch 10:  84%|████████▍ | 244/289 [01:57<00:21,  2.07it/s, loss=0.0952, v_num=1666580, train_F1w=1.000, val_loss=0.0573, val_f1w=0.983]\n",
      "Epoch 10:  85%|████████▌ | 246/289 [01:57<00:20,  2.08it/s, loss=0.0952, v_num=1666580, train_F1w=1.000, val_loss=0.0573, val_f1w=0.983]\n",
      "Epoch 10:  86%|████████▌ | 248/289 [01:58<00:19,  2.10it/s, loss=0.0952, v_num=1666580, train_F1w=1.000, val_loss=0.0573, val_f1w=0.983]\n",
      "Epoch 10:  87%|████████▋ | 250/289 [01:58<00:18,  2.11it/s, loss=0.0952, v_num=1666580, train_F1w=1.000, val_loss=0.0573, val_f1w=0.983]\n",
      "Epoch 10:  87%|████████▋ | 252/289 [01:58<00:17,  2.13it/s, loss=0.0952, v_num=1666580, train_F1w=1.000, val_loss=0.0573, val_f1w=0.983]\n",
      "Epoch 10:  88%|████████▊ | 254/289 [01:58<00:16,  2.14it/s, loss=0.0952, v_num=1666580, train_F1w=1.000, val_loss=0.0573, val_f1w=0.983]\n",
      "Epoch 10:  89%|████████▊ | 256/289 [01:58<00:15,  2.16it/s, loss=0.0952, v_num=1666580, train_F1w=1.000, val_loss=0.0573, val_f1w=0.983]\n",
      "Epoch 10:  89%|████████▉ | 258/289 [01:58<00:14,  2.17it/s, loss=0.0952, v_num=1666580, train_F1w=1.000, val_loss=0.0573, val_f1w=0.983]\n",
      "Epoch 10:  90%|████████▉ | 260/289 [01:58<00:13,  2.18it/s, loss=0.0952, v_num=1666580, train_F1w=1.000, val_loss=0.0573, val_f1w=0.983]\n",
      "Epoch 10:  91%|█████████ | 262/289 [01:59<00:12,  2.20it/s, loss=0.0952, v_num=1666580, train_F1w=1.000, val_loss=0.0573, val_f1w=0.983]\n",
      "Epoch 10:  91%|█████████▏| 264/289 [01:59<00:11,  2.21it/s, loss=0.0952, v_num=1666580, train_F1w=1.000, val_loss=0.0573, val_f1w=0.983]\n",
      "Epoch 10:  92%|█████████▏| 266/289 [01:59<00:10,  2.23it/s, loss=0.0952, v_num=1666580, train_F1w=1.000, val_loss=0.0573, val_f1w=0.983]\n",
      "Epoch 10:  93%|█████████▎| 268/289 [01:59<00:09,  2.24it/s, loss=0.0952, v_num=1666580, train_F1w=1.000, val_loss=0.0573, val_f1w=0.983]\n",
      "Epoch 10:  93%|█████████▎| 270/289 [01:59<00:08,  2.26it/s, loss=0.0952, v_num=1666580, train_F1w=1.000, val_loss=0.0573, val_f1w=0.983]\n",
      "Epoch 10:  94%|█████████▍| 272/289 [01:59<00:07,  2.27it/s, loss=0.0952, v_num=1666580, train_F1w=1.000, val_loss=0.0573, val_f1w=0.983]\n",
      "Epoch 10:  95%|█████████▍| 274/289 [02:00<00:06,  2.28it/s, loss=0.0952, v_num=1666580, train_F1w=1.000, val_loss=0.0573, val_f1w=0.983]\n",
      "Epoch 10:  96%|█████████▌| 276/289 [02:00<00:05,  2.30it/s, loss=0.0952, v_num=1666580, train_F1w=1.000, val_loss=0.0573, val_f1w=0.983]\n",
      "Epoch 10:  96%|█████████▌| 278/289 [02:00<00:04,  2.31it/s, loss=0.0952, v_num=1666580, train_F1w=1.000, val_loss=0.0573, val_f1w=0.983]\n",
      "Epoch 10:  97%|█████████▋| 280/289 [02:00<00:03,  2.32it/s, loss=0.0952, v_num=1666580, train_F1w=1.000, val_loss=0.0573, val_f1w=0.983]\n",
      "Epoch 10:  98%|█████████▊| 282/289 [02:00<00:02,  2.34it/s, loss=0.0952, v_num=1666580, train_F1w=1.000, val_loss=0.0573, val_f1w=0.983]\n",
      "Epoch 10:  98%|█████████▊| 284/289 [02:00<00:02,  2.35it/s, loss=0.0952, v_num=1666580, train_F1w=1.000, val_loss=0.0573, val_f1w=0.983]\n",
      "Epoch 10:  99%|█████████▉| 286/289 [02:00<00:01,  2.36it/s, loss=0.0952, v_num=1666580, train_F1w=1.000, val_loss=0.0573, val_f1w=0.983]\n",
      "Epoch 10: 100%|█████████▉| 288/289 [02:01<00:00,  2.38it/s, loss=0.0952, v_num=1666580, train_F1w=1.000, val_loss=0.0573, val_f1w=0.983]\n",
      "Epoch 10: 100%|██████████| 289/289 [02:01<00:00,  2.38it/s, loss=0.0952, v_num=1666580, train_F1w=1.000, val_loss=0.160, val_f1w=0.947] \n",
      "Epoch 10: 100%|██████████| 289/289 [02:01<00:00,  2.38it/s, loss=0.0952, v_num=1666580, train_F1w=1.000, val_loss=0.160, val_f1w=0.947]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 2628: avg_val_f1w was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 289/289 [02:01<00:00,  2.38it/s, loss=0.0952, v_num=1666580, train_F1w=1.000, val_loss=0.160, val_f1w=0.947]\n"
     ]
    }
   ],
   "source": [
    "## Sanity-check\n",
    "trainer = pl.Trainer(gpus=gpu,\n",
    "                     callbacks=[checkpoint_callback, early_stop_callback],\n",
    "                    #  deterministic=True,\n",
    "                     enable_progress_bar=True,\n",
    "                    #  progress_bar_\n",
    "                    #  limit_train_batches=2,\n",
    "                    #  limit_val_batches=2,\n",
    "                     max_epochs=20)\n",
    "\n",
    "# Then you call the fit method passing the PL-module and PL-Datamodule, to start training.\n",
    "trainer.fit(baseline_model, dm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cf257649-5774-455c-9cae-1af74f57c804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_epoch_loss[1:])\n",
    "plt.plot(val_epoch_loss[1:])\n",
    "\n",
    "plt.legend(['Train', 'Val'])\n",
    "plt.title('Loss Performance')\n",
    "plt.xlabel('epochs')\n",
    "plt.savefig('loss_performace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "146e4d67-a91a-42f8-b505-44fab40be663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "                 Pinus       0.00      0.00      0.00        28\n",
      "               Erica.m       0.26      1.00      0.41       184\n",
      "             Cistus sp       0.00      0.00      0.00        69\n",
      "             Lavandula       0.00      0.00      0.00        74\n",
      "             Citrus sp       0.00      0.00      0.00        53\n",
      "     Helianthus annuus       0.00      0.00      0.00        91\n",
      "        Eucalyptus sp.       0.00      0.00      0.00        95\n",
      "Rosmarinus officinalis       0.00      0.00      0.00        52\n",
      "              Brassica       0.43      0.57      0.49       140\n",
      "                Cardus       0.00      0.00      0.00        19\n",
      "                 Tilia       0.00      0.00      0.00        67\n",
      "             Taraxacum       0.00      0.00      0.00        25\n",
      "\n",
      "              accuracy                           0.29       897\n",
      "             macro avg       0.06      0.13      0.08       897\n",
      "          weighted avg       0.12      0.29      0.16       897\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Base-epoch=5-val_loss=0.06-avg_val_f1w=0.99.ckpt\" #\"Base-epoch=7-val_loss=0.05-avg_val_f1w=0.99.ckpt\"\n",
    "baseline_model  = LitModel(model=resnet_model)\n",
    "# /mnt/gpid08/datasets/remote_sensing/tmp_from_gpid07/honey/results/Base-epoch=7-val_loss=0.06-avg_val_f1w=0.98.ckpt\n",
    "\n",
    "checkpoint =  torch.load('/mnt/gpid08/datasets/remote_sensing/tmp_from_gpid07/honey/results/'+ model_name)\n",
    "\n",
    "baseline_model.load_state_dict(checkpoint['state_dict'])\n",
    "# baseline_model.load_from_checkpoint(\"Base-epoch=1-val_loss=0.05-avg_val_f1w=0.98.ckpt\")\n",
    "\n",
    "targets, preds = [],[]\n",
    "baseline_model.to('cuda')\n",
    "for ii, data in enumerate(dm.test_dataloader()):\n",
    "    with torch.no_grad():\n",
    "        targets.append(data['target'].numpy()) #torch.argmax(data['target'],dim=1).numpy()\n",
    "        # y = y.reshape((-1,1))\n",
    "        # print(y.shape)\n",
    "        ## inference\n",
    "        o = baseline_model.predict(data['image'].to('cuda')).cpu().numpy()\n",
    "        # print(o.shape)\n",
    "        preds.append(o)\n",
    "        # print(y)\n",
    "        # print(o)\n",
    "        # if ii==2:\n",
    "        #   break\n",
    "# print(ii)\n",
    "preds2 = np.vstack([x for x in preds]).reshape(-1,1)\n",
    "targets2 = np.vstack([x for x in targets]).reshape(-1,1)\n",
    "target_names = ['Pinus','Erica.m', 'Cistus sp', 'Lavandula', 'Citrus sp', 'Helianthus annuus',\n",
    "          'Eucalyptus sp.', 'Rosmarinus officinalis', 'Brassica', 'Cardus', 'Tilia', 'Taraxacum']\n",
    "\n",
    "print(classification_report(targets2, preds2, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688aab11-6ea4-45e5-beea-1defa9174662",
   "metadata": {},
   "source": [
    "## Adding Weights to the cross Entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3da9842e-6346-4e9f-950e-4525cc07c463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>weights</th>\n",
       "      <th>weights_norm</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Brassica</th>\n",
       "      <td>1198</td>\n",
       "      <td>0.000851</td>\n",
       "      <td>0.029288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cardus</th>\n",
       "      <td>161</td>\n",
       "      <td>0.006335</td>\n",
       "      <td>0.217930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cistus sp</th>\n",
       "      <td>589</td>\n",
       "      <td>0.001732</td>\n",
       "      <td>0.059570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Citrus sp</th>\n",
       "      <td>447</td>\n",
       "      <td>0.002282</td>\n",
       "      <td>0.078494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Erica.m</th>\n",
       "      <td>1576</td>\n",
       "      <td>0.000647</td>\n",
       "      <td>0.022263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eucalyptus sp.</th>\n",
       "      <td>807</td>\n",
       "      <td>0.001264</td>\n",
       "      <td>0.043478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Helianthus annuus</th>\n",
       "      <td>780</td>\n",
       "      <td>0.001308</td>\n",
       "      <td>0.044983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lavandula</th>\n",
       "      <td>636</td>\n",
       "      <td>0.001604</td>\n",
       "      <td>0.055168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pinus</th>\n",
       "      <td>241</td>\n",
       "      <td>0.004232</td>\n",
       "      <td>0.145588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rosmarinus officinalis</th>\n",
       "      <td>444</td>\n",
       "      <td>0.002297</td>\n",
       "      <td>0.079024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Taraxacum</th>\n",
       "      <td>215</td>\n",
       "      <td>0.004744</td>\n",
       "      <td>0.163194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tilia</th>\n",
       "      <td>575</td>\n",
       "      <td>0.001774</td>\n",
       "      <td>0.061020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        name   weights weights_norm\n",
       "                       count                       \n",
       "labels                                             \n",
       "Brassica                1198  0.000851     0.029288\n",
       "Cardus                   161  0.006335     0.217930\n",
       "Cistus sp                589  0.001732     0.059570\n",
       "Citrus sp                447  0.002282     0.078494\n",
       "Erica.m                 1576  0.000647     0.022263\n",
       "Eucalyptus sp.           807  0.001264     0.043478\n",
       "Helianthus annuus        780  0.001308     0.044983\n",
       "Lavandula                636  0.001604     0.055168\n",
       "Pinus                    241  0.004232     0.145588\n",
       "Rosmarinus officinalis   444  0.002297     0.079024\n",
       "Taraxacum                215  0.004744     0.163194\n",
       "Tilia                    575  0.001774     0.061020"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_train = df_train.groupby('labels').agg({'count'})\n",
    "count_train['weights'] = 1.02/count_train\n",
    "count_train['weights_norm'] = count_train['weights']/count_train['weights'].sum()\n",
    "# count_train['weights_nor'].sum()\n",
    "count_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "44d19ae7-3b10-4ab1-ac38-9c58e6eafeea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1456, 0.0223, 0.0596, 0.0552, 0.0785, 0.0450, 0.0435, 0.0790, 0.0293,\n",
       "        0.2179, 0.0610, 0.1632])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# self.classes = ['Pinus','Erica.m', 'Cistus sp', 'Lavandula', 'Citrus sp', 'Helianthus annuus',\n",
    "#                 'Eucalyptus sp.', 'Rosmarinus officinalis', 'Brassica', 'Cardus', 'Tilia', 'Taraxacum']\n",
    "class_weights=torch.Tensor([count_train.loc['Pinus','weights_norm'].item(),\n",
    "                      count_train.loc['Erica.m','weights_norm'].item(),\n",
    "                      count_train.loc['Cistus sp','weights_norm'].item(),\n",
    "                      count_train.loc['Lavandula','weights_norm'].item(),\n",
    "                      count_train.loc['Citrus sp','weights_norm'].item(),\n",
    "                      count_train.loc['Helianthus annuus','weights_norm'].item(),\n",
    "                      count_train.loc['Eucalyptus sp.','weights_norm'].item(),\n",
    "                      count_train.loc['Rosmarinus officinalis','weights_norm'].item(),\n",
    "                      count_train.loc['Brassica','weights_norm'].item(),\n",
    "                      count_train.loc['Cardus','weights_norm'].item(),\n",
    "                      count_train.loc['Tilia','weights_norm'].item(),\n",
    "                      count_train.loc['Taraxacum','weights_norm'].item(),\n",
    "                      ])\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "35a0a0aa-e606-459a-afb2-86df3cbd756f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_epoch_loss_CE, val_epoch_acc_CE = [], []\n",
    "train_epoch_loss_CE, train_epoch_acc_CE = [], []\n",
    "\n",
    "class LitModel_CEW(pl.LightningModule):\n",
    "    def __init__(self, model, class_weights):\n",
    "        super().__init__( )\n",
    "        # self.save_hyperparameters()\n",
    "        self.model = model\n",
    "        self.class_weights=class_weights.to('cuda')\n",
    "        # self.class_weights\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            y_hat = self(x)\n",
    "            return torch.argmax(y_hat, axis=1)\n",
    "    def compute_loss_and_metrics(self, batch):\n",
    "        x, y = batch['image'], batch['target']\n",
    "        # print(f'X: {x.shape} \\t Y: {y.shape}')\n",
    "        y_hat = self(x)\n",
    "        # print(f'Output: {y_hat.shape}')\n",
    "        loss = F.cross_entropy(y_hat, y, weight=self.class_weights)\n",
    "        # acc = (torch.argmax(y_hat, axis=1) == y).sum().item() / y.shape[0]\n",
    "        y1 = y.detach().cpu().numpy()\n",
    "        # print(y1.shape)\n",
    "        y_hat1 = torch.argmax(y_hat, axis=1)\n",
    "        y_hat1 = y_hat1.detach().cpu().numpy()\n",
    "        # print(y_hat1.shape)\n",
    "        f1w = f1_score(y1, y_hat1, average='weighted')\n",
    "        return loss, f1w\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, f1w = self.compute_loss_and_metrics(batch)\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_F1w', f1w, prog_bar=True)\n",
    "        #print(f'Training_step: loss> {loss} acc:{acc}')\n",
    "        return {'loss':loss,'f1w':torch.tensor(f1w)}\n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_train_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        avg_train_f1w  = torch.stack([x['f1w'] for x in outputs]).mean()\n",
    "        train_epoch_loss_CE.append(avg_train_loss.item())\n",
    "        train_epoch_acc_CE.append(avg_train_f1w.item())\n",
    "        #print(f'Epoch {self.current_epoch} TrainLOSS:{avg_train_loss} TrainACC:{avg_train_acc}  ')\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, f1w = self.compute_loss_and_metrics(batch)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_f1w', f1w, prog_bar=True)\n",
    "        return {'val_loss': torch.tensor(loss.item()), 'val_f1w': torch.tensor(f1w)}\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_val_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        avg_val_f1w  = torch.stack([x['val_f1w'] for x in outputs]).mean()\n",
    "        self.log('EarlyStop_Log', avg_val_loss.detach(), on_epoch=True, sync_dist=True)\n",
    "        self.log('avg_val_f1w', avg_val_f1w.detach(), on_epoch=True, sync_dist=True)\n",
    "        val_epoch_loss_CE.append(avg_val_loss.item())\n",
    "        val_epoch_acc_CE.append(avg_val_f1w.item())\n",
    "        #print(f'VAL-Epoch {self.current_epoch} LOSS:{avg_val_loss} ACC:{avg_val_acc} ')\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.0001)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, \n",
    "                                                                    T_0=10,\n",
    "                                                                    T_mult=1,\n",
    "                                                                    eta_min=1e-7,\n",
    "                                                                    verbose=True,\n",
    "                                                                    )\n",
    "\n",
    "        # lr_scheduler = {'scheduler': MultiStepLR(optimizer, milestones=[10,20,30,40], gamma=0.5,),'interval': 'epoch','frequency':1}\n",
    "        return [optimizer], [lr_scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2ff8c466-46ff-490f-9cac-1567b8200988",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model_CEW  = LitModel_CEW(model=resnet_model, class_weights=class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8faffdb6-6e45-462e-80f1-72bbef40acb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(dirpath='/mnt/gpid08/datasets/remote_sensing/tmp_from_gpid07/honey/results/', \n",
    "                                      filename='Base-CEW-{epoch}-{val_loss:.2f}-{avg_val_f1w:.2f}',\n",
    "                                      monitor='avg_val_f1w',\n",
    "                                      verbose=True,\n",
    "                                      save_last=None,\n",
    "                                      save_top_k=1,\n",
    "                                      save_weights_only=False,\n",
    "                                      mode='max',\n",
    "                                      auto_insert_metric_name=True,\n",
    "                                      )\n",
    "\n",
    "\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor='EarlyStop_Log',\\\n",
    "                                    min_delta=0.00,\\\n",
    "                                    patience=5,\\\n",
    "                                    verbose=False,\\\n",
    "                                    mode='min')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "842d5716-5194-4129-9839-91b29dce723f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Set SLURM handle signals.\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | ResNet | 23.5 M\n",
      "---------------------------------\n",
      "23.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.130    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     0: adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 0:  82%|████████▏ | 238/289 [01:53<00:24,  2.09it/s, loss=0.0425, v_num=1666580, train_F1w=0.983]Epoch     1: adjusting learning rate of group 0 to 9.7555e-05.\n",
      "Epoch 0:  83%|████████▎ | 239/289 [01:54<00:23,  2.09it/s, loss=0.0427, v_num=1666580, train_F1w=1.000]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  83%|████████▎ | 241/289 [01:54<00:22,  2.11it/s, loss=0.0427, v_num=1666580, train_F1w=1.000]\n",
      "Epoch 0:  84%|████████▍ | 243/289 [01:54<00:21,  2.12it/s, loss=0.0427, v_num=1666580, train_F1w=1.000]\n",
      "Epoch 0:  85%|████████▍ | 245/289 [01:54<00:20,  2.14it/s, loss=0.0427, v_num=1666580, train_F1w=1.000]\n",
      "Epoch 0:  85%|████████▌ | 247/289 [01:54<00:19,  2.15it/s, loss=0.0427, v_num=1666580, train_F1w=1.000]\n",
      "Epoch 0:  86%|████████▌ | 249/289 [01:55<00:18,  2.16it/s, loss=0.0427, v_num=1666580, train_F1w=1.000]\n",
      "Epoch 0:  87%|████████▋ | 251/289 [01:55<00:17,  2.18it/s, loss=0.0427, v_num=1666580, train_F1w=1.000]\n",
      "Epoch 0:  88%|████████▊ | 253/289 [01:55<00:16,  2.19it/s, loss=0.0427, v_num=1666580, train_F1w=1.000]\n",
      "Epoch 0:  88%|████████▊ | 255/289 [01:55<00:15,  2.21it/s, loss=0.0427, v_num=1666580, train_F1w=1.000]\n",
      "Epoch 0:  89%|████████▉ | 257/289 [01:55<00:14,  2.22it/s, loss=0.0427, v_num=1666580, train_F1w=1.000]\n",
      "Epoch 0:  90%|████████▉ | 259/289 [01:55<00:13,  2.24it/s, loss=0.0427, v_num=1666580, train_F1w=1.000]\n",
      "Epoch 0:  90%|█████████ | 261/289 [01:55<00:12,  2.25it/s, loss=0.0427, v_num=1666580, train_F1w=1.000]\n",
      "Epoch 0:  91%|█████████ | 263/289 [01:56<00:11,  2.27it/s, loss=0.0427, v_num=1666580, train_F1w=1.000]\n",
      "Epoch 0:  92%|█████████▏| 265/289 [01:56<00:10,  2.28it/s, loss=0.0427, v_num=1666580, train_F1w=1.000]\n",
      "Epoch 0:  92%|█████████▏| 267/289 [01:56<00:09,  2.29it/s, loss=0.0427, v_num=1666580, train_F1w=1.000]\n",
      "Epoch 0:  93%|█████████▎| 269/289 [01:56<00:08,  2.31it/s, loss=0.0427, v_num=1666580, train_F1w=1.000]\n",
      "Epoch 0:  94%|█████████▍| 271/289 [01:56<00:07,  2.32it/s, loss=0.0427, v_num=1666580, train_F1w=1.000]\n",
      "Epoch 0:  94%|█████████▍| 273/289 [01:56<00:06,  2.34it/s, loss=0.0427, v_num=1666580, train_F1w=1.000]\n",
      "Epoch 0:  95%|█████████▌| 275/289 [01:57<00:05,  2.35it/s, loss=0.0427, v_num=1666580, train_F1w=1.000]\n",
      "Epoch 0:  96%|█████████▌| 277/289 [01:57<00:05,  2.36it/s, loss=0.0427, v_num=1666580, train_F1w=1.000]\n",
      "Epoch 0:  97%|█████████▋| 279/289 [01:57<00:04,  2.38it/s, loss=0.0427, v_num=1666580, train_F1w=1.000]\n",
      "Epoch 0:  97%|█████████▋| 281/289 [01:57<00:03,  2.39it/s, loss=0.0427, v_num=1666580, train_F1w=1.000]\n",
      "Epoch 0:  98%|█████████▊| 283/289 [01:57<00:02,  2.41it/s, loss=0.0427, v_num=1666580, train_F1w=1.000]\n",
      "Epoch 0:  99%|█████████▊| 285/289 [01:57<00:01,  2.42it/s, loss=0.0427, v_num=1666580, train_F1w=1.000]\n",
      "Epoch 0:  99%|█████████▉| 287/289 [01:57<00:00,  2.43it/s, loss=0.0427, v_num=1666580, train_F1w=1.000]\n",
      "Epoch 0: 100%|██████████| 289/289 [01:58<00:00,  2.45it/s, loss=0.0427, v_num=1666580, train_F1w=1.000, val_loss=0.0683, val_f1w=0.983]\n",
      "Epoch 0: 100%|██████████| 289/289 [01:58<00:00,  2.45it/s, loss=0.0427, v_num=1666580, train_F1w=1.000, val_loss=0.0683, val_f1w=0.983]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 238: avg_val_f1w reached 0.98317 (best 0.98317), saving model to \"/mnt/gpid08/datasets/remote_sensing/tmp_from_gpid07/honey/results/Base-CEW-epoch=0-val_loss=0.07-avg_val_f1w=0.98.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  82%|████████▏ | 238/289 [01:52<00:24,  2.11it/s, loss=0.0182, v_num=1666580, train_F1w=0.981, val_loss=0.0683, val_f1w=0.983]Epoch     2: adjusting learning rate of group 0 to 9.0460e-05.\n",
      "Epoch 1:  83%|████████▎ | 240/289 [01:52<00:23,  2.13it/s, loss=0.0193, v_num=1666580, train_F1w=1.000, val_loss=0.0683, val_f1w=0.983]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  84%|████████▎ | 242/289 [01:53<00:21,  2.14it/s, loss=0.0193, v_num=1666580, train_F1w=1.000, val_loss=0.0683, val_f1w=0.983]\n",
      "Epoch 1:  84%|████████▍ | 244/289 [01:53<00:20,  2.16it/s, loss=0.0193, v_num=1666580, train_F1w=1.000, val_loss=0.0683, val_f1w=0.983]\n",
      "Epoch 1:  85%|████████▌ | 246/289 [01:53<00:19,  2.17it/s, loss=0.0193, v_num=1666580, train_F1w=1.000, val_loss=0.0683, val_f1w=0.983]\n",
      "Epoch 1:  86%|████████▌ | 248/289 [01:53<00:18,  2.19it/s, loss=0.0193, v_num=1666580, train_F1w=1.000, val_loss=0.0683, val_f1w=0.983]\n",
      "Epoch 1:  87%|████████▋ | 250/289 [01:53<00:17,  2.20it/s, loss=0.0193, v_num=1666580, train_F1w=1.000, val_loss=0.0683, val_f1w=0.983]\n",
      "Epoch 1:  87%|████████▋ | 252/289 [01:53<00:16,  2.22it/s, loss=0.0193, v_num=1666580, train_F1w=1.000, val_loss=0.0683, val_f1w=0.983]\n",
      "Epoch 1:  88%|████████▊ | 254/289 [01:53<00:15,  2.23it/s, loss=0.0193, v_num=1666580, train_F1w=1.000, val_loss=0.0683, val_f1w=0.983]\n",
      "Epoch 1:  89%|████████▊ | 256/289 [01:53<00:14,  2.25it/s, loss=0.0193, v_num=1666580, train_F1w=1.000, val_loss=0.0683, val_f1w=0.983]\n",
      "Epoch 1:  89%|████████▉ | 258/289 [01:54<00:13,  2.26it/s, loss=0.0193, v_num=1666580, train_F1w=1.000, val_loss=0.0683, val_f1w=0.983]\n",
      "Epoch 1:  90%|████████▉ | 260/289 [01:54<00:12,  2.28it/s, loss=0.0193, v_num=1666580, train_F1w=1.000, val_loss=0.0683, val_f1w=0.983]\n",
      "Epoch 1:  91%|█████████ | 262/289 [01:54<00:11,  2.29it/s, loss=0.0193, v_num=1666580, train_F1w=1.000, val_loss=0.0683, val_f1w=0.983]\n",
      "Epoch 1:  91%|█████████▏| 264/289 [01:54<00:10,  2.31it/s, loss=0.0193, v_num=1666580, train_F1w=1.000, val_loss=0.0683, val_f1w=0.983]\n",
      "Epoch 1:  92%|█████████▏| 266/289 [01:54<00:09,  2.32it/s, loss=0.0193, v_num=1666580, train_F1w=1.000, val_loss=0.0683, val_f1w=0.983]\n",
      "Epoch 1:  93%|█████████▎| 268/289 [01:54<00:08,  2.33it/s, loss=0.0193, v_num=1666580, train_F1w=1.000, val_loss=0.0683, val_f1w=0.983]\n",
      "Epoch 1:  93%|█████████▎| 270/289 [01:54<00:08,  2.35it/s, loss=0.0193, v_num=1666580, train_F1w=1.000, val_loss=0.0683, val_f1w=0.983]\n",
      "Epoch 1:  94%|█████████▍| 272/289 [01:55<00:07,  2.36it/s, loss=0.0193, v_num=1666580, train_F1w=1.000, val_loss=0.0683, val_f1w=0.983]\n",
      "Epoch 1:  95%|█████████▍| 274/289 [01:55<00:06,  2.38it/s, loss=0.0193, v_num=1666580, train_F1w=1.000, val_loss=0.0683, val_f1w=0.983]\n",
      "Epoch 1:  96%|█████████▌| 276/289 [01:55<00:05,  2.39it/s, loss=0.0193, v_num=1666580, train_F1w=1.000, val_loss=0.0683, val_f1w=0.983]\n",
      "Epoch 1:  96%|█████████▌| 278/289 [01:55<00:04,  2.41it/s, loss=0.0193, v_num=1666580, train_F1w=1.000, val_loss=0.0683, val_f1w=0.983]\n",
      "Epoch 1:  97%|█████████▋| 280/289 [01:55<00:03,  2.42it/s, loss=0.0193, v_num=1666580, train_F1w=1.000, val_loss=0.0683, val_f1w=0.983]\n",
      "Epoch 1:  98%|█████████▊| 282/289 [01:55<00:02,  2.44it/s, loss=0.0193, v_num=1666580, train_F1w=1.000, val_loss=0.0683, val_f1w=0.983]\n",
      "Epoch 1:  98%|█████████▊| 284/289 [01:55<00:02,  2.45it/s, loss=0.0193, v_num=1666580, train_F1w=1.000, val_loss=0.0683, val_f1w=0.983]\n",
      "Epoch 1:  99%|█████████▉| 286/289 [01:56<00:01,  2.46it/s, loss=0.0193, v_num=1666580, train_F1w=1.000, val_loss=0.0683, val_f1w=0.983]\n",
      "Epoch 1: 100%|█████████▉| 288/289 [01:56<00:00,  2.48it/s, loss=0.0193, v_num=1666580, train_F1w=1.000, val_loss=0.0683, val_f1w=0.983]\n",
      "Epoch 1: 100%|██████████| 289/289 [01:56<00:00,  2.48it/s, loss=0.0193, v_num=1666580, train_F1w=1.000, val_loss=0.103, val_f1w=0.968] \n",
      "Epoch 1: 100%|██████████| 289/289 [01:56<00:00,  2.48it/s, loss=0.0193, v_num=1666580, train_F1w=1.000, val_loss=0.103, val_f1w=0.968]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 477: avg_val_f1w was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  82%|████████▏ | 238/289 [01:52<00:24,  2.11it/s, loss=0.0146, v_num=1666580, train_F1w=0.973, val_loss=0.103, val_f1w=0.968]Epoch     3: adjusting learning rate of group 0 to 7.9410e-05.\n",
      "Epoch 2:  83%|████████▎ | 240/289 [01:53<00:23,  2.12it/s, loss=0.0154, v_num=1666580, train_F1w=0.973, val_loss=0.103, val_f1w=0.968]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  84%|████████▎ | 242/289 [01:53<00:22,  2.13it/s, loss=0.0154, v_num=1666580, train_F1w=0.973, val_loss=0.103, val_f1w=0.968]\n",
      "Epoch 2:  84%|████████▍ | 244/289 [01:53<00:20,  2.15it/s, loss=0.0154, v_num=1666580, train_F1w=0.973, val_loss=0.103, val_f1w=0.968]\n",
      "Epoch 2:  85%|████████▌ | 246/289 [01:53<00:19,  2.16it/s, loss=0.0154, v_num=1666580, train_F1w=0.973, val_loss=0.103, val_f1w=0.968]\n",
      "Epoch 2:  86%|████████▌ | 248/289 [01:53<00:18,  2.18it/s, loss=0.0154, v_num=1666580, train_F1w=0.973, val_loss=0.103, val_f1w=0.968]\n",
      "Epoch 2:  87%|████████▋ | 250/289 [01:54<00:17,  2.19it/s, loss=0.0154, v_num=1666580, train_F1w=0.973, val_loss=0.103, val_f1w=0.968]\n",
      "Epoch 2:  87%|████████▋ | 252/289 [01:54<00:16,  2.21it/s, loss=0.0154, v_num=1666580, train_F1w=0.973, val_loss=0.103, val_f1w=0.968]\n",
      "Epoch 2:  88%|████████▊ | 254/289 [01:54<00:15,  2.22it/s, loss=0.0154, v_num=1666580, train_F1w=0.973, val_loss=0.103, val_f1w=0.968]\n",
      "Epoch 2:  89%|████████▊ | 256/289 [01:54<00:14,  2.23it/s, loss=0.0154, v_num=1666580, train_F1w=0.973, val_loss=0.103, val_f1w=0.968]\n",
      "Epoch 2:  89%|████████▉ | 258/289 [01:54<00:13,  2.25it/s, loss=0.0154, v_num=1666580, train_F1w=0.973, val_loss=0.103, val_f1w=0.968]\n",
      "Epoch 2:  90%|████████▉ | 260/289 [01:54<00:12,  2.26it/s, loss=0.0154, v_num=1666580, train_F1w=0.973, val_loss=0.103, val_f1w=0.968]\n",
      "Epoch 2:  91%|█████████ | 262/289 [01:55<00:11,  2.28it/s, loss=0.0154, v_num=1666580, train_F1w=0.973, val_loss=0.103, val_f1w=0.968]\n",
      "Epoch 2:  91%|█████████▏| 264/289 [01:55<00:10,  2.29it/s, loss=0.0154, v_num=1666580, train_F1w=0.973, val_loss=0.103, val_f1w=0.968]\n",
      "Epoch 2:  92%|█████████▏| 266/289 [01:55<00:09,  2.31it/s, loss=0.0154, v_num=1666580, train_F1w=0.973, val_loss=0.103, val_f1w=0.968]\n",
      "Epoch 2:  93%|█████████▎| 268/289 [01:55<00:09,  2.32it/s, loss=0.0154, v_num=1666580, train_F1w=0.973, val_loss=0.103, val_f1w=0.968]\n",
      "Epoch 2:  93%|█████████▎| 270/289 [01:55<00:08,  2.33it/s, loss=0.0154, v_num=1666580, train_F1w=0.973, val_loss=0.103, val_f1w=0.968]\n",
      "Epoch 2:  94%|█████████▍| 272/289 [01:55<00:07,  2.35it/s, loss=0.0154, v_num=1666580, train_F1w=0.973, val_loss=0.103, val_f1w=0.968]\n",
      "Epoch 2:  95%|█████████▍| 274/289 [01:55<00:06,  2.36it/s, loss=0.0154, v_num=1666580, train_F1w=0.973, val_loss=0.103, val_f1w=0.968]\n",
      "Epoch 2:  96%|█████████▌| 276/289 [01:56<00:05,  2.38it/s, loss=0.0154, v_num=1666580, train_F1w=0.973, val_loss=0.103, val_f1w=0.968]\n",
      "Epoch 2:  96%|█████████▌| 278/289 [01:56<00:04,  2.39it/s, loss=0.0154, v_num=1666580, train_F1w=0.973, val_loss=0.103, val_f1w=0.968]\n",
      "Epoch 2:  97%|█████████▋| 280/289 [01:56<00:03,  2.40it/s, loss=0.0154, v_num=1666580, train_F1w=0.973, val_loss=0.103, val_f1w=0.968]\n",
      "Epoch 2:  98%|█████████▊| 282/289 [01:56<00:02,  2.42it/s, loss=0.0154, v_num=1666580, train_F1w=0.973, val_loss=0.103, val_f1w=0.968]\n",
      "Epoch 2:  98%|█████████▊| 284/289 [01:56<00:02,  2.43it/s, loss=0.0154, v_num=1666580, train_F1w=0.973, val_loss=0.103, val_f1w=0.968]\n",
      "Epoch 2:  99%|█████████▉| 286/289 [01:56<00:01,  2.45it/s, loss=0.0154, v_num=1666580, train_F1w=0.973, val_loss=0.103, val_f1w=0.968]\n",
      "Epoch 2: 100%|█████████▉| 288/289 [01:57<00:00,  2.46it/s, loss=0.0154, v_num=1666580, train_F1w=0.973, val_loss=0.103, val_f1w=0.968]\n",
      "Epoch 2: 100%|██████████| 289/289 [01:57<00:00,  2.47it/s, loss=0.0154, v_num=1666580, train_F1w=0.973, val_loss=0.0836, val_f1w=0.971]\n",
      "Epoch 2: 100%|██████████| 289/289 [01:57<00:00,  2.47it/s, loss=0.0154, v_num=1666580, train_F1w=0.973, val_loss=0.0836, val_f1w=0.971]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 716: avg_val_f1w was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  82%|████████▏ | 238/289 [01:52<00:24,  2.11it/s, loss=0.00644, v_num=1666580, train_F1w=1.000, val_loss=0.0836, val_f1w=0.971]Epoch     4: adjusting learning rate of group 0 to 6.5485e-05.\n",
      "Epoch 3:  83%|████████▎ | 240/289 [01:53<00:23,  2.12it/s, loss=0.00635, v_num=1666580, train_F1w=1.000, val_loss=0.0836, val_f1w=0.971]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  84%|████████▎ | 242/289 [01:53<00:22,  2.13it/s, loss=0.00635, v_num=1666580, train_F1w=1.000, val_loss=0.0836, val_f1w=0.971]\n",
      "Epoch 3:  84%|████████▍ | 244/289 [01:53<00:20,  2.15it/s, loss=0.00635, v_num=1666580, train_F1w=1.000, val_loss=0.0836, val_f1w=0.971]\n",
      "Epoch 3:  85%|████████▌ | 246/289 [01:53<00:19,  2.16it/s, loss=0.00635, v_num=1666580, train_F1w=1.000, val_loss=0.0836, val_f1w=0.971]\n",
      "Epoch 3:  86%|████████▌ | 248/289 [01:53<00:18,  2.18it/s, loss=0.00635, v_num=1666580, train_F1w=1.000, val_loss=0.0836, val_f1w=0.971]\n",
      "Epoch 3:  87%|████████▋ | 250/289 [01:54<00:17,  2.19it/s, loss=0.00635, v_num=1666580, train_F1w=1.000, val_loss=0.0836, val_f1w=0.971]\n",
      "Epoch 3:  87%|████████▋ | 252/289 [01:54<00:16,  2.21it/s, loss=0.00635, v_num=1666580, train_F1w=1.000, val_loss=0.0836, val_f1w=0.971]\n",
      "Epoch 3:  88%|████████▊ | 254/289 [01:54<00:15,  2.22it/s, loss=0.00635, v_num=1666580, train_F1w=1.000, val_loss=0.0836, val_f1w=0.971]\n",
      "Epoch 3:  89%|████████▊ | 256/289 [01:54<00:14,  2.24it/s, loss=0.00635, v_num=1666580, train_F1w=1.000, val_loss=0.0836, val_f1w=0.971]\n",
      "Epoch 3:  89%|████████▉ | 258/289 [01:54<00:13,  2.25it/s, loss=0.00635, v_num=1666580, train_F1w=1.000, val_loss=0.0836, val_f1w=0.971]\n",
      "Epoch 3:  90%|████████▉ | 260/289 [01:54<00:12,  2.26it/s, loss=0.00635, v_num=1666580, train_F1w=1.000, val_loss=0.0836, val_f1w=0.971]\n",
      "Epoch 3:  91%|█████████ | 262/289 [01:54<00:11,  2.28it/s, loss=0.00635, v_num=1666580, train_F1w=1.000, val_loss=0.0836, val_f1w=0.971]\n",
      "Epoch 3:  91%|█████████▏| 264/289 [01:55<00:10,  2.29it/s, loss=0.00635, v_num=1666580, train_F1w=1.000, val_loss=0.0836, val_f1w=0.971]\n",
      "Epoch 3:  92%|█████████▏| 266/289 [01:55<00:09,  2.31it/s, loss=0.00635, v_num=1666580, train_F1w=1.000, val_loss=0.0836, val_f1w=0.971]\n",
      "Epoch 3:  93%|█████████▎| 268/289 [01:55<00:09,  2.32it/s, loss=0.00635, v_num=1666580, train_F1w=1.000, val_loss=0.0836, val_f1w=0.971]\n",
      "Epoch 3:  93%|█████████▎| 270/289 [01:55<00:08,  2.34it/s, loss=0.00635, v_num=1666580, train_F1w=1.000, val_loss=0.0836, val_f1w=0.971]\n",
      "Epoch 3:  94%|█████████▍| 272/289 [01:55<00:07,  2.35it/s, loss=0.00635, v_num=1666580, train_F1w=1.000, val_loss=0.0836, val_f1w=0.971]\n",
      "Epoch 3:  95%|█████████▍| 274/289 [01:55<00:06,  2.36it/s, loss=0.00635, v_num=1666580, train_F1w=1.000, val_loss=0.0836, val_f1w=0.971]\n",
      "Epoch 3:  96%|█████████▌| 276/289 [01:56<00:05,  2.38it/s, loss=0.00635, v_num=1666580, train_F1w=1.000, val_loss=0.0836, val_f1w=0.971]\n",
      "Epoch 3:  96%|█████████▌| 278/289 [01:56<00:04,  2.39it/s, loss=0.00635, v_num=1666580, train_F1w=1.000, val_loss=0.0836, val_f1w=0.971]\n",
      "Epoch 3:  97%|█████████▋| 280/289 [01:56<00:03,  2.41it/s, loss=0.00635, v_num=1666580, train_F1w=1.000, val_loss=0.0836, val_f1w=0.971]\n",
      "Epoch 3:  98%|█████████▊| 282/289 [01:56<00:02,  2.42it/s, loss=0.00635, v_num=1666580, train_F1w=1.000, val_loss=0.0836, val_f1w=0.971]\n",
      "Epoch 3:  98%|█████████▊| 284/289 [01:56<00:02,  2.43it/s, loss=0.00635, v_num=1666580, train_F1w=1.000, val_loss=0.0836, val_f1w=0.971]\n",
      "Epoch 3:  99%|█████████▉| 286/289 [01:56<00:01,  2.45it/s, loss=0.00635, v_num=1666580, train_F1w=1.000, val_loss=0.0836, val_f1w=0.971]\n",
      "Epoch 3: 100%|█████████▉| 288/289 [01:57<00:00,  2.46it/s, loss=0.00635, v_num=1666580, train_F1w=1.000, val_loss=0.0836, val_f1w=0.971]\n",
      "Epoch 3: 100%|██████████| 289/289 [01:57<00:00,  2.47it/s, loss=0.00635, v_num=1666580, train_F1w=1.000, val_loss=0.0691, val_f1w=0.982]\n",
      "Epoch 3: 100%|██████████| 289/289 [01:57<00:00,  2.47it/s, loss=0.00635, v_num=1666580, train_F1w=1.000, val_loss=0.0691, val_f1w=0.982]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 955: avg_val_f1w was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:  82%|████████▏ | 238/289 [01:52<00:24,  2.12it/s, loss=0.00376, v_num=1666580, train_F1w=1.000, val_loss=0.0691, val_f1w=0.982]Epoch     5: adjusting learning rate of group 0 to 5.0050e-05.\n",
      "Epoch 4:  83%|████████▎ | 240/289 [01:52<00:23,  2.13it/s, loss=0.00315, v_num=1666580, train_F1w=1.000, val_loss=0.0691, val_f1w=0.982]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:  84%|████████▎ | 242/289 [01:52<00:21,  2.14it/s, loss=0.00315, v_num=1666580, train_F1w=1.000, val_loss=0.0691, val_f1w=0.982]\n",
      "Epoch 4:  84%|████████▍ | 244/289 [01:53<00:20,  2.16it/s, loss=0.00315, v_num=1666580, train_F1w=1.000, val_loss=0.0691, val_f1w=0.982]\n",
      "Epoch 4:  85%|████████▌ | 246/289 [01:53<00:19,  2.17it/s, loss=0.00315, v_num=1666580, train_F1w=1.000, val_loss=0.0691, val_f1w=0.982]\n",
      "Epoch 4:  86%|████████▌ | 248/289 [01:53<00:18,  2.19it/s, loss=0.00315, v_num=1666580, train_F1w=1.000, val_loss=0.0691, val_f1w=0.982]\n",
      "Epoch 4:  87%|████████▋ | 250/289 [01:53<00:17,  2.20it/s, loss=0.00315, v_num=1666580, train_F1w=1.000, val_loss=0.0691, val_f1w=0.982]\n",
      "Epoch 4:  87%|████████▋ | 252/289 [01:53<00:16,  2.22it/s, loss=0.00315, v_num=1666580, train_F1w=1.000, val_loss=0.0691, val_f1w=0.982]\n",
      "Epoch 4:  88%|████████▊ | 254/289 [01:53<00:15,  2.23it/s, loss=0.00315, v_num=1666580, train_F1w=1.000, val_loss=0.0691, val_f1w=0.982]\n",
      "Epoch 4:  89%|████████▊ | 256/289 [01:53<00:14,  2.25it/s, loss=0.00315, v_num=1666580, train_F1w=1.000, val_loss=0.0691, val_f1w=0.982]\n",
      "Epoch 4:  89%|████████▉ | 258/289 [01:54<00:13,  2.26it/s, loss=0.00315, v_num=1666580, train_F1w=1.000, val_loss=0.0691, val_f1w=0.982]\n",
      "Epoch 4:  90%|████████▉ | 260/289 [01:54<00:12,  2.28it/s, loss=0.00315, v_num=1666580, train_F1w=1.000, val_loss=0.0691, val_f1w=0.982]\n",
      "Epoch 4:  91%|█████████ | 262/289 [01:54<00:11,  2.29it/s, loss=0.00315, v_num=1666580, train_F1w=1.000, val_loss=0.0691, val_f1w=0.982]\n",
      "Epoch 4:  91%|█████████▏| 264/289 [01:54<00:10,  2.30it/s, loss=0.00315, v_num=1666580, train_F1w=1.000, val_loss=0.0691, val_f1w=0.982]\n",
      "Epoch 4:  92%|█████████▏| 266/289 [01:54<00:09,  2.32it/s, loss=0.00315, v_num=1666580, train_F1w=1.000, val_loss=0.0691, val_f1w=0.982]\n",
      "Epoch 4:  93%|█████████▎| 268/289 [01:54<00:09,  2.33it/s, loss=0.00315, v_num=1666580, train_F1w=1.000, val_loss=0.0691, val_f1w=0.982]\n",
      "Epoch 4:  93%|█████████▎| 270/289 [01:55<00:08,  2.35it/s, loss=0.00315, v_num=1666580, train_F1w=1.000, val_loss=0.0691, val_f1w=0.982]\n",
      "Epoch 4:  94%|█████████▍| 272/289 [01:55<00:07,  2.36it/s, loss=0.00315, v_num=1666580, train_F1w=1.000, val_loss=0.0691, val_f1w=0.982]\n",
      "Epoch 4:  95%|█████████▍| 274/289 [01:55<00:06,  2.37it/s, loss=0.00315, v_num=1666580, train_F1w=1.000, val_loss=0.0691, val_f1w=0.982]\n",
      "Epoch 4:  96%|█████████▌| 276/289 [01:55<00:05,  2.39it/s, loss=0.00315, v_num=1666580, train_F1w=1.000, val_loss=0.0691, val_f1w=0.982]\n",
      "Epoch 4:  96%|█████████▌| 278/289 [01:55<00:04,  2.40it/s, loss=0.00315, v_num=1666580, train_F1w=1.000, val_loss=0.0691, val_f1w=0.982]\n",
      "Epoch 4:  97%|█████████▋| 280/289 [01:55<00:03,  2.42it/s, loss=0.00315, v_num=1666580, train_F1w=1.000, val_loss=0.0691, val_f1w=0.982]\n",
      "Epoch 4:  98%|█████████▊| 282/289 [01:56<00:02,  2.43it/s, loss=0.00315, v_num=1666580, train_F1w=1.000, val_loss=0.0691, val_f1w=0.982]\n",
      "Epoch 4:  98%|█████████▊| 284/289 [01:56<00:02,  2.44it/s, loss=0.00315, v_num=1666580, train_F1w=1.000, val_loss=0.0691, val_f1w=0.982]\n",
      "Epoch 4:  99%|█████████▉| 286/289 [01:56<00:01,  2.46it/s, loss=0.00315, v_num=1666580, train_F1w=1.000, val_loss=0.0691, val_f1w=0.982]\n",
      "Epoch 4: 100%|█████████▉| 288/289 [01:56<00:00,  2.47it/s, loss=0.00315, v_num=1666580, train_F1w=1.000, val_loss=0.0691, val_f1w=0.982]\n",
      "Epoch 4: 100%|██████████| 289/289 [01:56<00:00,  2.48it/s, loss=0.00315, v_num=1666580, train_F1w=1.000, val_loss=0.074, val_f1w=0.984] \n",
      "Epoch 4: 100%|██████████| 289/289 [01:56<00:00,  2.48it/s, loss=0.00315, v_num=1666580, train_F1w=1.000, val_loss=0.074, val_f1w=0.984]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 1194: avg_val_f1w reached 0.98400 (best 0.98400), saving model to \"/mnt/gpid08/datasets/remote_sensing/tmp_from_gpid07/honey/results/Base-CEW-epoch=4-val_loss=0.07-avg_val_f1w=0.98.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:  82%|████████▏ | 238/289 [01:52<00:24,  2.11it/s, loss=0.00125, v_num=1666580, train_F1w=1.000, val_loss=0.074, val_f1w=0.984] Epoch     6: adjusting learning rate of group 0 to 3.4615e-05.\n",
      "Epoch 5:  83%|████████▎ | 240/289 [01:52<00:23,  2.13it/s, loss=0.00124, v_num=1666580, train_F1w=1.000, val_loss=0.074, val_f1w=0.984]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5:  84%|████████▎ | 242/289 [01:53<00:21,  2.14it/s, loss=0.00124, v_num=1666580, train_F1w=1.000, val_loss=0.074, val_f1w=0.984]\n",
      "Epoch 5:  84%|████████▍ | 244/289 [01:53<00:20,  2.16it/s, loss=0.00124, v_num=1666580, train_F1w=1.000, val_loss=0.074, val_f1w=0.984]\n",
      "Epoch 5:  85%|████████▌ | 246/289 [01:53<00:19,  2.17it/s, loss=0.00124, v_num=1666580, train_F1w=1.000, val_loss=0.074, val_f1w=0.984]\n",
      "Epoch 5:  86%|████████▌ | 248/289 [01:53<00:18,  2.19it/s, loss=0.00124, v_num=1666580, train_F1w=1.000, val_loss=0.074, val_f1w=0.984]\n",
      "Epoch 5:  87%|████████▋ | 250/289 [01:53<00:17,  2.20it/s, loss=0.00124, v_num=1666580, train_F1w=1.000, val_loss=0.074, val_f1w=0.984]\n",
      "Epoch 5:  87%|████████▋ | 252/289 [01:53<00:16,  2.21it/s, loss=0.00124, v_num=1666580, train_F1w=1.000, val_loss=0.074, val_f1w=0.984]\n",
      "Epoch 5:  88%|████████▊ | 254/289 [01:53<00:15,  2.23it/s, loss=0.00124, v_num=1666580, train_F1w=1.000, val_loss=0.074, val_f1w=0.984]\n",
      "Epoch 5:  89%|████████▊ | 256/289 [01:54<00:14,  2.24it/s, loss=0.00124, v_num=1666580, train_F1w=1.000, val_loss=0.074, val_f1w=0.984]\n",
      "Epoch 5:  89%|████████▉ | 258/289 [01:54<00:13,  2.26it/s, loss=0.00124, v_num=1666580, train_F1w=1.000, val_loss=0.074, val_f1w=0.984]\n",
      "Epoch 5:  90%|████████▉ | 260/289 [01:54<00:12,  2.27it/s, loss=0.00124, v_num=1666580, train_F1w=1.000, val_loss=0.074, val_f1w=0.984]\n",
      "Epoch 5:  91%|█████████ | 262/289 [01:54<00:11,  2.29it/s, loss=0.00124, v_num=1666580, train_F1w=1.000, val_loss=0.074, val_f1w=0.984]\n",
      "Epoch 5:  91%|█████████▏| 264/289 [01:54<00:10,  2.30it/s, loss=0.00124, v_num=1666580, train_F1w=1.000, val_loss=0.074, val_f1w=0.984]\n",
      "Epoch 5:  92%|█████████▏| 266/289 [01:54<00:09,  2.32it/s, loss=0.00124, v_num=1666580, train_F1w=1.000, val_loss=0.074, val_f1w=0.984]\n",
      "Epoch 5:  93%|█████████▎| 268/289 [01:55<00:09,  2.33it/s, loss=0.00124, v_num=1666580, train_F1w=1.000, val_loss=0.074, val_f1w=0.984]\n",
      "Epoch 5:  93%|█████████▎| 270/289 [01:55<00:08,  2.34it/s, loss=0.00124, v_num=1666580, train_F1w=1.000, val_loss=0.074, val_f1w=0.984]\n",
      "Epoch 5:  94%|█████████▍| 272/289 [01:55<00:07,  2.36it/s, loss=0.00124, v_num=1666580, train_F1w=1.000, val_loss=0.074, val_f1w=0.984]\n",
      "Epoch 5:  95%|█████████▍| 274/289 [01:55<00:06,  2.37it/s, loss=0.00124, v_num=1666580, train_F1w=1.000, val_loss=0.074, val_f1w=0.984]\n",
      "Epoch 5:  96%|█████████▌| 276/289 [01:55<00:05,  2.39it/s, loss=0.00124, v_num=1666580, train_F1w=1.000, val_loss=0.074, val_f1w=0.984]\n",
      "Epoch 5:  96%|█████████▌| 278/289 [01:55<00:04,  2.40it/s, loss=0.00124, v_num=1666580, train_F1w=1.000, val_loss=0.074, val_f1w=0.984]\n",
      "Epoch 5:  97%|█████████▋| 280/289 [01:55<00:03,  2.41it/s, loss=0.00124, v_num=1666580, train_F1w=1.000, val_loss=0.074, val_f1w=0.984]\n",
      "Epoch 5:  98%|█████████▊| 282/289 [01:56<00:02,  2.43it/s, loss=0.00124, v_num=1666580, train_F1w=1.000, val_loss=0.074, val_f1w=0.984]\n",
      "Epoch 5:  98%|█████████▊| 284/289 [01:56<00:02,  2.44it/s, loss=0.00124, v_num=1666580, train_F1w=1.000, val_loss=0.074, val_f1w=0.984]\n",
      "Epoch 5:  99%|█████████▉| 286/289 [01:56<00:01,  2.46it/s, loss=0.00124, v_num=1666580, train_F1w=1.000, val_loss=0.074, val_f1w=0.984]\n",
      "Epoch 5: 100%|█████████▉| 288/289 [01:56<00:00,  2.47it/s, loss=0.00124, v_num=1666580, train_F1w=1.000, val_loss=0.074, val_f1w=0.984]\n",
      "Epoch 5: 100%|██████████| 289/289 [01:56<00:00,  2.48it/s, loss=0.00124, v_num=1666580, train_F1w=1.000, val_loss=0.0516, val_f1w=0.982]\n",
      "Epoch 5: 100%|██████████| 289/289 [01:56<00:00,  2.48it/s, loss=0.00124, v_num=1666580, train_F1w=1.000, val_loss=0.0516, val_f1w=0.982]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 1433: avg_val_f1w was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:  82%|████████▏ | 238/289 [01:52<00:24,  2.12it/s, loss=0.00289, v_num=1666580, train_F1w=1.000, val_loss=0.0516, val_f1w=0.982] Epoch     7: adjusting learning rate of group 0 to 2.0690e-05.\n",
      "Epoch 6:  83%|████████▎ | 240/289 [01:52<00:23,  2.13it/s, loss=0.00291, v_num=1666580, train_F1w=1.000, val_loss=0.0516, val_f1w=0.982]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6:  84%|████████▎ | 242/289 [01:53<00:21,  2.14it/s, loss=0.00291, v_num=1666580, train_F1w=1.000, val_loss=0.0516, val_f1w=0.982]\n",
      "Epoch 6:  84%|████████▍ | 244/289 [01:53<00:20,  2.16it/s, loss=0.00291, v_num=1666580, train_F1w=1.000, val_loss=0.0516, val_f1w=0.982]\n",
      "Epoch 6:  85%|████████▌ | 246/289 [01:53<00:19,  2.17it/s, loss=0.00291, v_num=1666580, train_F1w=1.000, val_loss=0.0516, val_f1w=0.982]\n",
      "Epoch 6:  86%|████████▌ | 248/289 [01:53<00:18,  2.19it/s, loss=0.00291, v_num=1666580, train_F1w=1.000, val_loss=0.0516, val_f1w=0.982]\n",
      "Epoch 6:  87%|████████▋ | 250/289 [01:53<00:17,  2.20it/s, loss=0.00291, v_num=1666580, train_F1w=1.000, val_loss=0.0516, val_f1w=0.982]\n",
      "Epoch 6:  87%|████████▋ | 252/289 [01:53<00:16,  2.21it/s, loss=0.00291, v_num=1666580, train_F1w=1.000, val_loss=0.0516, val_f1w=0.982]\n",
      "Epoch 6:  88%|████████▊ | 254/289 [01:53<00:15,  2.23it/s, loss=0.00291, v_num=1666580, train_F1w=1.000, val_loss=0.0516, val_f1w=0.982]\n",
      "Epoch 6:  89%|████████▊ | 256/289 [01:54<00:14,  2.24it/s, loss=0.00291, v_num=1666580, train_F1w=1.000, val_loss=0.0516, val_f1w=0.982]\n",
      "Epoch 6:  89%|████████▉ | 258/289 [01:54<00:13,  2.26it/s, loss=0.00291, v_num=1666580, train_F1w=1.000, val_loss=0.0516, val_f1w=0.982]\n",
      "Epoch 6:  90%|████████▉ | 260/289 [01:54<00:12,  2.27it/s, loss=0.00291, v_num=1666580, train_F1w=1.000, val_loss=0.0516, val_f1w=0.982]\n",
      "Epoch 6:  91%|█████████ | 262/289 [01:54<00:11,  2.29it/s, loss=0.00291, v_num=1666580, train_F1w=1.000, val_loss=0.0516, val_f1w=0.982]\n",
      "Epoch 6:  91%|█████████▏| 264/289 [01:54<00:10,  2.30it/s, loss=0.00291, v_num=1666580, train_F1w=1.000, val_loss=0.0516, val_f1w=0.982]\n",
      "Epoch 6:  92%|█████████▏| 266/289 [01:54<00:09,  2.32it/s, loss=0.00291, v_num=1666580, train_F1w=1.000, val_loss=0.0516, val_f1w=0.982]\n",
      "Epoch 6:  93%|█████████▎| 268/289 [01:55<00:09,  2.33it/s, loss=0.00291, v_num=1666580, train_F1w=1.000, val_loss=0.0516, val_f1w=0.982]\n",
      "Epoch 6:  93%|█████████▎| 270/289 [01:55<00:08,  2.34it/s, loss=0.00291, v_num=1666580, train_F1w=1.000, val_loss=0.0516, val_f1w=0.982]\n",
      "Epoch 6:  94%|█████████▍| 272/289 [01:55<00:07,  2.36it/s, loss=0.00291, v_num=1666580, train_F1w=1.000, val_loss=0.0516, val_f1w=0.982]\n",
      "Epoch 6:  95%|█████████▍| 274/289 [01:55<00:06,  2.37it/s, loss=0.00291, v_num=1666580, train_F1w=1.000, val_loss=0.0516, val_f1w=0.982]\n",
      "Epoch 6:  96%|█████████▌| 276/289 [01:55<00:05,  2.39it/s, loss=0.00291, v_num=1666580, train_F1w=1.000, val_loss=0.0516, val_f1w=0.982]\n",
      "Epoch 6:  96%|█████████▌| 278/289 [01:55<00:04,  2.40it/s, loss=0.00291, v_num=1666580, train_F1w=1.000, val_loss=0.0516, val_f1w=0.982]\n",
      "Epoch 6:  97%|█████████▋| 280/289 [01:55<00:03,  2.41it/s, loss=0.00291, v_num=1666580, train_F1w=1.000, val_loss=0.0516, val_f1w=0.982]\n",
      "Epoch 6:  98%|█████████▊| 282/289 [01:56<00:02,  2.43it/s, loss=0.00291, v_num=1666580, train_F1w=1.000, val_loss=0.0516, val_f1w=0.982]\n",
      "Epoch 6:  98%|█████████▊| 284/289 [01:56<00:02,  2.44it/s, loss=0.00291, v_num=1666580, train_F1w=1.000, val_loss=0.0516, val_f1w=0.982]\n",
      "Epoch 6:  99%|█████████▉| 286/289 [01:56<00:01,  2.46it/s, loss=0.00291, v_num=1666580, train_F1w=1.000, val_loss=0.0516, val_f1w=0.982]\n",
      "Epoch 6: 100%|█████████▉| 288/289 [01:56<00:00,  2.47it/s, loss=0.00291, v_num=1666580, train_F1w=1.000, val_loss=0.0516, val_f1w=0.982]\n",
      "Epoch 6: 100%|██████████| 289/289 [01:56<00:00,  2.48it/s, loss=0.00291, v_num=1666580, train_F1w=1.000, val_loss=0.0603, val_f1w=0.980]\n",
      "Epoch 6: 100%|██████████| 289/289 [01:56<00:00,  2.48it/s, loss=0.00291, v_num=1666580, train_F1w=1.000, val_loss=0.0603, val_f1w=0.980]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 1672: avg_val_f1w was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:  82%|████████▏ | 238/289 [01:52<00:24,  2.12it/s, loss=0.00138, v_num=1666580, train_F1w=1.000, val_loss=0.0603, val_f1w=0.980] Epoch     8: adjusting learning rate of group 0 to 9.6396e-06.\n",
      "Epoch 7:  83%|████████▎ | 240/289 [01:52<00:23,  2.13it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0603, val_f1w=0.980]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7:  84%|████████▎ | 242/289 [01:52<00:21,  2.14it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0603, val_f1w=0.980]\n",
      "Epoch 7:  84%|████████▍ | 244/289 [01:53<00:20,  2.16it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0603, val_f1w=0.980]\n",
      "Epoch 7:  85%|████████▌ | 246/289 [01:53<00:19,  2.17it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0603, val_f1w=0.980]\n",
      "Epoch 7:  86%|████████▌ | 248/289 [01:53<00:18,  2.19it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0603, val_f1w=0.980]\n",
      "Epoch 7:  87%|████████▋ | 250/289 [01:53<00:17,  2.20it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0603, val_f1w=0.980]\n",
      "Epoch 7:  87%|████████▋ | 252/289 [01:53<00:16,  2.22it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0603, val_f1w=0.980]\n",
      "Epoch 7:  88%|████████▊ | 254/289 [01:53<00:15,  2.23it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0603, val_f1w=0.980]\n",
      "Epoch 7:  89%|████████▊ | 256/289 [01:54<00:14,  2.25it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0603, val_f1w=0.980]\n",
      "Epoch 7:  89%|████████▉ | 258/289 [01:54<00:13,  2.26it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0603, val_f1w=0.980]\n",
      "Epoch 7:  90%|████████▉ | 260/289 [01:54<00:12,  2.27it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0603, val_f1w=0.980]\n",
      "Epoch 7:  91%|█████████ | 262/289 [01:54<00:11,  2.29it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0603, val_f1w=0.980]\n",
      "Epoch 7:  91%|█████████▏| 264/289 [01:54<00:10,  2.30it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0603, val_f1w=0.980]\n",
      "Epoch 7:  92%|█████████▏| 266/289 [01:54<00:09,  2.32it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0603, val_f1w=0.980]\n",
      "Epoch 7:  93%|█████████▎| 268/289 [01:54<00:09,  2.33it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0603, val_f1w=0.980]\n",
      "Epoch 7:  93%|█████████▎| 270/289 [01:55<00:08,  2.35it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0603, val_f1w=0.980]\n",
      "Epoch 7:  94%|█████████▍| 272/289 [01:55<00:07,  2.36it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0603, val_f1w=0.980]\n",
      "Epoch 7:  95%|█████████▍| 274/289 [01:55<00:06,  2.37it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0603, val_f1w=0.980]\n",
      "Epoch 7:  96%|█████████▌| 276/289 [01:55<00:05,  2.39it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0603, val_f1w=0.980]\n",
      "Epoch 7:  96%|█████████▌| 278/289 [01:55<00:04,  2.40it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0603, val_f1w=0.980]\n",
      "Epoch 7:  97%|█████████▋| 280/289 [01:55<00:03,  2.42it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0603, val_f1w=0.980]\n",
      "Epoch 7:  98%|█████████▊| 282/289 [01:56<00:02,  2.43it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0603, val_f1w=0.980]\n",
      "Epoch 7:  98%|█████████▊| 284/289 [01:56<00:02,  2.44it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0603, val_f1w=0.980]\n",
      "Epoch 7:  99%|█████████▉| 286/289 [01:56<00:01,  2.46it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0603, val_f1w=0.980]\n",
      "Epoch 7: 100%|█████████▉| 288/289 [01:56<00:00,  2.47it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0603, val_f1w=0.980]\n",
      "Epoch 7: 100%|██████████| 289/289 [01:56<00:00,  2.48it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0692, val_f1w=0.982]\n",
      "Epoch 7: 100%|██████████| 289/289 [01:56<00:00,  2.48it/s, loss=0.00131, v_num=1666580, train_F1w=1.000, val_loss=0.0692, val_f1w=0.982]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 1911: avg_val_f1w was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:  82%|████████▏ | 238/289 [01:52<00:24,  2.12it/s, loss=0.00312, v_num=1666580, train_F1w=0.970, val_loss=0.0692, val_f1w=0.982] Epoch     9: adjusting learning rate of group 0 to 2.5447e-06.\n",
      "Epoch 8:  83%|████████▎ | 240/289 [01:52<00:22,  2.13it/s, loss=0.00311, v_num=1666580, train_F1w=1.000, val_loss=0.0692, val_f1w=0.982]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8:  84%|████████▎ | 242/289 [01:52<00:21,  2.15it/s, loss=0.00311, v_num=1666580, train_F1w=1.000, val_loss=0.0692, val_f1w=0.982]\n",
      "Epoch 8:  84%|████████▍ | 244/289 [01:52<00:20,  2.16it/s, loss=0.00311, v_num=1666580, train_F1w=1.000, val_loss=0.0692, val_f1w=0.982]\n",
      "Epoch 8:  85%|████████▌ | 246/289 [01:53<00:19,  2.18it/s, loss=0.00311, v_num=1666580, train_F1w=1.000, val_loss=0.0692, val_f1w=0.982]\n",
      "Epoch 8:  86%|████████▌ | 248/289 [01:53<00:18,  2.19it/s, loss=0.00311, v_num=1666580, train_F1w=1.000, val_loss=0.0692, val_f1w=0.982]\n",
      "Epoch 8:  87%|████████▋ | 250/289 [01:53<00:17,  2.21it/s, loss=0.00311, v_num=1666580, train_F1w=1.000, val_loss=0.0692, val_f1w=0.982]\n",
      "Epoch 8:  87%|████████▋ | 252/289 [01:53<00:16,  2.22it/s, loss=0.00311, v_num=1666580, train_F1w=1.000, val_loss=0.0692, val_f1w=0.982]\n",
      "Epoch 8:  88%|████████▊ | 254/289 [01:53<00:15,  2.24it/s, loss=0.00311, v_num=1666580, train_F1w=1.000, val_loss=0.0692, val_f1w=0.982]\n",
      "Epoch 8:  89%|████████▊ | 256/289 [01:53<00:14,  2.25it/s, loss=0.00311, v_num=1666580, train_F1w=1.000, val_loss=0.0692, val_f1w=0.982]\n",
      "Epoch 8:  89%|████████▉ | 258/289 [01:53<00:13,  2.26it/s, loss=0.00311, v_num=1666580, train_F1w=1.000, val_loss=0.0692, val_f1w=0.982]\n",
      "Epoch 8:  90%|████████▉ | 260/289 [01:54<00:12,  2.28it/s, loss=0.00311, v_num=1666580, train_F1w=1.000, val_loss=0.0692, val_f1w=0.982]\n",
      "Epoch 8:  91%|█████████ | 262/289 [01:54<00:11,  2.29it/s, loss=0.00311, v_num=1666580, train_F1w=1.000, val_loss=0.0692, val_f1w=0.982]\n",
      "Epoch 8:  91%|█████████▏| 264/289 [01:54<00:10,  2.31it/s, loss=0.00311, v_num=1666580, train_F1w=1.000, val_loss=0.0692, val_f1w=0.982]\n",
      "Epoch 8:  92%|█████████▏| 266/289 [01:54<00:09,  2.32it/s, loss=0.00311, v_num=1666580, train_F1w=1.000, val_loss=0.0692, val_f1w=0.982]\n",
      "Epoch 8:  93%|█████████▎| 268/289 [01:54<00:08,  2.34it/s, loss=0.00311, v_num=1666580, train_F1w=1.000, val_loss=0.0692, val_f1w=0.982]\n",
      "Epoch 8:  93%|█████████▎| 270/289 [01:54<00:08,  2.35it/s, loss=0.00311, v_num=1666580, train_F1w=1.000, val_loss=0.0692, val_f1w=0.982]\n",
      "Epoch 8:  94%|█████████▍| 272/289 [01:55<00:07,  2.36it/s, loss=0.00311, v_num=1666580, train_F1w=1.000, val_loss=0.0692, val_f1w=0.982]\n",
      "Epoch 8:  95%|█████████▍| 274/289 [01:55<00:06,  2.38it/s, loss=0.00311, v_num=1666580, train_F1w=1.000, val_loss=0.0692, val_f1w=0.982]\n",
      "Epoch 8:  96%|█████████▌| 276/289 [01:55<00:05,  2.39it/s, loss=0.00311, v_num=1666580, train_F1w=1.000, val_loss=0.0692, val_f1w=0.982]\n",
      "Epoch 8:  96%|█████████▌| 278/289 [01:55<00:04,  2.41it/s, loss=0.00311, v_num=1666580, train_F1w=1.000, val_loss=0.0692, val_f1w=0.982]\n",
      "Epoch 8:  97%|█████████▋| 280/289 [01:55<00:03,  2.42it/s, loss=0.00311, v_num=1666580, train_F1w=1.000, val_loss=0.0692, val_f1w=0.982]\n",
      "Epoch 8:  98%|█████████▊| 282/289 [01:55<00:02,  2.44it/s, loss=0.00311, v_num=1666580, train_F1w=1.000, val_loss=0.0692, val_f1w=0.982]\n",
      "Epoch 8:  98%|█████████▊| 284/289 [01:55<00:02,  2.45it/s, loss=0.00311, v_num=1666580, train_F1w=1.000, val_loss=0.0692, val_f1w=0.982]\n",
      "Epoch 8:  99%|█████████▉| 286/289 [01:56<00:01,  2.46it/s, loss=0.00311, v_num=1666580, train_F1w=1.000, val_loss=0.0692, val_f1w=0.982]\n",
      "Epoch 8: 100%|█████████▉| 288/289 [01:56<00:00,  2.48it/s, loss=0.00311, v_num=1666580, train_F1w=1.000, val_loss=0.0692, val_f1w=0.982]\n",
      "Epoch 8: 100%|██████████| 289/289 [01:56<00:00,  2.48it/s, loss=0.00311, v_num=1666580, train_F1w=1.000, val_loss=0.0778, val_f1w=0.980]\n",
      "Epoch 8: 100%|██████████| 289/289 [01:56<00:00,  2.48it/s, loss=0.00311, v_num=1666580, train_F1w=1.000, val_loss=0.0778, val_f1w=0.980]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 2150: avg_val_f1w was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:  82%|████████▏ | 238/289 [01:52<00:24,  2.12it/s, loss=0.000319, v_num=1666580, train_F1w=1.000, val_loss=0.0778, val_f1w=0.980]Epoch    10: adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 9:  83%|████████▎ | 240/289 [01:52<00:22,  2.13it/s, loss=0.000246, v_num=1666580, train_F1w=1.000, val_loss=0.0778, val_f1w=0.980]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9:  84%|████████▎ | 242/289 [01:52<00:21,  2.15it/s, loss=0.000246, v_num=1666580, train_F1w=1.000, val_loss=0.0778, val_f1w=0.980]\n",
      "Epoch 9:  84%|████████▍ | 244/289 [01:52<00:20,  2.16it/s, loss=0.000246, v_num=1666580, train_F1w=1.000, val_loss=0.0778, val_f1w=0.980]\n",
      "Epoch 9:  85%|████████▌ | 246/289 [01:52<00:19,  2.18it/s, loss=0.000246, v_num=1666580, train_F1w=1.000, val_loss=0.0778, val_f1w=0.980]\n",
      "Epoch 9:  86%|████████▌ | 248/289 [01:53<00:18,  2.19it/s, loss=0.000246, v_num=1666580, train_F1w=1.000, val_loss=0.0778, val_f1w=0.980]\n",
      "Epoch 9:  87%|████████▋ | 250/289 [01:53<00:17,  2.21it/s, loss=0.000246, v_num=1666580, train_F1w=1.000, val_loss=0.0778, val_f1w=0.980]\n",
      "Epoch 9:  87%|████████▋ | 252/289 [01:53<00:16,  2.22it/s, loss=0.000246, v_num=1666580, train_F1w=1.000, val_loss=0.0778, val_f1w=0.980]\n",
      "Epoch 9:  88%|████████▊ | 254/289 [01:53<00:15,  2.24it/s, loss=0.000246, v_num=1666580, train_F1w=1.000, val_loss=0.0778, val_f1w=0.980]\n",
      "Epoch 9:  89%|████████▊ | 256/289 [01:53<00:14,  2.25it/s, loss=0.000246, v_num=1666580, train_F1w=1.000, val_loss=0.0778, val_f1w=0.980]\n",
      "Epoch 9:  89%|████████▉ | 258/289 [01:53<00:13,  2.27it/s, loss=0.000246, v_num=1666580, train_F1w=1.000, val_loss=0.0778, val_f1w=0.980]\n",
      "Epoch 9:  90%|████████▉ | 260/289 [01:54<00:12,  2.28it/s, loss=0.000246, v_num=1666580, train_F1w=1.000, val_loss=0.0778, val_f1w=0.980]\n",
      "Epoch 9:  91%|█████████ | 262/289 [01:54<00:11,  2.29it/s, loss=0.000246, v_num=1666580, train_F1w=1.000, val_loss=0.0778, val_f1w=0.980]\n",
      "Epoch 9:  91%|█████████▏| 264/289 [01:54<00:10,  2.31it/s, loss=0.000246, v_num=1666580, train_F1w=1.000, val_loss=0.0778, val_f1w=0.980]\n",
      "Epoch 9:  92%|█████████▏| 266/289 [01:54<00:09,  2.32it/s, loss=0.000246, v_num=1666580, train_F1w=1.000, val_loss=0.0778, val_f1w=0.980]\n",
      "Epoch 9:  93%|█████████▎| 268/289 [01:54<00:08,  2.34it/s, loss=0.000246, v_num=1666580, train_F1w=1.000, val_loss=0.0778, val_f1w=0.980]\n",
      "Epoch 9:  93%|█████████▎| 270/289 [01:54<00:08,  2.35it/s, loss=0.000246, v_num=1666580, train_F1w=1.000, val_loss=0.0778, val_f1w=0.980]\n",
      "Epoch 9:  94%|█████████▍| 272/289 [01:54<00:07,  2.37it/s, loss=0.000246, v_num=1666580, train_F1w=1.000, val_loss=0.0778, val_f1w=0.980]\n",
      "Epoch 9:  95%|█████████▍| 274/289 [01:55<00:06,  2.38it/s, loss=0.000246, v_num=1666580, train_F1w=1.000, val_loss=0.0778, val_f1w=0.980]\n",
      "Epoch 9:  96%|█████████▌| 276/289 [01:55<00:05,  2.39it/s, loss=0.000246, v_num=1666580, train_F1w=1.000, val_loss=0.0778, val_f1w=0.980]\n",
      "Epoch 9:  96%|█████████▌| 278/289 [01:55<00:04,  2.41it/s, loss=0.000246, v_num=1666580, train_F1w=1.000, val_loss=0.0778, val_f1w=0.980]\n",
      "Epoch 9:  97%|█████████▋| 280/289 [01:55<00:03,  2.42it/s, loss=0.000246, v_num=1666580, train_F1w=1.000, val_loss=0.0778, val_f1w=0.980]\n",
      "Epoch 9:  98%|█████████▊| 282/289 [01:55<00:02,  2.44it/s, loss=0.000246, v_num=1666580, train_F1w=1.000, val_loss=0.0778, val_f1w=0.980]\n",
      "Epoch 9:  98%|█████████▊| 284/289 [01:55<00:02,  2.45it/s, loss=0.000246, v_num=1666580, train_F1w=1.000, val_loss=0.0778, val_f1w=0.980]\n",
      "Epoch 9:  99%|█████████▉| 286/289 [01:56<00:01,  2.46it/s, loss=0.000246, v_num=1666580, train_F1w=1.000, val_loss=0.0778, val_f1w=0.980]\n",
      "Epoch 9: 100%|█████████▉| 288/289 [01:56<00:00,  2.48it/s, loss=0.000246, v_num=1666580, train_F1w=1.000, val_loss=0.0778, val_f1w=0.980]\n",
      "Epoch 9: 100%|██████████| 289/289 [01:56<00:00,  2.48it/s, loss=0.000246, v_num=1666580, train_F1w=1.000, val_loss=0.0672, val_f1w=0.978]\n",
      "Epoch 9: 100%|██████████| 289/289 [01:56<00:00,  2.48it/s, loss=0.000246, v_num=1666580, train_F1w=1.000, val_loss=0.0672, val_f1w=0.978]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 2389: avg_val_f1w was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:  82%|████████▏ | 238/289 [01:52<00:24,  2.11it/s, loss=0.177, v_num=1666580, train_F1w=0.973, val_loss=0.0672, val_f1w=0.978]  Epoch    11: adjusting learning rate of group 0 to 9.7555e-05.\n",
      "Epoch 10:  83%|████████▎ | 240/289 [01:52<00:23,  2.13it/s, loss=0.168, v_num=1666580, train_F1w=0.970, val_loss=0.0672, val_f1w=0.978]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 10:  84%|████████▎ | 242/289 [01:53<00:21,  2.14it/s, loss=0.168, v_num=1666580, train_F1w=0.970, val_loss=0.0672, val_f1w=0.978]\n",
      "Epoch 10:  84%|████████▍ | 244/289 [01:53<00:20,  2.16it/s, loss=0.168, v_num=1666580, train_F1w=0.970, val_loss=0.0672, val_f1w=0.978]\n",
      "Epoch 10:  85%|████████▌ | 246/289 [01:53<00:19,  2.17it/s, loss=0.168, v_num=1666580, train_F1w=0.970, val_loss=0.0672, val_f1w=0.978]\n",
      "Epoch 10:  86%|████████▌ | 248/289 [01:53<00:18,  2.19it/s, loss=0.168, v_num=1666580, train_F1w=0.970, val_loss=0.0672, val_f1w=0.978]\n",
      "Epoch 10:  87%|████████▋ | 250/289 [01:53<00:17,  2.20it/s, loss=0.168, v_num=1666580, train_F1w=0.970, val_loss=0.0672, val_f1w=0.978]\n",
      "Epoch 10:  87%|████████▋ | 252/289 [01:53<00:16,  2.21it/s, loss=0.168, v_num=1666580, train_F1w=0.970, val_loss=0.0672, val_f1w=0.978]\n",
      "Epoch 10:  88%|████████▊ | 254/289 [01:53<00:15,  2.23it/s, loss=0.168, v_num=1666580, train_F1w=0.970, val_loss=0.0672, val_f1w=0.978]\n",
      "Epoch 10:  89%|████████▊ | 256/289 [01:54<00:14,  2.24it/s, loss=0.168, v_num=1666580, train_F1w=0.970, val_loss=0.0672, val_f1w=0.978]\n",
      "Epoch 10:  89%|████████▉ | 258/289 [01:54<00:13,  2.26it/s, loss=0.168, v_num=1666580, train_F1w=0.970, val_loss=0.0672, val_f1w=0.978]\n",
      "Epoch 10:  90%|████████▉ | 260/289 [01:54<00:12,  2.27it/s, loss=0.168, v_num=1666580, train_F1w=0.970, val_loss=0.0672, val_f1w=0.978]\n",
      "Epoch 10:  91%|█████████ | 262/289 [01:54<00:11,  2.29it/s, loss=0.168, v_num=1666580, train_F1w=0.970, val_loss=0.0672, val_f1w=0.978]\n",
      "Epoch 10:  91%|█████████▏| 264/289 [01:54<00:10,  2.30it/s, loss=0.168, v_num=1666580, train_F1w=0.970, val_loss=0.0672, val_f1w=0.978]\n",
      "Epoch 10:  92%|█████████▏| 266/289 [01:54<00:09,  2.32it/s, loss=0.168, v_num=1666580, train_F1w=0.970, val_loss=0.0672, val_f1w=0.978]\n",
      "Epoch 10:  93%|█████████▎| 268/289 [01:55<00:09,  2.33it/s, loss=0.168, v_num=1666580, train_F1w=0.970, val_loss=0.0672, val_f1w=0.978]\n",
      "Epoch 10:  93%|█████████▎| 270/289 [01:55<00:08,  2.34it/s, loss=0.168, v_num=1666580, train_F1w=0.970, val_loss=0.0672, val_f1w=0.978]\n",
      "Epoch 10:  94%|█████████▍| 272/289 [01:55<00:07,  2.36it/s, loss=0.168, v_num=1666580, train_F1w=0.970, val_loss=0.0672, val_f1w=0.978]\n",
      "Epoch 10:  95%|█████████▍| 274/289 [01:55<00:06,  2.37it/s, loss=0.168, v_num=1666580, train_F1w=0.970, val_loss=0.0672, val_f1w=0.978]\n",
      "Epoch 10:  96%|█████████▌| 276/289 [01:55<00:05,  2.39it/s, loss=0.168, v_num=1666580, train_F1w=0.970, val_loss=0.0672, val_f1w=0.978]\n",
      "Epoch 10:  96%|█████████▌| 278/289 [01:55<00:04,  2.40it/s, loss=0.168, v_num=1666580, train_F1w=0.970, val_loss=0.0672, val_f1w=0.978]\n",
      "Epoch 10:  97%|█████████▋| 280/289 [01:55<00:03,  2.41it/s, loss=0.168, v_num=1666580, train_F1w=0.970, val_loss=0.0672, val_f1w=0.978]\n",
      "Epoch 10:  98%|█████████▊| 282/289 [01:56<00:02,  2.43it/s, loss=0.168, v_num=1666580, train_F1w=0.970, val_loss=0.0672, val_f1w=0.978]\n",
      "Epoch 10:  98%|█████████▊| 284/289 [01:56<00:02,  2.44it/s, loss=0.168, v_num=1666580, train_F1w=0.970, val_loss=0.0672, val_f1w=0.978]\n",
      "Epoch 10:  99%|█████████▉| 286/289 [01:56<00:01,  2.46it/s, loss=0.168, v_num=1666580, train_F1w=0.970, val_loss=0.0672, val_f1w=0.978]\n",
      "Epoch 10: 100%|█████████▉| 288/289 [01:56<00:00,  2.47it/s, loss=0.168, v_num=1666580, train_F1w=0.970, val_loss=0.0672, val_f1w=0.978]\n",
      "Epoch 10: 100%|██████████| 289/289 [01:56<00:00,  2.48it/s, loss=0.168, v_num=1666580, train_F1w=0.970, val_loss=0.173, val_f1w=0.947] \n",
      "Epoch 10: 100%|██████████| 289/289 [01:56<00:00,  2.48it/s, loss=0.168, v_num=1666580, train_F1w=0.970, val_loss=0.173, val_f1w=0.947]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 2628: avg_val_f1w was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 289/289 [01:56<00:00,  2.48it/s, loss=0.168, v_num=1666580, train_F1w=0.970, val_loss=0.173, val_f1w=0.947]\n"
     ]
    }
   ],
   "source": [
    "## Sanity-check\n",
    "trainer = pl.Trainer(gpus=gpu,\n",
    "                     callbacks=[checkpoint_callback, early_stop_callback],\n",
    "                    #  deterministic=True,\n",
    "                     enable_progress_bar=True,\n",
    "                    #  progress_bar_\n",
    "                    #  limit_train_batches=2,\n",
    "                    #  limit_val_batches=2,\n",
    "                     max_epochs=20)\n",
    "\n",
    "# Then you call the fit method passing the PL-module and PL-Datamodule, to start training.\n",
    "trainer.fit(baseline_model_CEW, dm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6b79f4-c9d4-4c0a-b940-498239a12cb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "47571657-16cb-4dc4-837e-4d9a0be6fc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "896\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "                 Pinus       0.00      0.00      0.00        28\n",
      "               Erica.m       0.63      0.98      0.77       184\n",
      "             Cistus sp       0.00      0.00      0.00        69\n",
      "             Lavandula       0.00      0.00      0.00        74\n",
      "             Citrus sp       0.00      0.00      0.00        53\n",
      "     Helianthus annuus       0.00      0.00      0.00        91\n",
      "        Eucalyptus sp.       0.00      0.00      0.00        95\n",
      "Rosmarinus officinalis       0.00      0.00      0.00        52\n",
      "              Brassica       0.22      0.96      0.36       140\n",
      "                Cardus       0.00      0.00      0.00        19\n",
      "                 Tilia       0.00      0.00      0.00        67\n",
      "             Taraxacum       0.00      0.00      0.00        25\n",
      "\n",
      "              accuracy                           0.35       897\n",
      "             macro avg       0.07      0.16      0.09       897\n",
      "          weighted avg       0.16      0.35      0.21       897\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Base-CEW-epoch=4-val_loss=0.07-avg_val_f1w=0.98.ckpt\"\n",
    "\n",
    "baseline_model  = LitModel_CEW(model=resnet_model, class_weights=class_weights)\n",
    "\n",
    "# /mnt/gpid08/datasets/remote_sensing/tmp_from_gpid07/honey/results/Base-epoch=7-val_loss=0.06-avg_val_f1w=0.98.ckpt\n",
    "# baseline_model.model.load_from_checkpoint('/content/drive/MyDrive/dataset_honey/results/'+ model_name)\n",
    "\n",
    "checkpoint =  torch.load('/mnt/gpid08/datasets/remote_sensing/tmp_from_gpid07/honey/results/'+ model_name)\n",
    "\n",
    "baseline_model.load_state_dict(checkpoint['state_dict'])\n",
    "# baseline_model.load_from_checkpoint(\"Base-epoch=1-val_loss=0.05-avg_val_f1w=0.98.ckpt\")\n",
    "\n",
    "targets, preds = [],[]\n",
    "baseline_model.to('cuda')\n",
    "for ii, data in enumerate(dm.test_dataloader()):\n",
    "    with torch.no_grad():\n",
    "        targets.append(data['target'].numpy()) #torch.argmax(data['target'],dim=1).numpy()\n",
    "        # y = y.reshape((-1,1))\n",
    "        # print(y.shape)\n",
    "        ## inference\n",
    "        o = baseline_model.predict(data['image'].to('cuda')).cpu().numpy()\n",
    "        # print(o.shape)\n",
    "        preds.append(o)\n",
    "        # print(y)\n",
    "        # print(o)\n",
    "        # if ii==2:\n",
    "        #   break\n",
    "print(ii)\n",
    "preds2 = np.vstack([x for x in preds]).reshape(-1,1)\n",
    "targets2 = np.vstack([x for x in targets]).reshape(-1,1)\n",
    "target_names = ['Pinus','Erica.m', 'Cistus sp', 'Lavandula', 'Citrus sp', 'Helianthus annuus',\n",
    "          'Eucalyptus sp.', 'Rosmarinus officinalis', 'Brassica', 'Cardus', 'Tilia', 'Taraxacum']\n",
    "\n",
    "print(classification_report(targets2, preds2, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3145089c-343b-4518-8525-9e28b1232ebe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf92ab0-77a0-4648-9742-2b8aeca6b845",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "caaff23d-df70-4df0-95f6-cf9fd90a821f",
   "metadata": {},
   "source": [
    "## Weighted Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "797856d4-b134-4975-a52b-9087a35ed795",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import WeightedRandomSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7f1e9fda-5754-449e-b9cc-8216402fa8c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1455880557525017"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_train.loc['Pinus','weights_norm'].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5e180693-a3db-4e4b-991c-ee5d23d5ac00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/mnt/gpid08/datasets/remote_sensing/tmp_from_g...</td>\n",
       "      <td>Helianthus annuus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/mnt/gpid08/datasets/remote_sensing/tmp_from_g...</td>\n",
       "      <td>Brassica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/mnt/gpid08/datasets/remote_sensing/tmp_from_g...</td>\n",
       "      <td>Erica.m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/mnt/gpid08/datasets/remote_sensing/tmp_from_g...</td>\n",
       "      <td>Pinus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/mnt/gpid08/datasets/remote_sensing/tmp_from_g...</td>\n",
       "      <td>Erica.m</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name             labels\n",
       "0  /mnt/gpid08/datasets/remote_sensing/tmp_from_g...  Helianthus annuus\n",
       "1  /mnt/gpid08/datasets/remote_sensing/tmp_from_g...           Brassica\n",
       "2  /mnt/gpid08/datasets/remote_sensing/tmp_from_g...            Erica.m\n",
       "3  /mnt/gpid08/datasets/remote_sensing/tmp_from_g...              Pinus\n",
       "4  /mnt/gpid08/datasets/remote_sensing/tmp_from_g...            Erica.m"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "508e1392-62ad-4d0b-a9ea-76bdc7621dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7669])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "samples_weight = np.array([count_train.loc[t,'weights_norm'].item() for t in df_train['labels']])\n",
    "samples_weight=torch.from_numpy(samples_weight)\n",
    "\n",
    "print(samples_weight.shape)\n",
    "\n",
    "sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025c1647-964b-40c9-bc0e-442b3e8ed7b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1f5c2b79-f6bb-4903-96df-87924460c01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://juansensio.com/blog/062_multihead_attention\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, mode, df):\n",
    "        self.mode = mode\n",
    "        self.df = df \n",
    "        self.mean_img = (0.485, 0.456, 0.406 )\n",
    "        self.std_img = (0.229, 0.224, 0.225)\n",
    "        self.classes = ['Pinus','Erica.m', 'Cistus sp', 'Lavandula', 'Citrus sp', 'Helianthus annuus',\n",
    "                        'Eucalyptus sp.', 'Rosmarinus officinalis', 'Brassica', 'Cardus', 'Tilia', 'Taraxacum']\n",
    "    def __crop_padding(self,img):\n",
    "        ## convert to gray\n",
    "        img_gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        ## set threshold for 0\n",
    "        _,thresh = cv2.threshold(img_gray,10,255,cv2.THRESH_BINARY)\n",
    "        ## find contours\n",
    "        contours, hierarchy = cv2.findContours(thresh,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cnt = contours[0]\n",
    "        x,y,w,h = cv2.boundingRect(cnt)\n",
    "        crop = img[y:y+h,x:x+w,:]\n",
    "        return crop\n",
    "    def __getitem__(self, index):\n",
    "        name_img = self.df['name'].iloc[index]\n",
    "        label    = self.df['labels'].iloc[index]\n",
    "        ## READ IMAGE\n",
    "        image = plt.imread(name_img)\n",
    "        image = self.__crop_padding(image)\n",
    "        target = torch.tensor(self.classes.index(label))\n",
    "        # print(f'Image shape: {image.shape} \\t Target:{target}')\n",
    "        if self.mode=='train':\n",
    "            train_augm = albumentations.Compose(\n",
    "              [\n",
    "               albumentations.Resize(height=320,width=320),\n",
    "               albumentations.Normalize(self.mean_img, self.std_img, max_pixel_value=255.0, always_apply=True),\n",
    "              #  albumentations.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15),\n",
    "              #  albumentations.Flip(p=0.5)\n",
    "              ]\n",
    "            )\n",
    "            transformed = train_augm(image=image)\n",
    "            image=transformed['image']\n",
    "        else:\n",
    "            valid_augm = albumentations.Compose(\n",
    "              [\n",
    "               albumentations.Resize(height=320,width=320),\n",
    "               albumentations.Normalize(self.mean_img, self.std_img, max_pixel_value=255.0, always_apply=True)\n",
    "              ]\n",
    "            )\n",
    "            transformed = valid_augm(image=image)\n",
    "            image=transformed['image']\n",
    "        image = torch.from_numpy(image.transpose()).float()\n",
    "        target_oh = torch.nn.functional.one_hot(target, num_classes=12).float()\n",
    "        data = {\"image\":image,\n",
    "                \"target_oh\":target_oh,\n",
    "                'target':target,\n",
    "                'class_name':label } \n",
    "        # print(f'Image shape: {image.shape} \\t Target:{target}')\n",
    "        return data\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "class HoneyDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size: int = 4, Dataset = Dataset):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.Dataset = Dataset\n",
    "        self.train_ds =  self.Dataset(mode='train',df= df_train)\n",
    "        self.val_ds   =  self.Dataset(mode='val', df= df_val)\n",
    "        self.test_ds   =  self.Dataset(mode='test', df= df_test)\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_ds,\n",
    "                          batch_size=24,\n",
    "                          # shuffle=True,\n",
    "                          num_workers=0,\n",
    "                          pin_memory=True,\n",
    "                          drop_last=True,\n",
    "                          sampler=sampler\n",
    "                          )\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_ds, batch_size=8, shuffle=False, num_workers=0, pin_memory=True, drop_last=True )\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_ds, batch_size=1, shuffle=False, num_workers=0, pin_memory=True, drop_last=False )\n",
    "    \n",
    "dm = HoneyDataModule(Dataset=Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2635a9d9-22ab-4090-9b6a-776c02d92514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([189., 203., 215., 191., 211., 186., 202., 193., 222., 224., 203.,\n",
       "       185.])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_samples=np.zeros(12)\n",
    "for ii, data in enumerate(dm.train_dataloader()):\n",
    "    # print(data['target'].unique(return_counts=True))\n",
    "    values, counts = data['target'].unique(return_counts=True)\n",
    "    for vv,cc in zip(values, counts):\n",
    "        total_samples[vv] += cc\n",
    "    if ii==100:\n",
    "        break\n",
    "total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7c86575a-f43b-4c16-ab0a-bb2a336efaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model_CEW_sampler  = LitModel_CEW(model=resnet_model, class_weights=class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "69c99603-9bda-4515-b8fb-4517ebbb9463",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Set SLURM handle signals.\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | ResNet | 23.5 M\n",
      "---------------------------------\n",
      "23.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.130    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     0: adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 0:  86%|████████▌ | 318/369 [01:54<00:18,  2.77it/s, loss=0.0514, v_num=1666580, train_F1w=1.000] Epoch     1: adjusting learning rate of group 0 to 9.7555e-05.\n",
      "Epoch 0:  86%|████████▋ | 319/369 [01:55<00:18,  2.77it/s, loss=0.0602, v_num=1666580, train_F1w=0.962]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  87%|████████▋ | 321/369 [01:55<00:17,  2.78it/s, loss=0.0602, v_num=1666580, train_F1w=0.962]\n",
      "Epoch 0:  88%|████████▊ | 323/369 [01:55<00:16,  2.80it/s, loss=0.0602, v_num=1666580, train_F1w=0.962]\n",
      "Epoch 0:  88%|████████▊ | 325/369 [01:55<00:15,  2.81it/s, loss=0.0602, v_num=1666580, train_F1w=0.962]\n",
      "Epoch 0:  89%|████████▊ | 327/369 [01:55<00:14,  2.82it/s, loss=0.0602, v_num=1666580, train_F1w=0.962]\n",
      "Epoch 0:  89%|████████▉ | 329/369 [01:55<00:14,  2.84it/s, loss=0.0602, v_num=1666580, train_F1w=0.962]\n",
      "Epoch 0:  90%|████████▉ | 331/369 [01:56<00:13,  2.85it/s, loss=0.0602, v_num=1666580, train_F1w=0.962]\n",
      "Epoch 0:  90%|█████████ | 333/369 [01:56<00:12,  2.87it/s, loss=0.0602, v_num=1666580, train_F1w=0.962]\n",
      "Epoch 0:  91%|█████████ | 335/369 [01:56<00:11,  2.88it/s, loss=0.0602, v_num=1666580, train_F1w=0.962]\n",
      "Epoch 0:  91%|█████████▏| 337/369 [01:56<00:11,  2.89it/s, loss=0.0602, v_num=1666580, train_F1w=0.962]\n",
      "Epoch 0:  92%|█████████▏| 339/369 [01:56<00:10,  2.91it/s, loss=0.0602, v_num=1666580, train_F1w=0.962]\n",
      "Epoch 0:  92%|█████████▏| 341/369 [01:56<00:09,  2.92it/s, loss=0.0602, v_num=1666580, train_F1w=0.962]\n",
      "Epoch 0:  93%|█████████▎| 343/369 [01:56<00:08,  2.93it/s, loss=0.0602, v_num=1666580, train_F1w=0.962]\n",
      "Epoch 0:  93%|█████████▎| 345/369 [01:57<00:08,  2.95it/s, loss=0.0602, v_num=1666580, train_F1w=0.962]\n",
      "Epoch 0:  94%|█████████▍| 347/369 [01:57<00:07,  2.96it/s, loss=0.0602, v_num=1666580, train_F1w=0.962]\n",
      "Epoch 0:  95%|█████████▍| 349/369 [01:57<00:06,  2.97it/s, loss=0.0602, v_num=1666580, train_F1w=0.962]\n",
      "Epoch 0:  95%|█████████▌| 351/369 [01:57<00:06,  2.99it/s, loss=0.0602, v_num=1666580, train_F1w=0.962]\n",
      "Epoch 0:  96%|█████████▌| 353/369 [01:57<00:05,  3.00it/s, loss=0.0602, v_num=1666580, train_F1w=0.962]\n",
      "Epoch 0:  96%|█████████▌| 355/369 [01:57<00:04,  3.01it/s, loss=0.0602, v_num=1666580, train_F1w=0.962]\n",
      "Epoch 0:  97%|█████████▋| 357/369 [01:58<00:03,  3.02it/s, loss=0.0602, v_num=1666580, train_F1w=0.962]\n",
      "Epoch 0:  97%|█████████▋| 359/369 [01:58<00:03,  3.04it/s, loss=0.0602, v_num=1666580, train_F1w=0.962]\n",
      "Epoch 0:  98%|█████████▊| 361/369 [01:58<00:02,  3.05it/s, loss=0.0602, v_num=1666580, train_F1w=0.962]\n",
      "Epoch 0:  98%|█████████▊| 363/369 [01:58<00:01,  3.06it/s, loss=0.0602, v_num=1666580, train_F1w=0.962]\n",
      "Epoch 0:  99%|█████████▉| 365/369 [01:58<00:01,  3.08it/s, loss=0.0602, v_num=1666580, train_F1w=0.962]\n",
      "Epoch 0:  99%|█████████▉| 367/369 [01:58<00:00,  3.09it/s, loss=0.0602, v_num=1666580, train_F1w=0.962]\n",
      "Epoch 0: 100%|██████████| 369/369 [01:59<00:00,  3.10it/s, loss=0.0602, v_num=1666580, train_F1w=0.962, val_loss=0.207, val_f1w=0.917]\n",
      "Epoch 0: 100%|██████████| 369/369 [01:59<00:00,  3.10it/s, loss=0.0602, v_num=1666580, train_F1w=0.962, val_loss=0.207, val_f1w=0.917]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 318: avg_val_f1w reached 0.91682 (best 0.91682), saving model to \"/mnt/gpid08/datasets/remote_sensing/tmp_from_gpid07/honey/results/Base-CEW-Sampler-epoch=0-val_loss=0.21-avg_val_f1w=0.92.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  86%|████████▌ | 318/369 [01:55<00:18,  2.75it/s, loss=0.054, v_num=1666580, train_F1w=1.000, val_loss=0.207, val_f1w=0.917] Epoch     2: adjusting learning rate of group 0 to 9.0460e-05.\n",
      "Epoch 1:  87%|████████▋ | 320/369 [01:55<00:17,  2.76it/s, loss=0.0432, v_num=1666580, train_F1w=1.000, val_loss=0.207, val_f1w=0.917]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  87%|████████▋ | 322/369 [01:56<00:16,  2.77it/s, loss=0.0432, v_num=1666580, train_F1w=1.000, val_loss=0.207, val_f1w=0.917]\n",
      "Epoch 1:  88%|████████▊ | 324/369 [01:56<00:16,  2.79it/s, loss=0.0432, v_num=1666580, train_F1w=1.000, val_loss=0.207, val_f1w=0.917]\n",
      "Epoch 1:  88%|████████▊ | 326/369 [01:56<00:15,  2.80it/s, loss=0.0432, v_num=1666580, train_F1w=1.000, val_loss=0.207, val_f1w=0.917]\n",
      "Epoch 1:  89%|████████▉ | 328/369 [01:56<00:14,  2.81it/s, loss=0.0432, v_num=1666580, train_F1w=1.000, val_loss=0.207, val_f1w=0.917]\n",
      "Epoch 1:  89%|████████▉ | 330/369 [01:56<00:13,  2.83it/s, loss=0.0432, v_num=1666580, train_F1w=1.000, val_loss=0.207, val_f1w=0.917]\n",
      "Epoch 1:  90%|████████▉ | 332/369 [01:56<00:13,  2.84it/s, loss=0.0432, v_num=1666580, train_F1w=1.000, val_loss=0.207, val_f1w=0.917]\n",
      "Epoch 1:  91%|█████████ | 334/369 [01:57<00:12,  2.85it/s, loss=0.0432, v_num=1666580, train_F1w=1.000, val_loss=0.207, val_f1w=0.917]\n",
      "Epoch 1:  91%|█████████ | 336/369 [01:57<00:11,  2.87it/s, loss=0.0432, v_num=1666580, train_F1w=1.000, val_loss=0.207, val_f1w=0.917]\n",
      "Epoch 1:  92%|█████████▏| 338/369 [01:57<00:10,  2.88it/s, loss=0.0432, v_num=1666580, train_F1w=1.000, val_loss=0.207, val_f1w=0.917]\n",
      "Epoch 1:  92%|█████████▏| 340/369 [01:57<00:10,  2.89it/s, loss=0.0432, v_num=1666580, train_F1w=1.000, val_loss=0.207, val_f1w=0.917]\n",
      "Epoch 1:  93%|█████████▎| 342/369 [01:57<00:09,  2.91it/s, loss=0.0432, v_num=1666580, train_F1w=1.000, val_loss=0.207, val_f1w=0.917]\n",
      "Epoch 1:  93%|█████████▎| 344/369 [01:57<00:08,  2.92it/s, loss=0.0432, v_num=1666580, train_F1w=1.000, val_loss=0.207, val_f1w=0.917]\n",
      "Epoch 1:  94%|█████████▍| 346/369 [01:58<00:07,  2.93it/s, loss=0.0432, v_num=1666580, train_F1w=1.000, val_loss=0.207, val_f1w=0.917]\n",
      "Epoch 1:  94%|█████████▍| 348/369 [01:58<00:07,  2.95it/s, loss=0.0432, v_num=1666580, train_F1w=1.000, val_loss=0.207, val_f1w=0.917]\n",
      "Epoch 1:  95%|█████████▍| 350/369 [01:58<00:06,  2.96it/s, loss=0.0432, v_num=1666580, train_F1w=1.000, val_loss=0.207, val_f1w=0.917]\n",
      "Epoch 1:  95%|█████████▌| 352/369 [01:58<00:05,  2.97it/s, loss=0.0432, v_num=1666580, train_F1w=1.000, val_loss=0.207, val_f1w=0.917]\n",
      "Epoch 1:  96%|█████████▌| 354/369 [01:58<00:05,  2.98it/s, loss=0.0432, v_num=1666580, train_F1w=1.000, val_loss=0.207, val_f1w=0.917]\n",
      "Epoch 1:  96%|█████████▋| 356/369 [01:58<00:04,  3.00it/s, loss=0.0432, v_num=1666580, train_F1w=1.000, val_loss=0.207, val_f1w=0.917]\n",
      "Epoch 1:  97%|█████████▋| 358/369 [01:58<00:03,  3.01it/s, loss=0.0432, v_num=1666580, train_F1w=1.000, val_loss=0.207, val_f1w=0.917]\n",
      "Epoch 1:  98%|█████████▊| 360/369 [01:59<00:02,  3.02it/s, loss=0.0432, v_num=1666580, train_F1w=1.000, val_loss=0.207, val_f1w=0.917]\n",
      "Epoch 1:  98%|█████████▊| 362/369 [01:59<00:02,  3.04it/s, loss=0.0432, v_num=1666580, train_F1w=1.000, val_loss=0.207, val_f1w=0.917]\n",
      "Epoch 1:  99%|█████████▊| 364/369 [01:59<00:01,  3.05it/s, loss=0.0432, v_num=1666580, train_F1w=1.000, val_loss=0.207, val_f1w=0.917]\n",
      "Epoch 1:  99%|█████████▉| 366/369 [01:59<00:00,  3.06it/s, loss=0.0432, v_num=1666580, train_F1w=1.000, val_loss=0.207, val_f1w=0.917]\n",
      "Epoch 1: 100%|█████████▉| 368/369 [01:59<00:00,  3.07it/s, loss=0.0432, v_num=1666580, train_F1w=1.000, val_loss=0.207, val_f1w=0.917]\n",
      "Epoch 1: 100%|██████████| 369/369 [01:59<00:00,  3.08it/s, loss=0.0432, v_num=1666580, train_F1w=1.000, val_loss=0.070, val_f1w=0.977]\n",
      "Epoch 1: 100%|██████████| 369/369 [01:59<00:00,  3.08it/s, loss=0.0432, v_num=1666580, train_F1w=1.000, val_loss=0.070, val_f1w=0.977]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 637: avg_val_f1w reached 0.97733 (best 0.97733), saving model to \"/mnt/gpid08/datasets/remote_sensing/tmp_from_gpid07/honey/results/Base-CEW-Sampler-epoch=1-val_loss=0.07-avg_val_f1w=0.98.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  86%|████████▌ | 318/369 [01:55<00:18,  2.75it/s, loss=0.0101, v_num=1666580, train_F1w=1.000, val_loss=0.070, val_f1w=0.977] Epoch     3: adjusting learning rate of group 0 to 7.9410e-05.\n",
      "Epoch 2:  87%|████████▋ | 320/369 [01:55<00:17,  2.76it/s, loss=0.00909, v_num=1666580, train_F1w=1.000, val_loss=0.070, val_f1w=0.977]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  87%|████████▋ | 322/369 [01:56<00:16,  2.77it/s, loss=0.00909, v_num=1666580, train_F1w=1.000, val_loss=0.070, val_f1w=0.977]\n",
      "Epoch 2:  88%|████████▊ | 324/369 [01:56<00:16,  2.79it/s, loss=0.00909, v_num=1666580, train_F1w=1.000, val_loss=0.070, val_f1w=0.977]\n",
      "Epoch 2:  88%|████████▊ | 326/369 [01:56<00:15,  2.80it/s, loss=0.00909, v_num=1666580, train_F1w=1.000, val_loss=0.070, val_f1w=0.977]\n",
      "Epoch 2:  89%|████████▉ | 328/369 [01:56<00:14,  2.81it/s, loss=0.00909, v_num=1666580, train_F1w=1.000, val_loss=0.070, val_f1w=0.977]\n",
      "Epoch 2:  89%|████████▉ | 330/369 [01:56<00:13,  2.83it/s, loss=0.00909, v_num=1666580, train_F1w=1.000, val_loss=0.070, val_f1w=0.977]\n",
      "Epoch 2:  90%|████████▉ | 332/369 [01:56<00:13,  2.84it/s, loss=0.00909, v_num=1666580, train_F1w=1.000, val_loss=0.070, val_f1w=0.977]\n",
      "Epoch 2:  91%|█████████ | 334/369 [01:57<00:12,  2.85it/s, loss=0.00909, v_num=1666580, train_F1w=1.000, val_loss=0.070, val_f1w=0.977]\n",
      "Epoch 2:  91%|█████████ | 336/369 [01:57<00:11,  2.87it/s, loss=0.00909, v_num=1666580, train_F1w=1.000, val_loss=0.070, val_f1w=0.977]\n",
      "Epoch 2:  92%|█████████▏| 338/369 [01:57<00:10,  2.88it/s, loss=0.00909, v_num=1666580, train_F1w=1.000, val_loss=0.070, val_f1w=0.977]\n",
      "Epoch 2:  92%|█████████▏| 340/369 [01:57<00:10,  2.89it/s, loss=0.00909, v_num=1666580, train_F1w=1.000, val_loss=0.070, val_f1w=0.977]\n",
      "Epoch 2:  93%|█████████▎| 342/369 [01:57<00:09,  2.91it/s, loss=0.00909, v_num=1666580, train_F1w=1.000, val_loss=0.070, val_f1w=0.977]\n",
      "Epoch 2:  93%|█████████▎| 344/369 [01:57<00:08,  2.92it/s, loss=0.00909, v_num=1666580, train_F1w=1.000, val_loss=0.070, val_f1w=0.977]\n",
      "Epoch 2:  94%|█████████▍| 346/369 [01:57<00:07,  2.93it/s, loss=0.00909, v_num=1666580, train_F1w=1.000, val_loss=0.070, val_f1w=0.977]\n",
      "Epoch 2:  94%|█████████▍| 348/369 [01:58<00:07,  2.95it/s, loss=0.00909, v_num=1666580, train_F1w=1.000, val_loss=0.070, val_f1w=0.977]\n",
      "Epoch 2:  95%|█████████▍| 350/369 [01:58<00:06,  2.96it/s, loss=0.00909, v_num=1666580, train_F1w=1.000, val_loss=0.070, val_f1w=0.977]\n",
      "Epoch 2:  95%|█████████▌| 352/369 [01:58<00:05,  2.97it/s, loss=0.00909, v_num=1666580, train_F1w=1.000, val_loss=0.070, val_f1w=0.977]\n",
      "Epoch 2:  96%|█████████▌| 354/369 [01:58<00:05,  2.98it/s, loss=0.00909, v_num=1666580, train_F1w=1.000, val_loss=0.070, val_f1w=0.977]\n",
      "Epoch 2:  96%|█████████▋| 356/369 [01:58<00:04,  3.00it/s, loss=0.00909, v_num=1666580, train_F1w=1.000, val_loss=0.070, val_f1w=0.977]\n",
      "Epoch 2:  97%|█████████▋| 358/369 [01:58<00:03,  3.01it/s, loss=0.00909, v_num=1666580, train_F1w=1.000, val_loss=0.070, val_f1w=0.977]\n",
      "Epoch 2:  98%|█████████▊| 360/369 [01:59<00:02,  3.02it/s, loss=0.00909, v_num=1666580, train_F1w=1.000, val_loss=0.070, val_f1w=0.977]\n",
      "Epoch 2:  98%|█████████▊| 362/369 [01:59<00:02,  3.04it/s, loss=0.00909, v_num=1666580, train_F1w=1.000, val_loss=0.070, val_f1w=0.977]\n",
      "Epoch 2:  99%|█████████▊| 364/369 [01:59<00:01,  3.05it/s, loss=0.00909, v_num=1666580, train_F1w=1.000, val_loss=0.070, val_f1w=0.977]\n",
      "Epoch 2:  99%|█████████▉| 366/369 [01:59<00:00,  3.06it/s, loss=0.00909, v_num=1666580, train_F1w=1.000, val_loss=0.070, val_f1w=0.977]\n",
      "Epoch 2: 100%|█████████▉| 368/369 [01:59<00:00,  3.07it/s, loss=0.00909, v_num=1666580, train_F1w=1.000, val_loss=0.070, val_f1w=0.977]\n",
      "Epoch 2: 100%|██████████| 369/369 [01:59<00:00,  3.08it/s, loss=0.00909, v_num=1666580, train_F1w=1.000, val_loss=0.0435, val_f1w=0.987]\n",
      "Epoch 2: 100%|██████████| 369/369 [01:59<00:00,  3.08it/s, loss=0.00909, v_num=1666580, train_F1w=1.000, val_loss=0.0435, val_f1w=0.987]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 956: avg_val_f1w reached 0.98667 (best 0.98667), saving model to \"/mnt/gpid08/datasets/remote_sensing/tmp_from_gpid07/honey/results/Base-CEW-Sampler-epoch=2-val_loss=0.04-avg_val_f1w=0.99.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  86%|████████▌ | 318/369 [01:55<00:18,  2.75it/s, loss=0.00776, v_num=1666580, train_F1w=1.000, val_loss=0.0435, val_f1w=0.987]Epoch     4: adjusting learning rate of group 0 to 6.5485e-05.\n",
      "Epoch 3:  87%|████████▋ | 320/369 [01:55<00:17,  2.76it/s, loss=0.0077, v_num=1666580, train_F1w=1.000, val_loss=0.0435, val_f1w=0.987] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  87%|████████▋ | 322/369 [01:56<00:16,  2.77it/s, loss=0.0077, v_num=1666580, train_F1w=1.000, val_loss=0.0435, val_f1w=0.987]\n",
      "Epoch 3:  88%|████████▊ | 324/369 [01:56<00:16,  2.79it/s, loss=0.0077, v_num=1666580, train_F1w=1.000, val_loss=0.0435, val_f1w=0.987]\n",
      "Epoch 3:  88%|████████▊ | 326/369 [01:56<00:15,  2.80it/s, loss=0.0077, v_num=1666580, train_F1w=1.000, val_loss=0.0435, val_f1w=0.987]\n",
      "Epoch 3:  89%|████████▉ | 328/369 [01:56<00:14,  2.81it/s, loss=0.0077, v_num=1666580, train_F1w=1.000, val_loss=0.0435, val_f1w=0.987]\n",
      "Epoch 3:  89%|████████▉ | 330/369 [01:56<00:13,  2.83it/s, loss=0.0077, v_num=1666580, train_F1w=1.000, val_loss=0.0435, val_f1w=0.987]\n",
      "Epoch 3:  90%|████████▉ | 332/369 [01:56<00:13,  2.84it/s, loss=0.0077, v_num=1666580, train_F1w=1.000, val_loss=0.0435, val_f1w=0.987]\n",
      "Epoch 3:  91%|█████████ | 334/369 [01:56<00:12,  2.85it/s, loss=0.0077, v_num=1666580, train_F1w=1.000, val_loss=0.0435, val_f1w=0.987]\n",
      "Epoch 3:  91%|█████████ | 336/369 [01:57<00:11,  2.87it/s, loss=0.0077, v_num=1666580, train_F1w=1.000, val_loss=0.0435, val_f1w=0.987]\n",
      "Epoch 3:  92%|█████████▏| 338/369 [01:57<00:10,  2.88it/s, loss=0.0077, v_num=1666580, train_F1w=1.000, val_loss=0.0435, val_f1w=0.987]\n",
      "Epoch 3:  92%|█████████▏| 340/369 [01:57<00:10,  2.89it/s, loss=0.0077, v_num=1666580, train_F1w=1.000, val_loss=0.0435, val_f1w=0.987]\n",
      "Epoch 3:  93%|█████████▎| 342/369 [01:57<00:09,  2.91it/s, loss=0.0077, v_num=1666580, train_F1w=1.000, val_loss=0.0435, val_f1w=0.987]\n",
      "Epoch 3:  93%|█████████▎| 344/369 [01:57<00:08,  2.92it/s, loss=0.0077, v_num=1666580, train_F1w=1.000, val_loss=0.0435, val_f1w=0.987]\n",
      "Epoch 3:  94%|█████████▍| 346/369 [01:57<00:07,  2.93it/s, loss=0.0077, v_num=1666580, train_F1w=1.000, val_loss=0.0435, val_f1w=0.987]\n",
      "Epoch 3:  94%|█████████▍| 348/369 [01:58<00:07,  2.95it/s, loss=0.0077, v_num=1666580, train_F1w=1.000, val_loss=0.0435, val_f1w=0.987]\n",
      "Epoch 3:  95%|█████████▍| 350/369 [01:58<00:06,  2.96it/s, loss=0.0077, v_num=1666580, train_F1w=1.000, val_loss=0.0435, val_f1w=0.987]\n",
      "Epoch 3:  95%|█████████▌| 352/369 [01:58<00:05,  2.97it/s, loss=0.0077, v_num=1666580, train_F1w=1.000, val_loss=0.0435, val_f1w=0.987]\n",
      "Epoch 3:  96%|█████████▌| 354/369 [01:58<00:05,  2.99it/s, loss=0.0077, v_num=1666580, train_F1w=1.000, val_loss=0.0435, val_f1w=0.987]\n",
      "Epoch 3:  96%|█████████▋| 356/369 [01:58<00:04,  3.00it/s, loss=0.0077, v_num=1666580, train_F1w=1.000, val_loss=0.0435, val_f1w=0.987]\n",
      "Epoch 3:  97%|█████████▋| 358/369 [01:58<00:03,  3.01it/s, loss=0.0077, v_num=1666580, train_F1w=1.000, val_loss=0.0435, val_f1w=0.987]\n",
      "Epoch 3:  98%|█████████▊| 360/369 [01:59<00:02,  3.03it/s, loss=0.0077, v_num=1666580, train_F1w=1.000, val_loss=0.0435, val_f1w=0.987]\n",
      "Epoch 3:  98%|█████████▊| 362/369 [01:59<00:02,  3.04it/s, loss=0.0077, v_num=1666580, train_F1w=1.000, val_loss=0.0435, val_f1w=0.987]\n",
      "Epoch 3:  99%|█████████▊| 364/369 [01:59<00:01,  3.05it/s, loss=0.0077, v_num=1666580, train_F1w=1.000, val_loss=0.0435, val_f1w=0.987]\n",
      "Epoch 3:  99%|█████████▉| 366/369 [01:59<00:00,  3.06it/s, loss=0.0077, v_num=1666580, train_F1w=1.000, val_loss=0.0435, val_f1w=0.987]\n",
      "Epoch 3: 100%|█████████▉| 368/369 [01:59<00:00,  3.08it/s, loss=0.0077, v_num=1666580, train_F1w=1.000, val_loss=0.0435, val_f1w=0.987]\n",
      "Epoch 3: 100%|██████████| 369/369 [01:59<00:00,  3.08it/s, loss=0.0077, v_num=1666580, train_F1w=1.000, val_loss=0.0555, val_f1w=0.989]\n",
      "Epoch 3: 100%|██████████| 369/369 [01:59<00:00,  3.08it/s, loss=0.0077, v_num=1666580, train_F1w=1.000, val_loss=0.0555, val_f1w=0.989]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 1275: avg_val_f1w reached 0.98917 (best 0.98917), saving model to \"/mnt/gpid08/datasets/remote_sensing/tmp_from_gpid07/honey/results/Base-CEW-Sampler-epoch=3-val_loss=0.06-avg_val_f1w=0.99.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:  86%|████████▌ | 318/369 [01:55<00:18,  2.75it/s, loss=0.0049, v_num=1666580, train_F1w=1.000, val_loss=0.0555, val_f1w=0.989] Epoch     5: adjusting learning rate of group 0 to 5.0050e-05.\n",
      "Epoch 4:  87%|████████▋ | 320/369 [01:55<00:17,  2.76it/s, loss=0.00487, v_num=1666580, train_F1w=1.000, val_loss=0.0555, val_f1w=0.989]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:  87%|████████▋ | 322/369 [01:56<00:16,  2.78it/s, loss=0.00487, v_num=1666580, train_F1w=1.000, val_loss=0.0555, val_f1w=0.989]\n",
      "Epoch 4:  88%|████████▊ | 324/369 [01:56<00:16,  2.79it/s, loss=0.00487, v_num=1666580, train_F1w=1.000, val_loss=0.0555, val_f1w=0.989]\n",
      "Epoch 4:  88%|████████▊ | 326/369 [01:56<00:15,  2.80it/s, loss=0.00487, v_num=1666580, train_F1w=1.000, val_loss=0.0555, val_f1w=0.989]\n",
      "Epoch 4:  89%|████████▉ | 328/369 [01:56<00:14,  2.82it/s, loss=0.00487, v_num=1666580, train_F1w=1.000, val_loss=0.0555, val_f1w=0.989]\n",
      "Epoch 4:  89%|████████▉ | 330/369 [01:56<00:13,  2.83it/s, loss=0.00487, v_num=1666580, train_F1w=1.000, val_loss=0.0555, val_f1w=0.989]\n",
      "Epoch 4:  90%|████████▉ | 332/369 [01:56<00:13,  2.84it/s, loss=0.00487, v_num=1666580, train_F1w=1.000, val_loss=0.0555, val_f1w=0.989]\n",
      "Epoch 4:  91%|█████████ | 334/369 [01:56<00:12,  2.86it/s, loss=0.00487, v_num=1666580, train_F1w=1.000, val_loss=0.0555, val_f1w=0.989]\n",
      "Epoch 4:  91%|█████████ | 336/369 [01:57<00:11,  2.87it/s, loss=0.00487, v_num=1666580, train_F1w=1.000, val_loss=0.0555, val_f1w=0.989]\n",
      "Epoch 4:  92%|█████████▏| 338/369 [01:57<00:10,  2.88it/s, loss=0.00487, v_num=1666580, train_F1w=1.000, val_loss=0.0555, val_f1w=0.989]\n",
      "Epoch 4:  92%|█████████▏| 340/369 [01:57<00:10,  2.90it/s, loss=0.00487, v_num=1666580, train_F1w=1.000, val_loss=0.0555, val_f1w=0.989]\n",
      "Epoch 4:  93%|█████████▎| 342/369 [01:57<00:09,  2.91it/s, loss=0.00487, v_num=1666580, train_F1w=1.000, val_loss=0.0555, val_f1w=0.989]\n",
      "Epoch 4:  93%|█████████▎| 344/369 [01:57<00:08,  2.92it/s, loss=0.00487, v_num=1666580, train_F1w=1.000, val_loss=0.0555, val_f1w=0.989]\n",
      "Epoch 4:  94%|█████████▍| 346/369 [01:57<00:07,  2.94it/s, loss=0.00487, v_num=1666580, train_F1w=1.000, val_loss=0.0555, val_f1w=0.989]\n",
      "Epoch 4:  94%|█████████▍| 348/369 [01:58<00:07,  2.95it/s, loss=0.00487, v_num=1666580, train_F1w=1.000, val_loss=0.0555, val_f1w=0.989]\n",
      "Epoch 4:  95%|█████████▍| 350/369 [01:58<00:06,  2.96it/s, loss=0.00487, v_num=1666580, train_F1w=1.000, val_loss=0.0555, val_f1w=0.989]\n",
      "Epoch 4:  95%|█████████▌| 352/369 [01:58<00:05,  2.97it/s, loss=0.00487, v_num=1666580, train_F1w=1.000, val_loss=0.0555, val_f1w=0.989]\n",
      "Epoch 4:  96%|█████████▌| 354/369 [01:58<00:05,  2.99it/s, loss=0.00487, v_num=1666580, train_F1w=1.000, val_loss=0.0555, val_f1w=0.989]\n",
      "Epoch 4:  96%|█████████▋| 356/369 [01:58<00:04,  3.00it/s, loss=0.00487, v_num=1666580, train_F1w=1.000, val_loss=0.0555, val_f1w=0.989]\n",
      "Epoch 4:  97%|█████████▋| 358/369 [01:58<00:03,  3.01it/s, loss=0.00487, v_num=1666580, train_F1w=1.000, val_loss=0.0555, val_f1w=0.989]\n",
      "Epoch 4:  98%|█████████▊| 360/369 [01:58<00:02,  3.03it/s, loss=0.00487, v_num=1666580, train_F1w=1.000, val_loss=0.0555, val_f1w=0.989]\n",
      "Epoch 4:  98%|█████████▊| 362/369 [01:59<00:02,  3.04it/s, loss=0.00487, v_num=1666580, train_F1w=1.000, val_loss=0.0555, val_f1w=0.989]\n",
      "Epoch 4:  99%|█████████▊| 364/369 [01:59<00:01,  3.05it/s, loss=0.00487, v_num=1666580, train_F1w=1.000, val_loss=0.0555, val_f1w=0.989]\n",
      "Epoch 4:  99%|█████████▉| 366/369 [01:59<00:00,  3.07it/s, loss=0.00487, v_num=1666580, train_F1w=1.000, val_loss=0.0555, val_f1w=0.989]\n",
      "Epoch 4: 100%|█████████▉| 368/369 [01:59<00:00,  3.08it/s, loss=0.00487, v_num=1666580, train_F1w=1.000, val_loss=0.0555, val_f1w=0.989]\n",
      "Epoch 4: 100%|██████████| 369/369 [01:59<00:00,  3.08it/s, loss=0.00487, v_num=1666580, train_F1w=1.000, val_loss=0.0873, val_f1w=0.981]\n",
      "Epoch 4: 100%|██████████| 369/369 [01:59<00:00,  3.08it/s, loss=0.00487, v_num=1666580, train_F1w=1.000, val_loss=0.0873, val_f1w=0.981]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 1594: avg_val_f1w was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:  86%|████████▌ | 318/369 [01:55<00:18,  2.75it/s, loss=0.00137, v_num=1666580, train_F1w=1.000, val_loss=0.0873, val_f1w=0.981] Epoch     6: adjusting learning rate of group 0 to 3.4615e-05.\n",
      "Epoch 5:  87%|████████▋ | 320/369 [01:55<00:17,  2.77it/s, loss=0.0016, v_num=1666580, train_F1w=1.000, val_loss=0.0873, val_f1w=0.981] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5:  87%|████████▋ | 322/369 [01:55<00:16,  2.78it/s, loss=0.0016, v_num=1666580, train_F1w=1.000, val_loss=0.0873, val_f1w=0.981]\n",
      "Epoch 5:  88%|████████▊ | 324/369 [01:56<00:16,  2.79it/s, loss=0.0016, v_num=1666580, train_F1w=1.000, val_loss=0.0873, val_f1w=0.981]\n",
      "Epoch 5:  88%|████████▊ | 326/369 [01:56<00:15,  2.81it/s, loss=0.0016, v_num=1666580, train_F1w=1.000, val_loss=0.0873, val_f1w=0.981]\n",
      "Epoch 5:  89%|████████▉ | 328/369 [01:56<00:14,  2.82it/s, loss=0.0016, v_num=1666580, train_F1w=1.000, val_loss=0.0873, val_f1w=0.981]\n",
      "Epoch 5:  89%|████████▉ | 330/369 [01:56<00:13,  2.83it/s, loss=0.0016, v_num=1666580, train_F1w=1.000, val_loss=0.0873, val_f1w=0.981]\n",
      "Epoch 5:  90%|████████▉ | 332/369 [01:56<00:13,  2.85it/s, loss=0.0016, v_num=1666580, train_F1w=1.000, val_loss=0.0873, val_f1w=0.981]\n",
      "Epoch 5:  91%|█████████ | 334/369 [01:56<00:12,  2.86it/s, loss=0.0016, v_num=1666580, train_F1w=1.000, val_loss=0.0873, val_f1w=0.981]\n",
      "Epoch 5:  91%|█████████ | 336/369 [01:56<00:11,  2.87it/s, loss=0.0016, v_num=1666580, train_F1w=1.000, val_loss=0.0873, val_f1w=0.981]\n",
      "Epoch 5:  92%|█████████▏| 338/369 [01:57<00:10,  2.89it/s, loss=0.0016, v_num=1666580, train_F1w=1.000, val_loss=0.0873, val_f1w=0.981]\n",
      "Epoch 5:  92%|█████████▏| 340/369 [01:57<00:10,  2.90it/s, loss=0.0016, v_num=1666580, train_F1w=1.000, val_loss=0.0873, val_f1w=0.981]\n",
      "Epoch 5:  93%|█████████▎| 342/369 [01:57<00:09,  2.91it/s, loss=0.0016, v_num=1666580, train_F1w=1.000, val_loss=0.0873, val_f1w=0.981]\n",
      "Epoch 5:  93%|█████████▎| 344/369 [01:57<00:08,  2.93it/s, loss=0.0016, v_num=1666580, train_F1w=1.000, val_loss=0.0873, val_f1w=0.981]\n",
      "Epoch 5:  94%|█████████▍| 346/369 [01:57<00:07,  2.94it/s, loss=0.0016, v_num=1666580, train_F1w=1.000, val_loss=0.0873, val_f1w=0.981]\n",
      "Epoch 5:  94%|█████████▍| 348/369 [01:57<00:07,  2.95it/s, loss=0.0016, v_num=1666580, train_F1w=1.000, val_loss=0.0873, val_f1w=0.981]\n",
      "Epoch 5:  95%|█████████▍| 350/369 [01:58<00:06,  2.96it/s, loss=0.0016, v_num=1666580, train_F1w=1.000, val_loss=0.0873, val_f1w=0.981]\n",
      "Epoch 5:  95%|█████████▌| 352/369 [01:58<00:05,  2.98it/s, loss=0.0016, v_num=1666580, train_F1w=1.000, val_loss=0.0873, val_f1w=0.981]\n",
      "Epoch 5:  96%|█████████▌| 354/369 [01:58<00:05,  2.99it/s, loss=0.0016, v_num=1666580, train_F1w=1.000, val_loss=0.0873, val_f1w=0.981]\n",
      "Epoch 5:  96%|█████████▋| 356/369 [01:58<00:04,  3.00it/s, loss=0.0016, v_num=1666580, train_F1w=1.000, val_loss=0.0873, val_f1w=0.981]\n",
      "Epoch 5:  97%|█████████▋| 358/369 [01:58<00:03,  3.02it/s, loss=0.0016, v_num=1666580, train_F1w=1.000, val_loss=0.0873, val_f1w=0.981]\n",
      "Epoch 5:  98%|█████████▊| 360/369 [01:58<00:02,  3.03it/s, loss=0.0016, v_num=1666580, train_F1w=1.000, val_loss=0.0873, val_f1w=0.981]\n",
      "Epoch 5:  98%|█████████▊| 362/369 [01:59<00:02,  3.04it/s, loss=0.0016, v_num=1666580, train_F1w=1.000, val_loss=0.0873, val_f1w=0.981]\n",
      "Epoch 5:  99%|█████████▊| 364/369 [01:59<00:01,  3.05it/s, loss=0.0016, v_num=1666580, train_F1w=1.000, val_loss=0.0873, val_f1w=0.981]\n",
      "Epoch 5:  99%|█████████▉| 366/369 [01:59<00:00,  3.07it/s, loss=0.0016, v_num=1666580, train_F1w=1.000, val_loss=0.0873, val_f1w=0.981]\n",
      "Epoch 5: 100%|█████████▉| 368/369 [01:59<00:00,  3.08it/s, loss=0.0016, v_num=1666580, train_F1w=1.000, val_loss=0.0873, val_f1w=0.981]\n",
      "Epoch 5: 100%|██████████| 369/369 [01:59<00:00,  3.09it/s, loss=0.0016, v_num=1666580, train_F1w=1.000, val_loss=0.0727, val_f1w=0.988]\n",
      "Epoch 5: 100%|██████████| 369/369 [01:59<00:00,  3.08it/s, loss=0.0016, v_num=1666580, train_F1w=1.000, val_loss=0.0727, val_f1w=0.988]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 1913: avg_val_f1w was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:  86%|████████▌ | 318/369 [01:55<00:18,  2.75it/s, loss=0.00193, v_num=1666580, train_F1w=1.000, val_loss=0.0727, val_f1w=0.988] Epoch     7: adjusting learning rate of group 0 to 2.0690e-05.\n",
      "Epoch 6:  87%|████████▋ | 320/369 [01:55<00:17,  2.77it/s, loss=0.00193, v_num=1666580, train_F1w=1.000, val_loss=0.0727, val_f1w=0.988]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6:  87%|████████▋ | 322/369 [01:55<00:16,  2.78it/s, loss=0.00193, v_num=1666580, train_F1w=1.000, val_loss=0.0727, val_f1w=0.988]\n",
      "Epoch 6:  88%|████████▊ | 324/369 [01:56<00:16,  2.79it/s, loss=0.00193, v_num=1666580, train_F1w=1.000, val_loss=0.0727, val_f1w=0.988]\n",
      "Epoch 6:  88%|████████▊ | 326/369 [01:56<00:15,  2.80it/s, loss=0.00193, v_num=1666580, train_F1w=1.000, val_loss=0.0727, val_f1w=0.988]\n",
      "Epoch 6:  89%|████████▉ | 328/369 [01:56<00:14,  2.82it/s, loss=0.00193, v_num=1666580, train_F1w=1.000, val_loss=0.0727, val_f1w=0.988]\n",
      "Epoch 6:  89%|████████▉ | 330/369 [01:56<00:13,  2.83it/s, loss=0.00193, v_num=1666580, train_F1w=1.000, val_loss=0.0727, val_f1w=0.988]\n",
      "Epoch 6:  90%|████████▉ | 332/369 [01:56<00:13,  2.85it/s, loss=0.00193, v_num=1666580, train_F1w=1.000, val_loss=0.0727, val_f1w=0.988]\n",
      "Epoch 6:  91%|█████████ | 334/369 [01:56<00:12,  2.86it/s, loss=0.00193, v_num=1666580, train_F1w=1.000, val_loss=0.0727, val_f1w=0.988]\n",
      "Epoch 6:  91%|█████████ | 336/369 [01:56<00:11,  2.87it/s, loss=0.00193, v_num=1666580, train_F1w=1.000, val_loss=0.0727, val_f1w=0.988]\n",
      "Epoch 6:  92%|█████████▏| 338/369 [01:57<00:10,  2.89it/s, loss=0.00193, v_num=1666580, train_F1w=1.000, val_loss=0.0727, val_f1w=0.988]\n",
      "Epoch 6:  92%|█████████▏| 340/369 [01:57<00:10,  2.90it/s, loss=0.00193, v_num=1666580, train_F1w=1.000, val_loss=0.0727, val_f1w=0.988]\n",
      "Epoch 6:  93%|█████████▎| 342/369 [01:57<00:09,  2.91it/s, loss=0.00193, v_num=1666580, train_F1w=1.000, val_loss=0.0727, val_f1w=0.988]\n",
      "Epoch 6:  93%|█████████▎| 344/369 [01:57<00:08,  2.93it/s, loss=0.00193, v_num=1666580, train_F1w=1.000, val_loss=0.0727, val_f1w=0.988]\n",
      "Epoch 6:  94%|█████████▍| 346/369 [01:57<00:07,  2.94it/s, loss=0.00193, v_num=1666580, train_F1w=1.000, val_loss=0.0727, val_f1w=0.988]\n",
      "Epoch 6:  94%|█████████▍| 348/369 [01:57<00:07,  2.95it/s, loss=0.00193, v_num=1666580, train_F1w=1.000, val_loss=0.0727, val_f1w=0.988]\n",
      "Epoch 6:  95%|█████████▍| 350/369 [01:58<00:06,  2.96it/s, loss=0.00193, v_num=1666580, train_F1w=1.000, val_loss=0.0727, val_f1w=0.988]\n",
      "Epoch 6:  95%|█████████▌| 352/369 [01:58<00:05,  2.98it/s, loss=0.00193, v_num=1666580, train_F1w=1.000, val_loss=0.0727, val_f1w=0.988]\n",
      "Epoch 6:  96%|█████████▌| 354/369 [01:58<00:05,  2.99it/s, loss=0.00193, v_num=1666580, train_F1w=1.000, val_loss=0.0727, val_f1w=0.988]\n",
      "Epoch 6:  96%|█████████▋| 356/369 [01:58<00:04,  3.00it/s, loss=0.00193, v_num=1666580, train_F1w=1.000, val_loss=0.0727, val_f1w=0.988]\n",
      "Epoch 6:  97%|█████████▋| 358/369 [01:58<00:03,  3.02it/s, loss=0.00193, v_num=1666580, train_F1w=1.000, val_loss=0.0727, val_f1w=0.988]\n",
      "Epoch 6:  98%|█████████▊| 360/369 [01:58<00:02,  3.03it/s, loss=0.00193, v_num=1666580, train_F1w=1.000, val_loss=0.0727, val_f1w=0.988]\n",
      "Epoch 6:  98%|█████████▊| 362/369 [01:59<00:02,  3.04it/s, loss=0.00193, v_num=1666580, train_F1w=1.000, val_loss=0.0727, val_f1w=0.988]\n",
      "Epoch 6:  99%|█████████▊| 364/369 [01:59<00:01,  3.05it/s, loss=0.00193, v_num=1666580, train_F1w=1.000, val_loss=0.0727, val_f1w=0.988]\n",
      "Epoch 6:  99%|█████████▉| 366/369 [01:59<00:00,  3.07it/s, loss=0.00193, v_num=1666580, train_F1w=1.000, val_loss=0.0727, val_f1w=0.988]\n",
      "Epoch 6: 100%|█████████▉| 368/369 [01:59<00:00,  3.08it/s, loss=0.00193, v_num=1666580, train_F1w=1.000, val_loss=0.0727, val_f1w=0.988]\n",
      "Epoch 6: 100%|██████████| 369/369 [01:59<00:00,  3.09it/s, loss=0.00193, v_num=1666580, train_F1w=1.000, val_loss=0.0738, val_f1w=0.992]\n",
      "Epoch 6: 100%|██████████| 369/369 [01:59<00:00,  3.08it/s, loss=0.00193, v_num=1666580, train_F1w=1.000, val_loss=0.0738, val_f1w=0.992]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 2232: avg_val_f1w reached 0.99150 (best 0.99150), saving model to \"/mnt/gpid08/datasets/remote_sensing/tmp_from_gpid07/honey/results/Base-CEW-Sampler-epoch=6-val_loss=0.07-avg_val_f1w=0.99.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:  86%|████████▌ | 318/369 [01:55<00:18,  2.75it/s, loss=0.00107, v_num=1666580, train_F1w=1.000, val_loss=0.0738, val_f1w=0.992] Epoch     8: adjusting learning rate of group 0 to 9.6396e-06.\n",
      "Epoch 7:  87%|████████▋ | 320/369 [01:55<00:17,  2.77it/s, loss=0.00107, v_num=1666580, train_F1w=1.000, val_loss=0.0738, val_f1w=0.992]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7:  87%|████████▋ | 322/369 [01:55<00:16,  2.78it/s, loss=0.00107, v_num=1666580, train_F1w=1.000, val_loss=0.0738, val_f1w=0.992]\n",
      "Epoch 7:  88%|████████▊ | 324/369 [01:56<00:16,  2.79it/s, loss=0.00107, v_num=1666580, train_F1w=1.000, val_loss=0.0738, val_f1w=0.992]\n",
      "Epoch 7:  88%|████████▊ | 326/369 [01:56<00:15,  2.80it/s, loss=0.00107, v_num=1666580, train_F1w=1.000, val_loss=0.0738, val_f1w=0.992]\n",
      "Epoch 7:  89%|████████▉ | 328/369 [01:56<00:14,  2.82it/s, loss=0.00107, v_num=1666580, train_F1w=1.000, val_loss=0.0738, val_f1w=0.992]\n",
      "Epoch 7:  89%|████████▉ | 330/369 [01:56<00:13,  2.83it/s, loss=0.00107, v_num=1666580, train_F1w=1.000, val_loss=0.0738, val_f1w=0.992]\n",
      "Epoch 7:  90%|████████▉ | 332/369 [01:56<00:13,  2.85it/s, loss=0.00107, v_num=1666580, train_F1w=1.000, val_loss=0.0738, val_f1w=0.992]\n",
      "Epoch 7:  91%|█████████ | 334/369 [01:56<00:12,  2.86it/s, loss=0.00107, v_num=1666580, train_F1w=1.000, val_loss=0.0738, val_f1w=0.992]\n",
      "Epoch 7:  91%|█████████ | 336/369 [01:57<00:11,  2.87it/s, loss=0.00107, v_num=1666580, train_F1w=1.000, val_loss=0.0738, val_f1w=0.992]\n",
      "Epoch 7:  92%|█████████▏| 338/369 [01:57<00:10,  2.89it/s, loss=0.00107, v_num=1666580, train_F1w=1.000, val_loss=0.0738, val_f1w=0.992]\n",
      "Epoch 7:  92%|█████████▏| 340/369 [01:57<00:10,  2.90it/s, loss=0.00107, v_num=1666580, train_F1w=1.000, val_loss=0.0738, val_f1w=0.992]\n",
      "Epoch 7:  93%|█████████▎| 342/369 [01:57<00:09,  2.91it/s, loss=0.00107, v_num=1666580, train_F1w=1.000, val_loss=0.0738, val_f1w=0.992]\n",
      "Epoch 7:  93%|█████████▎| 344/369 [01:57<00:08,  2.92it/s, loss=0.00107, v_num=1666580, train_F1w=1.000, val_loss=0.0738, val_f1w=0.992]\n",
      "Epoch 7:  94%|█████████▍| 346/369 [01:57<00:07,  2.94it/s, loss=0.00107, v_num=1666580, train_F1w=1.000, val_loss=0.0738, val_f1w=0.992]\n",
      "Epoch 7:  94%|█████████▍| 348/369 [01:57<00:07,  2.95it/s, loss=0.00107, v_num=1666580, train_F1w=1.000, val_loss=0.0738, val_f1w=0.992]\n",
      "Epoch 7:  95%|█████████▍| 350/369 [01:58<00:06,  2.96it/s, loss=0.00107, v_num=1666580, train_F1w=1.000, val_loss=0.0738, val_f1w=0.992]\n",
      "Epoch 7:  95%|█████████▌| 352/369 [01:58<00:05,  2.98it/s, loss=0.00107, v_num=1666580, train_F1w=1.000, val_loss=0.0738, val_f1w=0.992]\n",
      "Epoch 7:  96%|█████████▌| 354/369 [01:58<00:05,  2.99it/s, loss=0.00107, v_num=1666580, train_F1w=1.000, val_loss=0.0738, val_f1w=0.992]\n",
      "Epoch 7:  96%|█████████▋| 356/369 [01:58<00:04,  3.00it/s, loss=0.00107, v_num=1666580, train_F1w=1.000, val_loss=0.0738, val_f1w=0.992]\n",
      "Epoch 7:  97%|█████████▋| 358/369 [01:58<00:03,  3.02it/s, loss=0.00107, v_num=1666580, train_F1w=1.000, val_loss=0.0738, val_f1w=0.992]\n",
      "Epoch 7:  98%|█████████▊| 360/369 [01:58<00:02,  3.03it/s, loss=0.00107, v_num=1666580, train_F1w=1.000, val_loss=0.0738, val_f1w=0.992]\n",
      "Epoch 7:  98%|█████████▊| 362/369 [01:59<00:02,  3.04it/s, loss=0.00107, v_num=1666580, train_F1w=1.000, val_loss=0.0738, val_f1w=0.992]\n",
      "Epoch 7:  99%|█████████▊| 364/369 [01:59<00:01,  3.05it/s, loss=0.00107, v_num=1666580, train_F1w=1.000, val_loss=0.0738, val_f1w=0.992]\n",
      "Epoch 7:  99%|█████████▉| 366/369 [01:59<00:00,  3.07it/s, loss=0.00107, v_num=1666580, train_F1w=1.000, val_loss=0.0738, val_f1w=0.992]\n",
      "Epoch 7: 100%|█████████▉| 368/369 [01:59<00:00,  3.08it/s, loss=0.00107, v_num=1666580, train_F1w=1.000, val_loss=0.0738, val_f1w=0.992]\n",
      "Epoch 7: 100%|██████████| 369/369 [01:59<00:00,  3.08it/s, loss=0.00107, v_num=1666580, train_F1w=1.000, val_loss=0.0499, val_f1w=0.994]\n",
      "Epoch 7: 100%|██████████| 369/369 [01:59<00:00,  3.08it/s, loss=0.00107, v_num=1666580, train_F1w=1.000, val_loss=0.0499, val_f1w=0.994]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 2551: avg_val_f1w reached 0.99400 (best 0.99400), saving model to \"/mnt/gpid08/datasets/remote_sensing/tmp_from_gpid07/honey/results/Base-CEW-Sampler-epoch=7-val_loss=0.05-avg_val_f1w=0.99.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 369/369 [02:00<00:00,  3.06it/s, loss=0.00107, v_num=1666580, train_F1w=1.000, val_loss=0.0499, val_f1w=0.994]\n"
     ]
    }
   ],
   "source": [
    "checkpoint_callback = ModelCheckpoint(dirpath='/mnt/gpid08/datasets/remote_sensing/tmp_from_gpid07/honey/results/', \n",
    "                                      filename='Base-CEW-Sampler-{epoch}-{val_loss:.2f}-{avg_val_f1w:.2f}',\n",
    "                                      monitor='avg_val_f1w',\n",
    "                                      verbose=True,\n",
    "                                      save_last=None,\n",
    "                                      save_top_k=1,\n",
    "                                      save_weights_only=False,\n",
    "                                      mode='max',\n",
    "                                      auto_insert_metric_name=True,\n",
    "                                      )\n",
    "\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor='EarlyStop_Log',\\\n",
    "                                    min_delta=0.00,\\\n",
    "                                    patience=5,\\\n",
    "                                    verbose=False,\\\n",
    "                                    mode='min')\n",
    "    \n",
    "\n",
    "## Sanity-check\n",
    "trainer = pl.Trainer(gpus=gpu,\n",
    "                     callbacks=[checkpoint_callback, early_stop_callback],\n",
    "                    #  deterministic=True,\n",
    "                     enable_progress_bar=True,\n",
    "                    #  progress_bar_\n",
    "                    #  limit_train_batches=2,\n",
    "                    #  limit_val_batches=2,\n",
    "                     max_epochs=20)\n",
    "\n",
    "# Then you call the fit method passing the PL-module and PL-Datamodule, to start training.\n",
    "trainer.fit(baseline_model_CEW_sampler, dm)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7bc3aed3-a07e-43f6-84ad-0ece3d698404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "896\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "                 Pinus       1.00      0.14      0.25        28\n",
      "               Erica.m       0.95      0.68      0.79       184\n",
      "             Cistus sp       0.74      0.20      0.32        69\n",
      "             Lavandula       0.50      0.58      0.54        74\n",
      "             Citrus sp       0.17      0.21      0.18        53\n",
      "     Helianthus annuus       0.00      0.00      0.00        91\n",
      "        Eucalyptus sp.       0.00      0.00      0.00        95\n",
      "Rosmarinus officinalis       0.00      0.00      0.00        52\n",
      "              Brassica       0.25      0.98      0.39       140\n",
      "                Cardus       0.12      0.05      0.07        19\n",
      "                 Tilia       1.00      0.37      0.54        67\n",
      "             Taraxacum       0.00      0.00      0.00        25\n",
      "\n",
      "              accuracy                           0.40       897\n",
      "             macro avg       0.39      0.27      0.26       897\n",
      "          weighted avg       0.45      0.40      0.35       897\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Base-CEW-Sampler-epoch=7-val_loss=0.05-avg_val_f1w=0.99.ckpt\"\n",
    "baseline_model  = LitModel_CEW(model=resnet_model, class_weights=class_weights)\n",
    "\n",
    "# baseline_model.model.load_from_checkpoint('/content/drive/MyDrive/dataset_honey/results/'+ model_name)\n",
    "\n",
    "checkpoint =  torch.load('/mnt/gpid08/datasets/remote_sensing/tmp_from_gpid07/honey/results/'+ model_name)\n",
    "\n",
    "baseline_model.load_state_dict(checkpoint['state_dict'])\n",
    "# baseline_model.load_from_checkpoint(\"Base-epoch=1-val_loss=0.05-avg_val_f1w=0.98.ckpt\")\n",
    "\n",
    "targets, preds = [],[]\n",
    "baseline_model.to('cuda')\n",
    "for ii, data in enumerate(dm.test_dataloader()):\n",
    "    with torch.no_grad():\n",
    "        targets.append(data['target'].numpy()) #torch.argmax(data['target'],dim=1).numpy()\n",
    "        # y = y.reshape((-1,1))\n",
    "        # print(y.shape)\n",
    "        ## inference\n",
    "        o = baseline_model.predict(data['image'].to('cuda')).cpu().numpy()\n",
    "        # print(o.shape)\n",
    "        preds.append(o)\n",
    "        # print(y)\n",
    "        # print(o)\n",
    "        # if ii==2:\n",
    "        #   break\n",
    "print(ii)\n",
    "preds2 = np.vstack([x for x in preds]).reshape(-1,1)\n",
    "targets2 = np.vstack([x for x in targets]).reshape(-1,1)\n",
    "target_names = ['Pinus','Erica.m', 'Cistus sp', 'Lavandula', 'Citrus sp', 'Helianthus annuus',\n",
    "          'Eucalyptus sp.', 'Rosmarinus officinalis', 'Brassica', 'Cardus', 'Tilia', 'Taraxacum']\n",
    "\n",
    "print(classification_report(targets2, preds2, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4eb749f-9c40-4105-9317-4676475198ac",
   "metadata": {},
   "source": [
    "## Focal Loss with sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a018b71c-2200-4206-aae9-f0678c86538a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_toolbelt import losses as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eea7430e-4108-4174-8b18-6c9d03d9e4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_epoch_loss_CE, val_epoch_acc_CE = [], []\n",
    "train_epoch_loss_CE, train_epoch_acc_CE = [], []\n",
    "\n",
    "class LitModel_Focal(pl.LightningModule):\n",
    "    def __init__(self, model, class_weights):\n",
    "        super().__init__( )\n",
    "        # self.save_hyperparameters()\n",
    "        self.model = model\n",
    "        self.class_weights=class_weights.to('cuda')\n",
    "        self.focal_loss = L.FocalLoss(alpha=0.25, gamma=2)\n",
    "        # self.class_weights\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            y_hat = self(x)\n",
    "            return torch.argmax(y_hat, axis=1)\n",
    "    def compute_loss_and_metrics(self, batch):\n",
    "        x, y = batch['image'], batch['target']\n",
    "        # print(f'X: {x.shape} \\t Y: {y.shape}')\n",
    "        y_hat = self(x)\n",
    "        # print(f'Output: {y_hat.shape}')\n",
    "        # loss = F.cross_entropy(y_hat, y, weight=self.class_weights)\n",
    "        loss = self.focal_loss(y_hat, y)\n",
    "        # acc = (torch.argmax(y_hat, axis=1) == y).sum().item() / y.shape[0]\n",
    "        y1 = y.detach().cpu().numpy()\n",
    "        # print(y1.shape)\n",
    "        y_hat1 = torch.argmax(y_hat, axis=1)\n",
    "        y_hat1 = y_hat1.detach().cpu().numpy()\n",
    "        # print(y_hat1.shape)\n",
    "        f1w = f1_score(y1, y_hat1, average='weighted')\n",
    "        return loss, f1w\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, f1w = self.compute_loss_and_metrics(batch)\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_F1w', f1w, prog_bar=True)\n",
    "        #print(f'Training_step: loss> {loss} acc:{acc}')\n",
    "        return {'loss':loss,'f1w':torch.tensor(f1w)}\n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_train_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        avg_train_f1w  = torch.stack([x['f1w'] for x in outputs]).mean()\n",
    "        train_epoch_loss_CE.append(avg_train_loss.item())\n",
    "        train_epoch_acc_CE.append(avg_train_f1w.item())\n",
    "        #print(f'Epoch {self.current_epoch} TrainLOSS:{avg_train_loss} TrainACC:{avg_train_acc}  ')\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, f1w = self.compute_loss_and_metrics(batch)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_f1w', f1w, prog_bar=True)\n",
    "        return {'val_loss': torch.tensor(loss.item()), 'val_f1w': torch.tensor(f1w)}\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_val_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        avg_val_f1w  = torch.stack([x['val_f1w'] for x in outputs]).mean()\n",
    "        self.log('EarlyStop_Log', avg_val_loss.detach(), on_epoch=True, sync_dist=True)\n",
    "        self.log('avg_val_f1w', avg_val_f1w.detach(), on_epoch=True, sync_dist=True)\n",
    "        val_epoch_loss_CE.append(avg_val_loss.item())\n",
    "        val_epoch_acc_CE.append(avg_val_f1w.item())\n",
    "        #print(f'VAL-Epoch {self.current_epoch} LOSS:{avg_val_loss} ACC:{avg_val_acc} ')\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.0001)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, \n",
    "                                                                    T_0=10,\n",
    "                                                                    T_mult=1,\n",
    "                                                                    eta_min=1e-7,\n",
    "                                                                    verbose=True,\n",
    "                                                                    )\n",
    "\n",
    "        # lr_scheduler = {'scheduler': MultiStepLR(optimizer, milestones=[10,20,30,40], gamma=0.5,),'interval': 'epoch','frequency':1}\n",
    "        return [optimizer], [lr_scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6530e67c-ec48-4725-a1e2-cd1eb5efca28",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model_focal_sampler  = LitModel_Focal(model=resnet_model, class_weights=class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "936c5bd8-7ff3-47f3-a030-11dca0fdab7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Set SLURM handle signals.\n",
      "\n",
      "  | Name       | Type      | Params\n",
      "-----------------------------------------\n",
      "0 | model      | ResNet    | 23.5 M\n",
      "1 | focal_loss | FocalLoss | 0     \n",
      "-----------------------------------------\n",
      "23.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.5 M    Total params\n",
      "94.130    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     0: adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 0:  86%|████████▌ | 318/369 [01:57<00:18,  2.71it/s, loss=0.00416, v_num=1666580, train_F1w=1.000] Epoch     1: adjusting learning rate of group 0 to 9.7555e-05.\n",
      "Epoch 0:  86%|████████▋ | 319/369 [01:57<00:18,  2.72it/s, loss=0.00345, v_num=1666580, train_F1w=1.000]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  87%|████████▋ | 321/369 [01:57<00:17,  2.73it/s, loss=0.00345, v_num=1666580, train_F1w=1.000]\n",
      "Epoch 0:  88%|████████▊ | 323/369 [01:57<00:16,  2.74it/s, loss=0.00345, v_num=1666580, train_F1w=1.000]\n",
      "Epoch 0:  88%|████████▊ | 325/369 [01:57<00:15,  2.76it/s, loss=0.00345, v_num=1666580, train_F1w=1.000]\n",
      "Epoch 0:  89%|████████▊ | 327/369 [01:58<00:15,  2.77it/s, loss=0.00345, v_num=1666580, train_F1w=1.000]\n",
      "Epoch 0:  89%|████████▉ | 329/369 [01:58<00:14,  2.78it/s, loss=0.00345, v_num=1666580, train_F1w=1.000]\n",
      "Epoch 0:  90%|████████▉ | 331/369 [01:58<00:13,  2.80it/s, loss=0.00345, v_num=1666580, train_F1w=1.000]\n",
      "Epoch 0:  90%|█████████ | 333/369 [01:58<00:12,  2.81it/s, loss=0.00345, v_num=1666580, train_F1w=1.000]\n",
      "Epoch 0:  91%|█████████ | 335/369 [01:58<00:12,  2.82it/s, loss=0.00345, v_num=1666580, train_F1w=1.000]\n",
      "Epoch 0:  91%|█████████▏| 337/369 [01:58<00:11,  2.84it/s, loss=0.00345, v_num=1666580, train_F1w=1.000]\n",
      "Epoch 0:  92%|█████████▏| 339/369 [01:59<00:10,  2.85it/s, loss=0.00345, v_num=1666580, train_F1w=1.000]\n",
      "Epoch 0:  92%|█████████▏| 341/369 [01:59<00:09,  2.86it/s, loss=0.00345, v_num=1666580, train_F1w=1.000]\n",
      "Epoch 0:  93%|█████████▎| 343/369 [01:59<00:09,  2.87it/s, loss=0.00345, v_num=1666580, train_F1w=1.000]\n",
      "Epoch 0:  93%|█████████▎| 345/369 [01:59<00:08,  2.89it/s, loss=0.00345, v_num=1666580, train_F1w=1.000]\n",
      "Epoch 0:  94%|█████████▍| 347/369 [01:59<00:07,  2.90it/s, loss=0.00345, v_num=1666580, train_F1w=1.000]\n",
      "Epoch 0:  95%|█████████▍| 349/369 [01:59<00:06,  2.91it/s, loss=0.00345, v_num=1666580, train_F1w=1.000]\n",
      "Epoch 0:  95%|█████████▌| 351/369 [01:59<00:06,  2.93it/s, loss=0.00345, v_num=1666580, train_F1w=1.000]\n",
      "Epoch 0:  96%|█████████▌| 353/369 [02:00<00:05,  2.94it/s, loss=0.00345, v_num=1666580, train_F1w=1.000]\n",
      "Epoch 0:  96%|█████████▌| 355/369 [02:00<00:04,  2.95it/s, loss=0.00345, v_num=1666580, train_F1w=1.000]\n",
      "Epoch 0:  97%|█████████▋| 357/369 [02:00<00:04,  2.96it/s, loss=0.00345, v_num=1666580, train_F1w=1.000]\n",
      "Epoch 0:  97%|█████████▋| 359/369 [02:00<00:03,  2.98it/s, loss=0.00345, v_num=1666580, train_F1w=1.000]\n",
      "Epoch 0:  98%|█████████▊| 361/369 [02:00<00:02,  2.99it/s, loss=0.00345, v_num=1666580, train_F1w=1.000]\n",
      "Epoch 0:  98%|█████████▊| 363/369 [02:00<00:01,  3.00it/s, loss=0.00345, v_num=1666580, train_F1w=1.000]\n",
      "Epoch 0:  99%|█████████▉| 365/369 [02:01<00:01,  3.02it/s, loss=0.00345, v_num=1666580, train_F1w=1.000]\n",
      "Epoch 0:  99%|█████████▉| 367/369 [02:01<00:00,  3.03it/s, loss=0.00345, v_num=1666580, train_F1w=1.000]\n",
      "Epoch 0: 100%|██████████| 369/369 [02:01<00:00,  3.04it/s, loss=0.00345, v_num=1666580, train_F1w=1.000, val_loss=0.0186, val_f1w=0.981]\n",
      "Epoch 0: 100%|██████████| 369/369 [02:01<00:00,  3.04it/s, loss=0.00345, v_num=1666580, train_F1w=1.000, val_loss=0.0186, val_f1w=0.981]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 318: avg_val_f1w reached 0.98060 (best 0.98060), saving model to \"/mnt/gpid08/datasets/remote_sensing/tmp_from_gpid07/honey/results/Base-Focal-Sampler-epoch=0-val_loss=0.02-avg_val_f1w=0.98.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  86%|████████▌ | 318/369 [01:56<00:18,  2.72it/s, loss=0.00292, v_num=1666580, train_F1w=1.000, val_loss=0.0186, val_f1w=0.981]Epoch     2: adjusting learning rate of group 0 to 9.0460e-05.\n",
      "Epoch 1:  87%|████████▋ | 320/369 [01:57<00:17,  2.73it/s, loss=0.00277, v_num=1666580, train_F1w=1.000, val_loss=0.0186, val_f1w=0.981]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  87%|████████▋ | 322/369 [01:57<00:17,  2.74it/s, loss=0.00277, v_num=1666580, train_F1w=1.000, val_loss=0.0186, val_f1w=0.981]\n",
      "Epoch 1:  88%|████████▊ | 324/369 [01:57<00:16,  2.76it/s, loss=0.00277, v_num=1666580, train_F1w=1.000, val_loss=0.0186, val_f1w=0.981]\n",
      "Epoch 1:  88%|████████▊ | 326/369 [01:57<00:15,  2.77it/s, loss=0.00277, v_num=1666580, train_F1w=1.000, val_loss=0.0186, val_f1w=0.981]\n",
      "Epoch 1:  89%|████████▉ | 328/369 [01:57<00:14,  2.78it/s, loss=0.00277, v_num=1666580, train_F1w=1.000, val_loss=0.0186, val_f1w=0.981]\n",
      "Epoch 1:  89%|████████▉ | 330/369 [01:58<00:13,  2.80it/s, loss=0.00277, v_num=1666580, train_F1w=1.000, val_loss=0.0186, val_f1w=0.981]\n",
      "Epoch 1:  90%|████████▉ | 332/369 [01:58<00:13,  2.81it/s, loss=0.00277, v_num=1666580, train_F1w=1.000, val_loss=0.0186, val_f1w=0.981]\n",
      "Epoch 1:  91%|█████████ | 334/369 [01:58<00:12,  2.82it/s, loss=0.00277, v_num=1666580, train_F1w=1.000, val_loss=0.0186, val_f1w=0.981]\n",
      "Epoch 1:  91%|█████████ | 336/369 [01:58<00:11,  2.84it/s, loss=0.00277, v_num=1666580, train_F1w=1.000, val_loss=0.0186, val_f1w=0.981]\n",
      "Epoch 1:  92%|█████████▏| 338/369 [01:58<00:10,  2.85it/s, loss=0.00277, v_num=1666580, train_F1w=1.000, val_loss=0.0186, val_f1w=0.981]\n",
      "Epoch 1:  92%|█████████▏| 340/369 [01:58<00:10,  2.86it/s, loss=0.00277, v_num=1666580, train_F1w=1.000, val_loss=0.0186, val_f1w=0.981]\n",
      "Epoch 1:  93%|█████████▎| 342/369 [01:58<00:09,  2.88it/s, loss=0.00277, v_num=1666580, train_F1w=1.000, val_loss=0.0186, val_f1w=0.981]\n",
      "Epoch 1:  93%|█████████▎| 344/369 [01:59<00:08,  2.89it/s, loss=0.00277, v_num=1666580, train_F1w=1.000, val_loss=0.0186, val_f1w=0.981]\n",
      "Epoch 1:  94%|█████████▍| 346/369 [01:59<00:07,  2.90it/s, loss=0.00277, v_num=1666580, train_F1w=1.000, val_loss=0.0186, val_f1w=0.981]\n",
      "Epoch 1:  94%|█████████▍| 348/369 [01:59<00:07,  2.91it/s, loss=0.00277, v_num=1666580, train_F1w=1.000, val_loss=0.0186, val_f1w=0.981]\n",
      "Epoch 1:  95%|█████████▍| 350/369 [01:59<00:06,  2.93it/s, loss=0.00277, v_num=1666580, train_F1w=1.000, val_loss=0.0186, val_f1w=0.981]\n",
      "Epoch 1:  95%|█████████▌| 352/369 [01:59<00:05,  2.94it/s, loss=0.00277, v_num=1666580, train_F1w=1.000, val_loss=0.0186, val_f1w=0.981]\n",
      "Epoch 1:  96%|█████████▌| 354/369 [01:59<00:05,  2.95it/s, loss=0.00277, v_num=1666580, train_F1w=1.000, val_loss=0.0186, val_f1w=0.981]\n",
      "Epoch 1:  96%|█████████▋| 356/369 [02:00<00:04,  2.97it/s, loss=0.00277, v_num=1666580, train_F1w=1.000, val_loss=0.0186, val_f1w=0.981]\n",
      "Epoch 1:  97%|█████████▋| 358/369 [02:00<00:03,  2.98it/s, loss=0.00277, v_num=1666580, train_F1w=1.000, val_loss=0.0186, val_f1w=0.981]\n",
      "Epoch 1:  98%|█████████▊| 360/369 [02:00<00:03,  2.99it/s, loss=0.00277, v_num=1666580, train_F1w=1.000, val_loss=0.0186, val_f1w=0.981]\n",
      "Epoch 1:  98%|█████████▊| 362/369 [02:00<00:02,  3.00it/s, loss=0.00277, v_num=1666580, train_F1w=1.000, val_loss=0.0186, val_f1w=0.981]\n",
      "Epoch 1:  99%|█████████▊| 364/369 [02:00<00:01,  3.02it/s, loss=0.00277, v_num=1666580, train_F1w=1.000, val_loss=0.0186, val_f1w=0.981]\n",
      "Epoch 1:  99%|█████████▉| 366/369 [02:00<00:00,  3.03it/s, loss=0.00277, v_num=1666580, train_F1w=1.000, val_loss=0.0186, val_f1w=0.981]\n",
      "Epoch 1: 100%|█████████▉| 368/369 [02:01<00:00,  3.04it/s, loss=0.00277, v_num=1666580, train_F1w=1.000, val_loss=0.0186, val_f1w=0.981]\n",
      "Epoch 1: 100%|██████████| 369/369 [02:01<00:00,  3.05it/s, loss=0.00277, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.986]\n",
      "Epoch 1: 100%|██████████| 369/369 [02:01<00:00,  3.05it/s, loss=0.00277, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.986]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 637: avg_val_f1w reached 0.98583 (best 0.98583), saving model to \"/mnt/gpid08/datasets/remote_sensing/tmp_from_gpid07/honey/results/Base-Focal-Sampler-epoch=1-val_loss=0.01-avg_val_f1w=0.99.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  86%|████████▌ | 318/369 [01:56<00:18,  2.73it/s, loss=0.00378, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.986] Epoch     3: adjusting learning rate of group 0 to 7.9410e-05.\n",
      "Epoch 2:  87%|████████▋ | 320/369 [01:56<00:17,  2.74it/s, loss=0.00393, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.986]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  87%|████████▋ | 322/369 [01:57<00:17,  2.75it/s, loss=0.00393, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.986]\n",
      "Epoch 2:  88%|████████▊ | 324/369 [01:57<00:16,  2.76it/s, loss=0.00393, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.986]\n",
      "Epoch 2:  88%|████████▊ | 326/369 [01:57<00:15,  2.78it/s, loss=0.00393, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.986]\n",
      "Epoch 2:  89%|████████▉ | 328/369 [01:57<00:14,  2.79it/s, loss=0.00393, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.986]\n",
      "Epoch 2:  89%|████████▉ | 330/369 [01:57<00:13,  2.80it/s, loss=0.00393, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.986]\n",
      "Epoch 2:  90%|████████▉ | 332/369 [01:57<00:13,  2.82it/s, loss=0.00393, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.986]\n",
      "Epoch 2:  91%|█████████ | 334/369 [01:58<00:12,  2.83it/s, loss=0.00393, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.986]\n",
      "Epoch 2:  91%|█████████ | 336/369 [01:58<00:11,  2.84it/s, loss=0.00393, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.986]\n",
      "Epoch 2:  92%|█████████▏| 338/369 [01:58<00:10,  2.86it/s, loss=0.00393, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.986]\n",
      "Epoch 2:  92%|█████████▏| 340/369 [01:58<00:10,  2.87it/s, loss=0.00393, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.986]\n",
      "Epoch 2:  93%|█████████▎| 342/369 [01:58<00:09,  2.88it/s, loss=0.00393, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.986]\n",
      "Epoch 2:  93%|█████████▎| 344/369 [01:58<00:08,  2.90it/s, loss=0.00393, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.986]\n",
      "Epoch 2:  94%|█████████▍| 346/369 [01:58<00:07,  2.91it/s, loss=0.00393, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.986]\n",
      "Epoch 2:  94%|█████████▍| 348/369 [01:59<00:07,  2.92it/s, loss=0.00393, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.986]\n",
      "Epoch 2:  95%|█████████▍| 350/369 [01:59<00:06,  2.93it/s, loss=0.00393, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.986]\n",
      "Epoch 2:  95%|█████████▌| 352/369 [01:59<00:05,  2.95it/s, loss=0.00393, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.986]\n",
      "Epoch 2:  96%|█████████▌| 354/369 [01:59<00:05,  2.96it/s, loss=0.00393, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.986]\n",
      "Epoch 2:  96%|█████████▋| 356/369 [01:59<00:04,  2.97it/s, loss=0.00393, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.986]\n",
      "Epoch 2:  97%|█████████▋| 358/369 [01:59<00:03,  2.98it/s, loss=0.00393, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.986]\n",
      "Epoch 2:  98%|█████████▊| 360/369 [02:00<00:03,  3.00it/s, loss=0.00393, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.986]\n",
      "Epoch 2:  98%|█████████▊| 362/369 [02:00<00:02,  3.01it/s, loss=0.00393, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.986]\n",
      "Epoch 2:  99%|█████████▊| 364/369 [02:00<00:01,  3.02it/s, loss=0.00393, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.986]\n",
      "Epoch 2:  99%|█████████▉| 366/369 [02:00<00:00,  3.04it/s, loss=0.00393, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.986]\n",
      "Epoch 2: 100%|█████████▉| 368/369 [02:00<00:00,  3.05it/s, loss=0.00393, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.986]\n",
      "Epoch 2: 100%|██████████| 369/369 [02:00<00:00,  3.05it/s, loss=0.00393, v_num=1666580, train_F1w=1.000, val_loss=0.0202, val_f1w=0.975]\n",
      "Epoch 2: 100%|██████████| 369/369 [02:00<00:00,  3.05it/s, loss=0.00393, v_num=1666580, train_F1w=1.000, val_loss=0.0202, val_f1w=0.975]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 956: avg_val_f1w was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  86%|████████▌ | 318/369 [01:56<00:18,  2.73it/s, loss=0.000932, v_num=1666580, train_F1w=1.000, val_loss=0.0202, val_f1w=0.975]Epoch     4: adjusting learning rate of group 0 to 6.5485e-05.\n",
      "Epoch 3:  87%|████████▋ | 320/369 [01:56<00:17,  2.74it/s, loss=0.000898, v_num=1666580, train_F1w=1.000, val_loss=0.0202, val_f1w=0.975]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  87%|████████▋ | 322/369 [01:57<00:17,  2.75it/s, loss=0.000898, v_num=1666580, train_F1w=1.000, val_loss=0.0202, val_f1w=0.975]\n",
      "Epoch 3:  88%|████████▊ | 324/369 [01:57<00:16,  2.76it/s, loss=0.000898, v_num=1666580, train_F1w=1.000, val_loss=0.0202, val_f1w=0.975]\n",
      "Epoch 3:  88%|████████▊ | 326/369 [01:57<00:15,  2.78it/s, loss=0.000898, v_num=1666580, train_F1w=1.000, val_loss=0.0202, val_f1w=0.975]\n",
      "Epoch 3:  89%|████████▉ | 328/369 [01:57<00:14,  2.79it/s, loss=0.000898, v_num=1666580, train_F1w=1.000, val_loss=0.0202, val_f1w=0.975]\n",
      "Epoch 3:  89%|████████▉ | 330/369 [01:57<00:13,  2.80it/s, loss=0.000898, v_num=1666580, train_F1w=1.000, val_loss=0.0202, val_f1w=0.975]\n",
      "Epoch 3:  90%|████████▉ | 332/369 [01:57<00:13,  2.82it/s, loss=0.000898, v_num=1666580, train_F1w=1.000, val_loss=0.0202, val_f1w=0.975]\n",
      "Epoch 3:  91%|█████████ | 334/369 [01:58<00:12,  2.83it/s, loss=0.000898, v_num=1666580, train_F1w=1.000, val_loss=0.0202, val_f1w=0.975]\n",
      "Epoch 3:  91%|█████████ | 336/369 [01:58<00:11,  2.84it/s, loss=0.000898, v_num=1666580, train_F1w=1.000, val_loss=0.0202, val_f1w=0.975]\n",
      "Epoch 3:  92%|█████████▏| 338/369 [01:58<00:10,  2.86it/s, loss=0.000898, v_num=1666580, train_F1w=1.000, val_loss=0.0202, val_f1w=0.975]\n",
      "Epoch 3:  92%|█████████▏| 340/369 [01:58<00:10,  2.87it/s, loss=0.000898, v_num=1666580, train_F1w=1.000, val_loss=0.0202, val_f1w=0.975]\n",
      "Epoch 3:  93%|█████████▎| 342/369 [01:58<00:09,  2.88it/s, loss=0.000898, v_num=1666580, train_F1w=1.000, val_loss=0.0202, val_f1w=0.975]\n",
      "Epoch 3:  93%|█████████▎| 344/369 [01:58<00:08,  2.90it/s, loss=0.000898, v_num=1666580, train_F1w=1.000, val_loss=0.0202, val_f1w=0.975]\n",
      "Epoch 3:  94%|█████████▍| 346/369 [01:58<00:07,  2.91it/s, loss=0.000898, v_num=1666580, train_F1w=1.000, val_loss=0.0202, val_f1w=0.975]\n",
      "Epoch 3:  94%|█████████▍| 348/369 [01:59<00:07,  2.92it/s, loss=0.000898, v_num=1666580, train_F1w=1.000, val_loss=0.0202, val_f1w=0.975]\n",
      "Epoch 3:  95%|█████████▍| 350/369 [01:59<00:06,  2.93it/s, loss=0.000898, v_num=1666580, train_F1w=1.000, val_loss=0.0202, val_f1w=0.975]\n",
      "Epoch 3:  95%|█████████▌| 352/369 [01:59<00:05,  2.95it/s, loss=0.000898, v_num=1666580, train_F1w=1.000, val_loss=0.0202, val_f1w=0.975]\n",
      "Epoch 3:  96%|█████████▌| 354/369 [01:59<00:05,  2.96it/s, loss=0.000898, v_num=1666580, train_F1w=1.000, val_loss=0.0202, val_f1w=0.975]\n",
      "Epoch 3:  96%|█████████▋| 356/369 [01:59<00:04,  2.97it/s, loss=0.000898, v_num=1666580, train_F1w=1.000, val_loss=0.0202, val_f1w=0.975]\n",
      "Epoch 3:  97%|█████████▋| 358/369 [01:59<00:03,  2.99it/s, loss=0.000898, v_num=1666580, train_F1w=1.000, val_loss=0.0202, val_f1w=0.975]\n",
      "Epoch 3:  98%|█████████▊| 360/369 [02:00<00:03,  3.00it/s, loss=0.000898, v_num=1666580, train_F1w=1.000, val_loss=0.0202, val_f1w=0.975]\n",
      "Epoch 3:  98%|█████████▊| 362/369 [02:00<00:02,  3.01it/s, loss=0.000898, v_num=1666580, train_F1w=1.000, val_loss=0.0202, val_f1w=0.975]\n",
      "Epoch 3:  99%|█████████▊| 364/369 [02:00<00:01,  3.02it/s, loss=0.000898, v_num=1666580, train_F1w=1.000, val_loss=0.0202, val_f1w=0.975]\n",
      "Epoch 3:  99%|█████████▉| 366/369 [02:00<00:00,  3.04it/s, loss=0.000898, v_num=1666580, train_F1w=1.000, val_loss=0.0202, val_f1w=0.975]\n",
      "Epoch 3: 100%|█████████▉| 368/369 [02:00<00:00,  3.05it/s, loss=0.000898, v_num=1666580, train_F1w=1.000, val_loss=0.0202, val_f1w=0.975]\n",
      "Epoch 3: 100%|██████████| 369/369 [02:00<00:00,  3.05it/s, loss=0.000898, v_num=1666580, train_F1w=1.000, val_loss=0.0123, val_f1w=0.988]\n",
      "Epoch 3: 100%|██████████| 369/369 [02:00<00:00,  3.05it/s, loss=0.000898, v_num=1666580, train_F1w=1.000, val_loss=0.0123, val_f1w=0.988]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 1275: avg_val_f1w reached 0.98817 (best 0.98817), saving model to \"/mnt/gpid08/datasets/remote_sensing/tmp_from_gpid07/honey/results/Base-Focal-Sampler-epoch=3-val_loss=0.01-avg_val_f1w=0.99.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:  86%|████████▌ | 318/369 [01:56<00:18,  2.73it/s, loss=0.000554, v_num=1666580, train_F1w=1.000, val_loss=0.0123, val_f1w=0.988]Epoch     5: adjusting learning rate of group 0 to 5.0050e-05.\n",
      "Epoch 4:  87%|████████▋ | 320/369 [01:56<00:17,  2.74it/s, loss=0.000553, v_num=1666580, train_F1w=1.000, val_loss=0.0123, val_f1w=0.988]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:  87%|████████▋ | 322/369 [01:56<00:17,  2.76it/s, loss=0.000553, v_num=1666580, train_F1w=1.000, val_loss=0.0123, val_f1w=0.988]\n",
      "Epoch 4:  88%|████████▊ | 324/369 [01:57<00:16,  2.77it/s, loss=0.000553, v_num=1666580, train_F1w=1.000, val_loss=0.0123, val_f1w=0.988]\n",
      "Epoch 4:  88%|████████▊ | 326/369 [01:57<00:15,  2.78it/s, loss=0.000553, v_num=1666580, train_F1w=1.000, val_loss=0.0123, val_f1w=0.988]\n",
      "Epoch 4:  89%|████████▉ | 328/369 [01:57<00:14,  2.80it/s, loss=0.000553, v_num=1666580, train_F1w=1.000, val_loss=0.0123, val_f1w=0.988]\n",
      "Epoch 4:  89%|████████▉ | 330/369 [01:57<00:13,  2.81it/s, loss=0.000553, v_num=1666580, train_F1w=1.000, val_loss=0.0123, val_f1w=0.988]\n",
      "Epoch 4:  90%|████████▉ | 332/369 [01:57<00:13,  2.82it/s, loss=0.000553, v_num=1666580, train_F1w=1.000, val_loss=0.0123, val_f1w=0.988]\n",
      "Epoch 4:  91%|█████████ | 334/369 [01:57<00:12,  2.84it/s, loss=0.000553, v_num=1666580, train_F1w=1.000, val_loss=0.0123, val_f1w=0.988]\n",
      "Epoch 4:  91%|█████████ | 336/369 [01:57<00:11,  2.85it/s, loss=0.000553, v_num=1666580, train_F1w=1.000, val_loss=0.0123, val_f1w=0.988]\n",
      "Epoch 4:  92%|█████████▏| 338/369 [01:58<00:10,  2.86it/s, loss=0.000553, v_num=1666580, train_F1w=1.000, val_loss=0.0123, val_f1w=0.988]\n",
      "Epoch 4:  92%|█████████▏| 340/369 [01:58<00:10,  2.88it/s, loss=0.000553, v_num=1666580, train_F1w=1.000, val_loss=0.0123, val_f1w=0.988]\n",
      "Epoch 4:  93%|█████████▎| 342/369 [01:58<00:09,  2.89it/s, loss=0.000553, v_num=1666580, train_F1w=1.000, val_loss=0.0123, val_f1w=0.988]\n",
      "Epoch 4:  93%|█████████▎| 344/369 [01:58<00:08,  2.90it/s, loss=0.000553, v_num=1666580, train_F1w=1.000, val_loss=0.0123, val_f1w=0.988]\n",
      "Epoch 4:  94%|█████████▍| 346/369 [01:58<00:07,  2.91it/s, loss=0.000553, v_num=1666580, train_F1w=1.000, val_loss=0.0123, val_f1w=0.988]\n",
      "Epoch 4:  94%|█████████▍| 348/369 [01:58<00:07,  2.93it/s, loss=0.000553, v_num=1666580, train_F1w=1.000, val_loss=0.0123, val_f1w=0.988]\n",
      "Epoch 4:  95%|█████████▍| 350/369 [01:59<00:06,  2.94it/s, loss=0.000553, v_num=1666580, train_F1w=1.000, val_loss=0.0123, val_f1w=0.988]\n",
      "Epoch 4:  95%|█████████▌| 352/369 [01:59<00:05,  2.95it/s, loss=0.000553, v_num=1666580, train_F1w=1.000, val_loss=0.0123, val_f1w=0.988]\n",
      "Epoch 4:  96%|█████████▌| 354/369 [01:59<00:05,  2.97it/s, loss=0.000553, v_num=1666580, train_F1w=1.000, val_loss=0.0123, val_f1w=0.988]\n",
      "Epoch 4:  96%|█████████▋| 356/369 [01:59<00:04,  2.98it/s, loss=0.000553, v_num=1666580, train_F1w=1.000, val_loss=0.0123, val_f1w=0.988]\n",
      "Epoch 4:  97%|█████████▋| 358/369 [01:59<00:03,  2.99it/s, loss=0.000553, v_num=1666580, train_F1w=1.000, val_loss=0.0123, val_f1w=0.988]\n",
      "Epoch 4:  98%|█████████▊| 360/369 [01:59<00:02,  3.00it/s, loss=0.000553, v_num=1666580, train_F1w=1.000, val_loss=0.0123, val_f1w=0.988]\n",
      "Epoch 4:  98%|█████████▊| 362/369 [01:59<00:02,  3.02it/s, loss=0.000553, v_num=1666580, train_F1w=1.000, val_loss=0.0123, val_f1w=0.988]\n",
      "Epoch 4:  99%|█████████▊| 364/369 [02:00<00:01,  3.03it/s, loss=0.000553, v_num=1666580, train_F1w=1.000, val_loss=0.0123, val_f1w=0.988]\n",
      "Epoch 4:  99%|█████████▉| 366/369 [02:00<00:00,  3.04it/s, loss=0.000553, v_num=1666580, train_F1w=1.000, val_loss=0.0123, val_f1w=0.988]\n",
      "Epoch 4: 100%|█████████▉| 368/369 [02:00<00:00,  3.06it/s, loss=0.000553, v_num=1666580, train_F1w=1.000, val_loss=0.0123, val_f1w=0.988]\n",
      "Epoch 4: 100%|██████████| 369/369 [02:00<00:00,  3.06it/s, loss=0.000553, v_num=1666580, train_F1w=1.000, val_loss=0.00939, val_f1w=0.989]\n",
      "Epoch 4: 100%|██████████| 369/369 [02:00<00:00,  3.06it/s, loss=0.000553, v_num=1666580, train_F1w=1.000, val_loss=0.00939, val_f1w=0.989]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 1594: avg_val_f1w reached 0.98900 (best 0.98900), saving model to \"/mnt/gpid08/datasets/remote_sensing/tmp_from_gpid07/honey/results/Base-Focal-Sampler-epoch=4-val_loss=0.01-avg_val_f1w=0.99.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:  86%|████████▌ | 318/369 [01:56<00:18,  2.73it/s, loss=0.000333, v_num=1666580, train_F1w=1.000, val_loss=0.00939, val_f1w=0.989]Epoch     6: adjusting learning rate of group 0 to 3.4615e-05.\n",
      "Epoch 5:  87%|████████▋ | 320/369 [01:56<00:17,  2.74it/s, loss=0.00105, v_num=1666580, train_F1w=0.977, val_loss=0.00939, val_f1w=0.989] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5:  87%|████████▋ | 322/369 [01:56<00:17,  2.75it/s, loss=0.00105, v_num=1666580, train_F1w=0.977, val_loss=0.00939, val_f1w=0.989]\n",
      "Epoch 5:  88%|████████▊ | 324/369 [01:57<00:16,  2.77it/s, loss=0.00105, v_num=1666580, train_F1w=0.977, val_loss=0.00939, val_f1w=0.989]\n",
      "Epoch 5:  88%|████████▊ | 326/369 [01:57<00:15,  2.78it/s, loss=0.00105, v_num=1666580, train_F1w=0.977, val_loss=0.00939, val_f1w=0.989]\n",
      "Epoch 5:  89%|████████▉ | 328/369 [01:57<00:14,  2.79it/s, loss=0.00105, v_num=1666580, train_F1w=0.977, val_loss=0.00939, val_f1w=0.989]\n",
      "Epoch 5:  89%|████████▉ | 330/369 [01:57<00:13,  2.81it/s, loss=0.00105, v_num=1666580, train_F1w=0.977, val_loss=0.00939, val_f1w=0.989]\n",
      "Epoch 5:  90%|████████▉ | 332/369 [01:57<00:13,  2.82it/s, loss=0.00105, v_num=1666580, train_F1w=0.977, val_loss=0.00939, val_f1w=0.989]\n",
      "Epoch 5:  91%|█████████ | 334/369 [01:57<00:12,  2.83it/s, loss=0.00105, v_num=1666580, train_F1w=0.977, val_loss=0.00939, val_f1w=0.989]\n",
      "Epoch 5:  91%|█████████ | 336/369 [01:57<00:11,  2.85it/s, loss=0.00105, v_num=1666580, train_F1w=0.977, val_loss=0.00939, val_f1w=0.989]\n",
      "Epoch 5:  92%|█████████▏| 338/369 [01:58<00:10,  2.86it/s, loss=0.00105, v_num=1666580, train_F1w=0.977, val_loss=0.00939, val_f1w=0.989]\n",
      "Epoch 5:  92%|█████████▏| 340/369 [01:58<00:10,  2.87it/s, loss=0.00105, v_num=1666580, train_F1w=0.977, val_loss=0.00939, val_f1w=0.989]\n",
      "Epoch 5:  93%|█████████▎| 342/369 [01:58<00:09,  2.89it/s, loss=0.00105, v_num=1666580, train_F1w=0.977, val_loss=0.00939, val_f1w=0.989]\n",
      "Epoch 5:  93%|█████████▎| 344/369 [01:58<00:08,  2.90it/s, loss=0.00105, v_num=1666580, train_F1w=0.977, val_loss=0.00939, val_f1w=0.989]\n",
      "Epoch 5:  94%|█████████▍| 346/369 [01:58<00:07,  2.91it/s, loss=0.00105, v_num=1666580, train_F1w=0.977, val_loss=0.00939, val_f1w=0.989]\n",
      "Epoch 5:  94%|█████████▍| 348/369 [01:58<00:07,  2.93it/s, loss=0.00105, v_num=1666580, train_F1w=0.977, val_loss=0.00939, val_f1w=0.989]\n",
      "Epoch 5:  95%|█████████▍| 350/369 [01:59<00:06,  2.94it/s, loss=0.00105, v_num=1666580, train_F1w=0.977, val_loss=0.00939, val_f1w=0.989]\n",
      "Epoch 5:  95%|█████████▌| 352/369 [01:59<00:05,  2.95it/s, loss=0.00105, v_num=1666580, train_F1w=0.977, val_loss=0.00939, val_f1w=0.989]\n",
      "Epoch 5:  96%|█████████▌| 354/369 [01:59<00:05,  2.96it/s, loss=0.00105, v_num=1666580, train_F1w=0.977, val_loss=0.00939, val_f1w=0.989]\n",
      "Epoch 5:  96%|█████████▋| 356/369 [01:59<00:04,  2.98it/s, loss=0.00105, v_num=1666580, train_F1w=0.977, val_loss=0.00939, val_f1w=0.989]\n",
      "Epoch 5:  97%|█████████▋| 358/369 [01:59<00:03,  2.99it/s, loss=0.00105, v_num=1666580, train_F1w=0.977, val_loss=0.00939, val_f1w=0.989]\n",
      "Epoch 5:  98%|█████████▊| 360/369 [01:59<00:02,  3.00it/s, loss=0.00105, v_num=1666580, train_F1w=0.977, val_loss=0.00939, val_f1w=0.989]\n",
      "Epoch 5:  98%|█████████▊| 362/369 [02:00<00:02,  3.02it/s, loss=0.00105, v_num=1666580, train_F1w=0.977, val_loss=0.00939, val_f1w=0.989]\n",
      "Epoch 5:  99%|█████████▊| 364/369 [02:00<00:01,  3.03it/s, loss=0.00105, v_num=1666580, train_F1w=0.977, val_loss=0.00939, val_f1w=0.989]\n",
      "Epoch 5:  99%|█████████▉| 366/369 [02:00<00:00,  3.04it/s, loss=0.00105, v_num=1666580, train_F1w=0.977, val_loss=0.00939, val_f1w=0.989]\n",
      "Epoch 5: 100%|█████████▉| 368/369 [02:00<00:00,  3.05it/s, loss=0.00105, v_num=1666580, train_F1w=0.977, val_loss=0.00939, val_f1w=0.989]\n",
      "Epoch 5: 100%|██████████| 369/369 [02:00<00:00,  3.06it/s, loss=0.00105, v_num=1666580, train_F1w=0.977, val_loss=0.00972, val_f1w=0.992]\n",
      "Epoch 5: 100%|██████████| 369/369 [02:00<00:00,  3.06it/s, loss=0.00105, v_num=1666580, train_F1w=0.977, val_loss=0.00972, val_f1w=0.992]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 1913: avg_val_f1w reached 0.99150 (best 0.99150), saving model to \"/mnt/gpid08/datasets/remote_sensing/tmp_from_gpid07/honey/results/Base-Focal-Sampler-epoch=5-val_loss=0.01-avg_val_f1w=0.99.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:  86%|████████▌ | 318/369 [01:56<00:18,  2.73it/s, loss=9.98e-05, v_num=1666580, train_F1w=1.000, val_loss=0.00972, val_f1w=0.992]Epoch     7: adjusting learning rate of group 0 to 2.0690e-05.\n",
      "Epoch 6:  87%|████████▋ | 320/369 [01:56<00:17,  2.74it/s, loss=0.0001, v_num=1666580, train_F1w=1.000, val_loss=0.00972, val_f1w=0.992]  \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6:  87%|████████▋ | 322/369 [01:57<00:17,  2.75it/s, loss=0.0001, v_num=1666580, train_F1w=1.000, val_loss=0.00972, val_f1w=0.992]\n",
      "Epoch 6:  88%|████████▊ | 324/369 [01:57<00:16,  2.76it/s, loss=0.0001, v_num=1666580, train_F1w=1.000, val_loss=0.00972, val_f1w=0.992]\n",
      "Epoch 6:  88%|████████▊ | 326/369 [01:57<00:15,  2.78it/s, loss=0.0001, v_num=1666580, train_F1w=1.000, val_loss=0.00972, val_f1w=0.992]\n",
      "Epoch 6:  89%|████████▉ | 328/369 [01:57<00:14,  2.79it/s, loss=0.0001, v_num=1666580, train_F1w=1.000, val_loss=0.00972, val_f1w=0.992]\n",
      "Epoch 6:  89%|████████▉ | 330/369 [01:57<00:13,  2.80it/s, loss=0.0001, v_num=1666580, train_F1w=1.000, val_loss=0.00972, val_f1w=0.992]\n",
      "Epoch 6:  90%|████████▉ | 332/369 [01:57<00:13,  2.82it/s, loss=0.0001, v_num=1666580, train_F1w=1.000, val_loss=0.00972, val_f1w=0.992]\n",
      "Epoch 6:  91%|█████████ | 334/369 [01:58<00:12,  2.83it/s, loss=0.0001, v_num=1666580, train_F1w=1.000, val_loss=0.00972, val_f1w=0.992]\n",
      "Epoch 6:  91%|█████████ | 336/369 [01:58<00:11,  2.84it/s, loss=0.0001, v_num=1666580, train_F1w=1.000, val_loss=0.00972, val_f1w=0.992]\n",
      "Epoch 6:  92%|█████████▏| 338/369 [01:58<00:10,  2.86it/s, loss=0.0001, v_num=1666580, train_F1w=1.000, val_loss=0.00972, val_f1w=0.992]\n",
      "Epoch 6:  92%|█████████▏| 340/369 [01:58<00:10,  2.87it/s, loss=0.0001, v_num=1666580, train_F1w=1.000, val_loss=0.00972, val_f1w=0.992]\n",
      "Epoch 6:  93%|█████████▎| 342/369 [01:58<00:09,  2.88it/s, loss=0.0001, v_num=1666580, train_F1w=1.000, val_loss=0.00972, val_f1w=0.992]\n",
      "Epoch 6:  93%|█████████▎| 344/369 [01:58<00:08,  2.90it/s, loss=0.0001, v_num=1666580, train_F1w=1.000, val_loss=0.00972, val_f1w=0.992]\n",
      "Epoch 6:  94%|█████████▍| 346/369 [01:58<00:07,  2.91it/s, loss=0.0001, v_num=1666580, train_F1w=1.000, val_loss=0.00972, val_f1w=0.992]\n",
      "Epoch 6:  94%|█████████▍| 348/369 [01:59<00:07,  2.92it/s, loss=0.0001, v_num=1666580, train_F1w=1.000, val_loss=0.00972, val_f1w=0.992]\n",
      "Epoch 6:  95%|█████████▍| 350/369 [01:59<00:06,  2.93it/s, loss=0.0001, v_num=1666580, train_F1w=1.000, val_loss=0.00972, val_f1w=0.992]\n",
      "Epoch 6:  95%|█████████▌| 352/369 [01:59<00:05,  2.95it/s, loss=0.0001, v_num=1666580, train_F1w=1.000, val_loss=0.00972, val_f1w=0.992]\n",
      "Epoch 6:  96%|█████████▌| 354/369 [01:59<00:05,  2.96it/s, loss=0.0001, v_num=1666580, train_F1w=1.000, val_loss=0.00972, val_f1w=0.992]\n",
      "Epoch 6:  96%|█████████▋| 356/369 [01:59<00:04,  2.97it/s, loss=0.0001, v_num=1666580, train_F1w=1.000, val_loss=0.00972, val_f1w=0.992]\n",
      "Epoch 6:  97%|█████████▋| 358/369 [01:59<00:03,  2.99it/s, loss=0.0001, v_num=1666580, train_F1w=1.000, val_loss=0.00972, val_f1w=0.992]\n",
      "Epoch 6:  98%|█████████▊| 360/369 [02:00<00:03,  3.00it/s, loss=0.0001, v_num=1666580, train_F1w=1.000, val_loss=0.00972, val_f1w=0.992]\n",
      "Epoch 6:  98%|█████████▊| 362/369 [02:00<00:02,  3.01it/s, loss=0.0001, v_num=1666580, train_F1w=1.000, val_loss=0.00972, val_f1w=0.992]\n",
      "Epoch 6:  99%|█████████▊| 364/369 [02:00<00:01,  3.02it/s, loss=0.0001, v_num=1666580, train_F1w=1.000, val_loss=0.00972, val_f1w=0.992]\n",
      "Epoch 6:  99%|█████████▉| 366/369 [02:00<00:00,  3.04it/s, loss=0.0001, v_num=1666580, train_F1w=1.000, val_loss=0.00972, val_f1w=0.992]\n",
      "Epoch 6: 100%|█████████▉| 368/369 [02:00<00:00,  3.05it/s, loss=0.0001, v_num=1666580, train_F1w=1.000, val_loss=0.00972, val_f1w=0.992]\n",
      "Epoch 6: 100%|██████████| 369/369 [02:00<00:00,  3.05it/s, loss=0.0001, v_num=1666580, train_F1w=1.000, val_loss=0.0124, val_f1w=0.980] \n",
      "Epoch 6: 100%|██████████| 369/369 [02:00<00:00,  3.05it/s, loss=0.0001, v_num=1666580, train_F1w=1.000, val_loss=0.0124, val_f1w=0.980]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 2232: avg_val_f1w was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:  86%|████████▌ | 318/369 [01:56<00:18,  2.72it/s, loss=0.000137, v_num=1666580, train_F1w=1.000, val_loss=0.0124, val_f1w=0.980]Epoch     8: adjusting learning rate of group 0 to 9.6396e-06.\n",
      "Epoch 7:  87%|████████▋ | 320/369 [01:56<00:17,  2.74it/s, loss=0.000137, v_num=1666580, train_F1w=1.000, val_loss=0.0124, val_f1w=0.980]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7:  87%|████████▋ | 322/369 [01:57<00:17,  2.75it/s, loss=0.000137, v_num=1666580, train_F1w=1.000, val_loss=0.0124, val_f1w=0.980]\n",
      "Epoch 7:  88%|████████▊ | 324/369 [01:57<00:16,  2.76it/s, loss=0.000137, v_num=1666580, train_F1w=1.000, val_loss=0.0124, val_f1w=0.980]\n",
      "Epoch 7:  88%|████████▊ | 326/369 [01:57<00:15,  2.77it/s, loss=0.000137, v_num=1666580, train_F1w=1.000, val_loss=0.0124, val_f1w=0.980]\n",
      "Epoch 7:  89%|████████▉ | 328/369 [01:57<00:14,  2.79it/s, loss=0.000137, v_num=1666580, train_F1w=1.000, val_loss=0.0124, val_f1w=0.980]\n",
      "Epoch 7:  89%|████████▉ | 330/369 [01:57<00:13,  2.80it/s, loss=0.000137, v_num=1666580, train_F1w=1.000, val_loss=0.0124, val_f1w=0.980]\n",
      "Epoch 7:  90%|████████▉ | 332/369 [01:57<00:13,  2.81it/s, loss=0.000137, v_num=1666580, train_F1w=1.000, val_loss=0.0124, val_f1w=0.980]\n",
      "Epoch 7:  91%|█████████ | 334/369 [01:58<00:12,  2.83it/s, loss=0.000137, v_num=1666580, train_F1w=1.000, val_loss=0.0124, val_f1w=0.980]\n",
      "Epoch 7:  91%|█████████ | 336/369 [01:58<00:11,  2.84it/s, loss=0.000137, v_num=1666580, train_F1w=1.000, val_loss=0.0124, val_f1w=0.980]\n",
      "Epoch 7:  92%|█████████▏| 338/369 [01:58<00:10,  2.85it/s, loss=0.000137, v_num=1666580, train_F1w=1.000, val_loss=0.0124, val_f1w=0.980]\n",
      "Epoch 7:  92%|█████████▏| 340/369 [01:58<00:10,  2.87it/s, loss=0.000137, v_num=1666580, train_F1w=1.000, val_loss=0.0124, val_f1w=0.980]\n",
      "Epoch 7:  93%|█████████▎| 342/369 [01:58<00:09,  2.88it/s, loss=0.000137, v_num=1666580, train_F1w=1.000, val_loss=0.0124, val_f1w=0.980]\n",
      "Epoch 7:  93%|█████████▎| 344/369 [01:58<00:08,  2.89it/s, loss=0.000137, v_num=1666580, train_F1w=1.000, val_loss=0.0124, val_f1w=0.980]\n",
      "Epoch 7:  94%|█████████▍| 346/369 [01:59<00:07,  2.91it/s, loss=0.000137, v_num=1666580, train_F1w=1.000, val_loss=0.0124, val_f1w=0.980]\n",
      "Epoch 7:  94%|█████████▍| 348/369 [01:59<00:07,  2.92it/s, loss=0.000137, v_num=1666580, train_F1w=1.000, val_loss=0.0124, val_f1w=0.980]\n",
      "Epoch 7:  95%|█████████▍| 350/369 [01:59<00:06,  2.93it/s, loss=0.000137, v_num=1666580, train_F1w=1.000, val_loss=0.0124, val_f1w=0.980]\n",
      "Epoch 7:  95%|█████████▌| 352/369 [01:59<00:05,  2.94it/s, loss=0.000137, v_num=1666580, train_F1w=1.000, val_loss=0.0124, val_f1w=0.980]\n",
      "Epoch 7:  96%|█████████▌| 354/369 [01:59<00:05,  2.96it/s, loss=0.000137, v_num=1666580, train_F1w=1.000, val_loss=0.0124, val_f1w=0.980]\n",
      "Epoch 7:  96%|█████████▋| 356/369 [01:59<00:04,  2.97it/s, loss=0.000137, v_num=1666580, train_F1w=1.000, val_loss=0.0124, val_f1w=0.980]\n",
      "Epoch 7:  97%|█████████▋| 358/369 [02:00<00:03,  2.98it/s, loss=0.000137, v_num=1666580, train_F1w=1.000, val_loss=0.0124, val_f1w=0.980]\n",
      "Epoch 7:  98%|█████████▊| 360/369 [02:00<00:03,  2.99it/s, loss=0.000137, v_num=1666580, train_F1w=1.000, val_loss=0.0124, val_f1w=0.980]\n",
      "Epoch 7:  98%|█████████▊| 362/369 [02:00<00:02,  3.01it/s, loss=0.000137, v_num=1666580, train_F1w=1.000, val_loss=0.0124, val_f1w=0.980]\n",
      "Epoch 7:  99%|█████████▊| 364/369 [02:00<00:01,  3.02it/s, loss=0.000137, v_num=1666580, train_F1w=1.000, val_loss=0.0124, val_f1w=0.980]\n",
      "Epoch 7:  99%|█████████▉| 366/369 [02:00<00:00,  3.03it/s, loss=0.000137, v_num=1666580, train_F1w=1.000, val_loss=0.0124, val_f1w=0.980]\n",
      "Epoch 7: 100%|█████████▉| 368/369 [02:00<00:00,  3.05it/s, loss=0.000137, v_num=1666580, train_F1w=1.000, val_loss=0.0124, val_f1w=0.980]\n",
      "Epoch 7: 100%|██████████| 369/369 [02:00<00:00,  3.05it/s, loss=0.000137, v_num=1666580, train_F1w=1.000, val_loss=0.011, val_f1w=0.983] \n",
      "Epoch 7: 100%|██████████| 369/369 [02:00<00:00,  3.05it/s, loss=0.000137, v_num=1666580, train_F1w=1.000, val_loss=0.011, val_f1w=0.983]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 2551: avg_val_f1w was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:  86%|████████▌ | 318/369 [01:56<00:18,  2.72it/s, loss=9.55e-05, v_num=1666580, train_F1w=1.000, val_loss=0.011, val_f1w=0.983]Epoch     9: adjusting learning rate of group 0 to 2.5447e-06.\n",
      "Epoch 8:  87%|████████▋ | 320/369 [01:57<00:17,  2.73it/s, loss=9.86e-05, v_num=1666580, train_F1w=1.000, val_loss=0.011, val_f1w=0.983]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8:  87%|████████▋ | 322/369 [01:57<00:17,  2.74it/s, loss=9.86e-05, v_num=1666580, train_F1w=1.000, val_loss=0.011, val_f1w=0.983]\n",
      "Epoch 8:  88%|████████▊ | 324/369 [01:57<00:16,  2.76it/s, loss=9.86e-05, v_num=1666580, train_F1w=1.000, val_loss=0.011, val_f1w=0.983]\n",
      "Epoch 8:  88%|████████▊ | 326/369 [01:57<00:15,  2.77it/s, loss=9.86e-05, v_num=1666580, train_F1w=1.000, val_loss=0.011, val_f1w=0.983]\n",
      "Epoch 8:  89%|████████▉ | 328/369 [01:57<00:14,  2.78it/s, loss=9.86e-05, v_num=1666580, train_F1w=1.000, val_loss=0.011, val_f1w=0.983]\n",
      "Epoch 8:  89%|████████▉ | 330/369 [01:57<00:13,  2.80it/s, loss=9.86e-05, v_num=1666580, train_F1w=1.000, val_loss=0.011, val_f1w=0.983]\n",
      "Epoch 8:  90%|████████▉ | 332/369 [01:58<00:13,  2.81it/s, loss=9.86e-05, v_num=1666580, train_F1w=1.000, val_loss=0.011, val_f1w=0.983]\n",
      "Epoch 8:  91%|█████████ | 334/369 [01:58<00:12,  2.82it/s, loss=9.86e-05, v_num=1666580, train_F1w=1.000, val_loss=0.011, val_f1w=0.983]\n",
      "Epoch 8:  91%|█████████ | 336/369 [01:58<00:11,  2.84it/s, loss=9.86e-05, v_num=1666580, train_F1w=1.000, val_loss=0.011, val_f1w=0.983]\n",
      "Epoch 8:  92%|█████████▏| 338/369 [01:58<00:10,  2.85it/s, loss=9.86e-05, v_num=1666580, train_F1w=1.000, val_loss=0.011, val_f1w=0.983]\n",
      "Epoch 8:  92%|█████████▏| 340/369 [01:58<00:10,  2.86it/s, loss=9.86e-05, v_num=1666580, train_F1w=1.000, val_loss=0.011, val_f1w=0.983]\n",
      "Epoch 8:  93%|█████████▎| 342/369 [01:58<00:09,  2.88it/s, loss=9.86e-05, v_num=1666580, train_F1w=1.000, val_loss=0.011, val_f1w=0.983]\n",
      "Epoch 8:  93%|█████████▎| 344/369 [01:59<00:08,  2.89it/s, loss=9.86e-05, v_num=1666580, train_F1w=1.000, val_loss=0.011, val_f1w=0.983]\n",
      "Epoch 8:  94%|█████████▍| 346/369 [01:59<00:07,  2.90it/s, loss=9.86e-05, v_num=1666580, train_F1w=1.000, val_loss=0.011, val_f1w=0.983]\n",
      "Epoch 8:  94%|█████████▍| 348/369 [01:59<00:07,  2.92it/s, loss=9.86e-05, v_num=1666580, train_F1w=1.000, val_loss=0.011, val_f1w=0.983]\n",
      "Epoch 8:  95%|█████████▍| 350/369 [01:59<00:06,  2.93it/s, loss=9.86e-05, v_num=1666580, train_F1w=1.000, val_loss=0.011, val_f1w=0.983]\n",
      "Epoch 8:  95%|█████████▌| 352/369 [01:59<00:05,  2.94it/s, loss=9.86e-05, v_num=1666580, train_F1w=1.000, val_loss=0.011, val_f1w=0.983]\n",
      "Epoch 8:  96%|█████████▌| 354/369 [01:59<00:05,  2.95it/s, loss=9.86e-05, v_num=1666580, train_F1w=1.000, val_loss=0.011, val_f1w=0.983]\n",
      "Epoch 8:  96%|█████████▋| 356/369 [01:59<00:04,  2.97it/s, loss=9.86e-05, v_num=1666580, train_F1w=1.000, val_loss=0.011, val_f1w=0.983]\n",
      "Epoch 8:  97%|█████████▋| 358/369 [02:00<00:03,  2.98it/s, loss=9.86e-05, v_num=1666580, train_F1w=1.000, val_loss=0.011, val_f1w=0.983]\n",
      "Epoch 8:  98%|█████████▊| 360/369 [02:00<00:03,  2.99it/s, loss=9.86e-05, v_num=1666580, train_F1w=1.000, val_loss=0.011, val_f1w=0.983]\n",
      "Epoch 8:  98%|█████████▊| 362/369 [02:00<00:02,  3.00it/s, loss=9.86e-05, v_num=1666580, train_F1w=1.000, val_loss=0.011, val_f1w=0.983]\n",
      "Epoch 8:  99%|█████████▊| 364/369 [02:00<00:01,  3.02it/s, loss=9.86e-05, v_num=1666580, train_F1w=1.000, val_loss=0.011, val_f1w=0.983]\n",
      "Epoch 8:  99%|█████████▉| 366/369 [02:00<00:00,  3.03it/s, loss=9.86e-05, v_num=1666580, train_F1w=1.000, val_loss=0.011, val_f1w=0.983]\n",
      "Epoch 8: 100%|█████████▉| 368/369 [02:00<00:00,  3.04it/s, loss=9.86e-05, v_num=1666580, train_F1w=1.000, val_loss=0.011, val_f1w=0.983]\n",
      "Epoch 8: 100%|██████████| 369/369 [02:01<00:00,  3.05it/s, loss=9.86e-05, v_num=1666580, train_F1w=1.000, val_loss=0.0113, val_f1w=0.986]\n",
      "Epoch 8: 100%|██████████| 369/369 [02:01<00:00,  3.05it/s, loss=9.86e-05, v_num=1666580, train_F1w=1.000, val_loss=0.0113, val_f1w=0.986]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 2870: avg_val_f1w was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:  86%|████████▌ | 318/369 [01:56<00:18,  2.73it/s, loss=9.29e-05, v_num=1666580, train_F1w=1.000, val_loss=0.0113, val_f1w=0.986]Epoch    10: adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 9:  87%|████████▋ | 320/369 [01:56<00:17,  2.74it/s, loss=9.33e-05, v_num=1666580, train_F1w=1.000, val_loss=0.0113, val_f1w=0.986]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9:  87%|████████▋ | 322/369 [01:56<00:17,  2.75it/s, loss=9.33e-05, v_num=1666580, train_F1w=1.000, val_loss=0.0113, val_f1w=0.986]\n",
      "Epoch 9:  88%|████████▊ | 324/369 [01:57<00:16,  2.77it/s, loss=9.33e-05, v_num=1666580, train_F1w=1.000, val_loss=0.0113, val_f1w=0.986]\n",
      "Epoch 9:  88%|████████▊ | 326/369 [01:57<00:15,  2.78it/s, loss=9.33e-05, v_num=1666580, train_F1w=1.000, val_loss=0.0113, val_f1w=0.986]\n",
      "Epoch 9:  89%|████████▉ | 328/369 [01:57<00:14,  2.79it/s, loss=9.33e-05, v_num=1666580, train_F1w=1.000, val_loss=0.0113, val_f1w=0.986]\n",
      "Epoch 9:  89%|████████▉ | 330/369 [01:57<00:13,  2.81it/s, loss=9.33e-05, v_num=1666580, train_F1w=1.000, val_loss=0.0113, val_f1w=0.986]\n",
      "Epoch 9:  90%|████████▉ | 332/369 [01:57<00:13,  2.82it/s, loss=9.33e-05, v_num=1666580, train_F1w=1.000, val_loss=0.0113, val_f1w=0.986]\n",
      "Epoch 9:  91%|█████████ | 334/369 [01:57<00:12,  2.83it/s, loss=9.33e-05, v_num=1666580, train_F1w=1.000, val_loss=0.0113, val_f1w=0.986]\n",
      "Epoch 9:  91%|█████████ | 336/369 [01:58<00:11,  2.85it/s, loss=9.33e-05, v_num=1666580, train_F1w=1.000, val_loss=0.0113, val_f1w=0.986]\n",
      "Epoch 9:  92%|█████████▏| 338/369 [01:58<00:10,  2.86it/s, loss=9.33e-05, v_num=1666580, train_F1w=1.000, val_loss=0.0113, val_f1w=0.986]\n",
      "Epoch 9:  92%|█████████▏| 340/369 [01:58<00:10,  2.87it/s, loss=9.33e-05, v_num=1666580, train_F1w=1.000, val_loss=0.0113, val_f1w=0.986]\n",
      "Epoch 9:  93%|█████████▎| 342/369 [01:58<00:09,  2.89it/s, loss=9.33e-05, v_num=1666580, train_F1w=1.000, val_loss=0.0113, val_f1w=0.986]\n",
      "Epoch 9:  93%|█████████▎| 344/369 [01:58<00:08,  2.90it/s, loss=9.33e-05, v_num=1666580, train_F1w=1.000, val_loss=0.0113, val_f1w=0.986]\n",
      "Epoch 9:  94%|█████████▍| 346/369 [01:58<00:07,  2.91it/s, loss=9.33e-05, v_num=1666580, train_F1w=1.000, val_loss=0.0113, val_f1w=0.986]\n",
      "Epoch 9:  94%|█████████▍| 348/369 [01:58<00:07,  2.93it/s, loss=9.33e-05, v_num=1666580, train_F1w=1.000, val_loss=0.0113, val_f1w=0.986]\n",
      "Epoch 9:  95%|█████████▍| 350/369 [01:59<00:06,  2.94it/s, loss=9.33e-05, v_num=1666580, train_F1w=1.000, val_loss=0.0113, val_f1w=0.986]\n",
      "Epoch 9:  95%|█████████▌| 352/369 [01:59<00:05,  2.95it/s, loss=9.33e-05, v_num=1666580, train_F1w=1.000, val_loss=0.0113, val_f1w=0.986]\n",
      "Epoch 9:  96%|█████████▌| 354/369 [01:59<00:05,  2.96it/s, loss=9.33e-05, v_num=1666580, train_F1w=1.000, val_loss=0.0113, val_f1w=0.986]\n",
      "Epoch 9:  96%|█████████▋| 356/369 [01:59<00:04,  2.98it/s, loss=9.33e-05, v_num=1666580, train_F1w=1.000, val_loss=0.0113, val_f1w=0.986]\n",
      "Epoch 9:  97%|█████████▋| 358/369 [01:59<00:03,  2.99it/s, loss=9.33e-05, v_num=1666580, train_F1w=1.000, val_loss=0.0113, val_f1w=0.986]\n",
      "Epoch 9:  98%|█████████▊| 360/369 [01:59<00:02,  3.00it/s, loss=9.33e-05, v_num=1666580, train_F1w=1.000, val_loss=0.0113, val_f1w=0.986]\n",
      "Epoch 9:  98%|█████████▊| 362/369 [02:00<00:02,  3.02it/s, loss=9.33e-05, v_num=1666580, train_F1w=1.000, val_loss=0.0113, val_f1w=0.986]\n",
      "Epoch 9:  99%|█████████▊| 364/369 [02:00<00:01,  3.03it/s, loss=9.33e-05, v_num=1666580, train_F1w=1.000, val_loss=0.0113, val_f1w=0.986]\n",
      "Epoch 9:  99%|█████████▉| 366/369 [02:00<00:00,  3.04it/s, loss=9.33e-05, v_num=1666580, train_F1w=1.000, val_loss=0.0113, val_f1w=0.986]\n",
      "Epoch 9: 100%|█████████▉| 368/369 [02:00<00:00,  3.05it/s, loss=9.33e-05, v_num=1666580, train_F1w=1.000, val_loss=0.0113, val_f1w=0.986]\n",
      "Epoch 9: 100%|██████████| 369/369 [02:00<00:00,  3.06it/s, loss=9.33e-05, v_num=1666580, train_F1w=1.000, val_loss=0.00931, val_f1w=0.990]\n",
      "Epoch 9: 100%|██████████| 369/369 [02:00<00:00,  3.06it/s, loss=9.33e-05, v_num=1666580, train_F1w=1.000, val_loss=0.00931, val_f1w=0.990]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 3189: avg_val_f1w was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:  86%|████████▌ | 318/369 [01:56<00:18,  2.73it/s, loss=0.00711, v_num=1666580, train_F1w=1.000, val_loss=0.00931, val_f1w=0.990] Epoch    11: adjusting learning rate of group 0 to 9.7555e-05.\n",
      "Epoch 10:  87%|████████▋ | 320/369 [01:56<00:17,  2.74it/s, loss=0.0123, v_num=1666580, train_F1w=0.977, val_loss=0.00931, val_f1w=0.990] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 10:  87%|████████▋ | 322/369 [01:57<00:17,  2.75it/s, loss=0.0123, v_num=1666580, train_F1w=0.977, val_loss=0.00931, val_f1w=0.990]\n",
      "Epoch 10:  88%|████████▊ | 324/369 [01:57<00:16,  2.76it/s, loss=0.0123, v_num=1666580, train_F1w=0.977, val_loss=0.00931, val_f1w=0.990]\n",
      "Epoch 10:  88%|████████▊ | 326/369 [01:57<00:15,  2.78it/s, loss=0.0123, v_num=1666580, train_F1w=0.977, val_loss=0.00931, val_f1w=0.990]\n",
      "Epoch 10:  89%|████████▉ | 328/369 [01:57<00:14,  2.79it/s, loss=0.0123, v_num=1666580, train_F1w=0.977, val_loss=0.00931, val_f1w=0.990]\n",
      "Epoch 10:  89%|████████▉ | 330/369 [01:57<00:13,  2.80it/s, loss=0.0123, v_num=1666580, train_F1w=0.977, val_loss=0.00931, val_f1w=0.990]\n",
      "Epoch 10:  90%|████████▉ | 332/369 [01:57<00:13,  2.82it/s, loss=0.0123, v_num=1666580, train_F1w=0.977, val_loss=0.00931, val_f1w=0.990]\n",
      "Epoch 10:  91%|█████████ | 334/369 [01:58<00:12,  2.83it/s, loss=0.0123, v_num=1666580, train_F1w=0.977, val_loss=0.00931, val_f1w=0.990]\n",
      "Epoch 10:  91%|█████████ | 336/369 [01:58<00:11,  2.84it/s, loss=0.0123, v_num=1666580, train_F1w=0.977, val_loss=0.00931, val_f1w=0.990]\n",
      "Epoch 10:  92%|█████████▏| 338/369 [01:58<00:10,  2.86it/s, loss=0.0123, v_num=1666580, train_F1w=0.977, val_loss=0.00931, val_f1w=0.990]\n",
      "Epoch 10:  92%|█████████▏| 340/369 [01:58<00:10,  2.87it/s, loss=0.0123, v_num=1666580, train_F1w=0.977, val_loss=0.00931, val_f1w=0.990]\n",
      "Epoch 10:  93%|█████████▎| 342/369 [01:58<00:09,  2.88it/s, loss=0.0123, v_num=1666580, train_F1w=0.977, val_loss=0.00931, val_f1w=0.990]\n",
      "Epoch 10:  93%|█████████▎| 344/369 [01:58<00:08,  2.90it/s, loss=0.0123, v_num=1666580, train_F1w=0.977, val_loss=0.00931, val_f1w=0.990]\n",
      "Epoch 10:  94%|█████████▍| 346/369 [01:58<00:07,  2.91it/s, loss=0.0123, v_num=1666580, train_F1w=0.977, val_loss=0.00931, val_f1w=0.990]\n",
      "Epoch 10:  94%|█████████▍| 348/369 [01:59<00:07,  2.92it/s, loss=0.0123, v_num=1666580, train_F1w=0.977, val_loss=0.00931, val_f1w=0.990]\n",
      "Epoch 10:  95%|█████████▍| 350/369 [01:59<00:06,  2.93it/s, loss=0.0123, v_num=1666580, train_F1w=0.977, val_loss=0.00931, val_f1w=0.990]\n",
      "Epoch 10:  95%|█████████▌| 352/369 [01:59<00:05,  2.95it/s, loss=0.0123, v_num=1666580, train_F1w=0.977, val_loss=0.00931, val_f1w=0.990]\n",
      "Epoch 10:  96%|█████████▌| 354/369 [01:59<00:05,  2.96it/s, loss=0.0123, v_num=1666580, train_F1w=0.977, val_loss=0.00931, val_f1w=0.990]\n",
      "Epoch 10:  96%|█████████▋| 356/369 [01:59<00:04,  2.97it/s, loss=0.0123, v_num=1666580, train_F1w=0.977, val_loss=0.00931, val_f1w=0.990]\n",
      "Epoch 10:  97%|█████████▋| 358/369 [01:59<00:03,  2.99it/s, loss=0.0123, v_num=1666580, train_F1w=0.977, val_loss=0.00931, val_f1w=0.990]\n",
      "Epoch 10:  98%|█████████▊| 360/369 [02:00<00:03,  3.00it/s, loss=0.0123, v_num=1666580, train_F1w=0.977, val_loss=0.00931, val_f1w=0.990]\n",
      "Epoch 10:  98%|█████████▊| 362/369 [02:00<00:02,  3.01it/s, loss=0.0123, v_num=1666580, train_F1w=0.977, val_loss=0.00931, val_f1w=0.990]\n",
      "Epoch 10:  99%|█████████▊| 364/369 [02:00<00:01,  3.02it/s, loss=0.0123, v_num=1666580, train_F1w=0.977, val_loss=0.00931, val_f1w=0.990]\n",
      "Epoch 10:  99%|█████████▉| 366/369 [02:00<00:00,  3.04it/s, loss=0.0123, v_num=1666580, train_F1w=0.977, val_loss=0.00931, val_f1w=0.990]\n",
      "Epoch 10: 100%|█████████▉| 368/369 [02:00<00:00,  3.05it/s, loss=0.0123, v_num=1666580, train_F1w=0.977, val_loss=0.00931, val_f1w=0.990]\n",
      "Epoch 10: 100%|██████████| 369/369 [02:00<00:00,  3.06it/s, loss=0.0123, v_num=1666580, train_F1w=0.977, val_loss=0.0279, val_f1w=0.960] \n",
      "Epoch 10: 100%|██████████| 369/369 [02:00<00:00,  3.05it/s, loss=0.0123, v_num=1666580, train_F1w=0.977, val_loss=0.0279, val_f1w=0.960]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 3508: avg_val_f1w was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11:  86%|████████▌ | 318/369 [01:56<00:18,  2.72it/s, loss=0.00686, v_num=1666580, train_F1w=1.000, val_loss=0.0279, val_f1w=0.960]Epoch    12: adjusting learning rate of group 0 to 9.0460e-05.\n",
      "Epoch 11:  87%|████████▋ | 320/369 [01:57<00:17,  2.73it/s, loss=0.00708, v_num=1666580, train_F1w=1.000, val_loss=0.0279, val_f1w=0.960]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 11:  87%|████████▋ | 322/369 [01:57<00:17,  2.75it/s, loss=0.00708, v_num=1666580, train_F1w=1.000, val_loss=0.0279, val_f1w=0.960]\n",
      "Epoch 11:  88%|████████▊ | 324/369 [01:57<00:16,  2.76it/s, loss=0.00708, v_num=1666580, train_F1w=1.000, val_loss=0.0279, val_f1w=0.960]\n",
      "Epoch 11:  88%|████████▊ | 326/369 [01:57<00:15,  2.77it/s, loss=0.00708, v_num=1666580, train_F1w=1.000, val_loss=0.0279, val_f1w=0.960]\n",
      "Epoch 11:  89%|████████▉ | 328/369 [01:57<00:14,  2.79it/s, loss=0.00708, v_num=1666580, train_F1w=1.000, val_loss=0.0279, val_f1w=0.960]\n",
      "Epoch 11:  89%|████████▉ | 330/369 [01:57<00:13,  2.80it/s, loss=0.00708, v_num=1666580, train_F1w=1.000, val_loss=0.0279, val_f1w=0.960]\n",
      "Epoch 11:  90%|████████▉ | 332/369 [01:58<00:13,  2.81it/s, loss=0.00708, v_num=1666580, train_F1w=1.000, val_loss=0.0279, val_f1w=0.960]\n",
      "Epoch 11:  91%|█████████ | 334/369 [01:58<00:12,  2.83it/s, loss=0.00708, v_num=1666580, train_F1w=1.000, val_loss=0.0279, val_f1w=0.960]\n",
      "Epoch 11:  91%|█████████ | 336/369 [01:58<00:11,  2.84it/s, loss=0.00708, v_num=1666580, train_F1w=1.000, val_loss=0.0279, val_f1w=0.960]\n",
      "Epoch 11:  92%|█████████▏| 338/369 [01:58<00:10,  2.85it/s, loss=0.00708, v_num=1666580, train_F1w=1.000, val_loss=0.0279, val_f1w=0.960]\n",
      "Epoch 11:  92%|█████████▏| 340/369 [01:58<00:10,  2.86it/s, loss=0.00708, v_num=1666580, train_F1w=1.000, val_loss=0.0279, val_f1w=0.960]\n",
      "Epoch 11:  93%|█████████▎| 342/369 [01:58<00:09,  2.88it/s, loss=0.00708, v_num=1666580, train_F1w=1.000, val_loss=0.0279, val_f1w=0.960]\n",
      "Epoch 11:  93%|█████████▎| 344/369 [01:58<00:08,  2.89it/s, loss=0.00708, v_num=1666580, train_F1w=1.000, val_loss=0.0279, val_f1w=0.960]\n",
      "Epoch 11:  94%|█████████▍| 346/369 [01:59<00:07,  2.90it/s, loss=0.00708, v_num=1666580, train_F1w=1.000, val_loss=0.0279, val_f1w=0.960]\n",
      "Epoch 11:  94%|█████████▍| 348/369 [01:59<00:07,  2.92it/s, loss=0.00708, v_num=1666580, train_F1w=1.000, val_loss=0.0279, val_f1w=0.960]\n",
      "Epoch 11:  95%|█████████▍| 350/369 [01:59<00:06,  2.93it/s, loss=0.00708, v_num=1666580, train_F1w=1.000, val_loss=0.0279, val_f1w=0.960]\n",
      "Epoch 11:  95%|█████████▌| 352/369 [01:59<00:05,  2.94it/s, loss=0.00708, v_num=1666580, train_F1w=1.000, val_loss=0.0279, val_f1w=0.960]\n",
      "Epoch 11:  96%|█████████▌| 354/369 [01:59<00:05,  2.96it/s, loss=0.00708, v_num=1666580, train_F1w=1.000, val_loss=0.0279, val_f1w=0.960]\n",
      "Epoch 11:  96%|█████████▋| 356/369 [01:59<00:04,  2.97it/s, loss=0.00708, v_num=1666580, train_F1w=1.000, val_loss=0.0279, val_f1w=0.960]\n",
      "Epoch 11:  97%|█████████▋| 358/369 [02:00<00:03,  2.98it/s, loss=0.00708, v_num=1666580, train_F1w=1.000, val_loss=0.0279, val_f1w=0.960]\n",
      "Epoch 11:  98%|█████████▊| 360/369 [02:00<00:03,  2.99it/s, loss=0.00708, v_num=1666580, train_F1w=1.000, val_loss=0.0279, val_f1w=0.960]\n",
      "Epoch 11:  98%|█████████▊| 362/369 [02:00<00:02,  3.01it/s, loss=0.00708, v_num=1666580, train_F1w=1.000, val_loss=0.0279, val_f1w=0.960]\n",
      "Epoch 11:  99%|█████████▊| 364/369 [02:00<00:01,  3.02it/s, loss=0.00708, v_num=1666580, train_F1w=1.000, val_loss=0.0279, val_f1w=0.960]\n",
      "Epoch 11:  99%|█████████▉| 366/369 [02:00<00:00,  3.03it/s, loss=0.00708, v_num=1666580, train_F1w=1.000, val_loss=0.0279, val_f1w=0.960]\n",
      "Epoch 11: 100%|█████████▉| 368/369 [02:00<00:00,  3.04it/s, loss=0.00708, v_num=1666580, train_F1w=1.000, val_loss=0.0279, val_f1w=0.960]\n",
      "Epoch 11: 100%|██████████| 369/369 [02:01<00:00,  3.05it/s, loss=0.00708, v_num=1666580, train_F1w=1.000, val_loss=0.0165, val_f1w=0.975]\n",
      "Epoch 11: 100%|██████████| 369/369 [02:01<00:00,  3.05it/s, loss=0.00708, v_num=1666580, train_F1w=1.000, val_loss=0.0165, val_f1w=0.975]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 3827: avg_val_f1w was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12:  86%|████████▌ | 318/369 [01:56<00:18,  2.72it/s, loss=0.00244, v_num=1666580, train_F1w=1.000, val_loss=0.0165, val_f1w=0.975]Epoch    13: adjusting learning rate of group 0 to 7.9410e-05.\n",
      "Epoch 12:  87%|████████▋ | 320/369 [01:57<00:17,  2.73it/s, loss=0.00247, v_num=1666580, train_F1w=1.000, val_loss=0.0165, val_f1w=0.975]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 12:  87%|████████▋ | 322/369 [01:57<00:17,  2.75it/s, loss=0.00247, v_num=1666580, train_F1w=1.000, val_loss=0.0165, val_f1w=0.975]\n",
      "Epoch 12:  88%|████████▊ | 324/369 [01:57<00:16,  2.76it/s, loss=0.00247, v_num=1666580, train_F1w=1.000, val_loss=0.0165, val_f1w=0.975]\n",
      "Epoch 12:  88%|████████▊ | 326/369 [01:57<00:15,  2.77it/s, loss=0.00247, v_num=1666580, train_F1w=1.000, val_loss=0.0165, val_f1w=0.975]\n",
      "Epoch 12:  89%|████████▉ | 328/369 [01:57<00:14,  2.79it/s, loss=0.00247, v_num=1666580, train_F1w=1.000, val_loss=0.0165, val_f1w=0.975]\n",
      "Epoch 12:  89%|████████▉ | 330/369 [01:57<00:13,  2.80it/s, loss=0.00247, v_num=1666580, train_F1w=1.000, val_loss=0.0165, val_f1w=0.975]\n",
      "Epoch 12:  90%|████████▉ | 332/369 [01:57<00:13,  2.81it/s, loss=0.00247, v_num=1666580, train_F1w=1.000, val_loss=0.0165, val_f1w=0.975]\n",
      "Epoch 12:  91%|█████████ | 334/369 [01:58<00:12,  2.83it/s, loss=0.00247, v_num=1666580, train_F1w=1.000, val_loss=0.0165, val_f1w=0.975]\n",
      "Epoch 12:  91%|█████████ | 336/369 [01:58<00:11,  2.84it/s, loss=0.00247, v_num=1666580, train_F1w=1.000, val_loss=0.0165, val_f1w=0.975]\n",
      "Epoch 12:  92%|█████████▏| 338/369 [01:58<00:10,  2.86it/s, loss=0.00247, v_num=1666580, train_F1w=1.000, val_loss=0.0165, val_f1w=0.975]\n",
      "Epoch 12:  92%|█████████▏| 340/369 [01:58<00:10,  2.87it/s, loss=0.00247, v_num=1666580, train_F1w=1.000, val_loss=0.0165, val_f1w=0.975]\n",
      "Epoch 12:  93%|█████████▎| 342/369 [01:58<00:09,  2.88it/s, loss=0.00247, v_num=1666580, train_F1w=1.000, val_loss=0.0165, val_f1w=0.975]\n",
      "Epoch 12:  93%|█████████▎| 344/369 [01:58<00:08,  2.90it/s, loss=0.00247, v_num=1666580, train_F1w=1.000, val_loss=0.0165, val_f1w=0.975]\n",
      "Epoch 12:  94%|█████████▍| 346/369 [01:58<00:07,  2.91it/s, loss=0.00247, v_num=1666580, train_F1w=1.000, val_loss=0.0165, val_f1w=0.975]\n",
      "Epoch 12:  94%|█████████▍| 348/369 [01:59<00:07,  2.92it/s, loss=0.00247, v_num=1666580, train_F1w=1.000, val_loss=0.0165, val_f1w=0.975]\n",
      "Epoch 12:  95%|█████████▍| 350/369 [01:59<00:06,  2.94it/s, loss=0.00247, v_num=1666580, train_F1w=1.000, val_loss=0.0165, val_f1w=0.975]\n",
      "Epoch 12:  95%|█████████▌| 352/369 [01:59<00:05,  2.95it/s, loss=0.00247, v_num=1666580, train_F1w=1.000, val_loss=0.0165, val_f1w=0.975]\n",
      "Epoch 12:  96%|█████████▌| 354/369 [01:59<00:05,  2.96it/s, loss=0.00247, v_num=1666580, train_F1w=1.000, val_loss=0.0165, val_f1w=0.975]\n",
      "Epoch 12:  96%|█████████▋| 356/369 [01:59<00:04,  2.97it/s, loss=0.00247, v_num=1666580, train_F1w=1.000, val_loss=0.0165, val_f1w=0.975]\n",
      "Epoch 12:  97%|█████████▋| 358/369 [01:59<00:03,  2.99it/s, loss=0.00247, v_num=1666580, train_F1w=1.000, val_loss=0.0165, val_f1w=0.975]\n",
      "Epoch 12:  98%|█████████▊| 360/369 [01:59<00:02,  3.00it/s, loss=0.00247, v_num=1666580, train_F1w=1.000, val_loss=0.0165, val_f1w=0.975]\n",
      "Epoch 12:  98%|█████████▊| 362/369 [02:00<00:02,  3.01it/s, loss=0.00247, v_num=1666580, train_F1w=1.000, val_loss=0.0165, val_f1w=0.975]\n",
      "Epoch 12:  99%|█████████▊| 364/369 [02:00<00:01,  3.03it/s, loss=0.00247, v_num=1666580, train_F1w=1.000, val_loss=0.0165, val_f1w=0.975]\n",
      "Epoch 12:  99%|█████████▉| 366/369 [02:00<00:00,  3.04it/s, loss=0.00247, v_num=1666580, train_F1w=1.000, val_loss=0.0165, val_f1w=0.975]\n",
      "Epoch 12: 100%|█████████▉| 368/369 [02:00<00:00,  3.05it/s, loss=0.00247, v_num=1666580, train_F1w=1.000, val_loss=0.0165, val_f1w=0.975]\n",
      "Epoch 12: 100%|██████████| 369/369 [02:00<00:00,  3.06it/s, loss=0.00247, v_num=1666580, train_F1w=1.000, val_loss=0.0172, val_f1w=0.976]\n",
      "Epoch 12: 100%|██████████| 369/369 [02:00<00:00,  3.06it/s, loss=0.00247, v_num=1666580, train_F1w=1.000, val_loss=0.0172, val_f1w=0.976]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 4146: avg_val_f1w was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13:  86%|████████▌ | 318/369 [01:56<00:18,  2.72it/s, loss=0.00232, v_num=1666580, train_F1w=1.000, val_loss=0.0172, val_f1w=0.976] Epoch    14: adjusting learning rate of group 0 to 6.5485e-05.\n",
      "Epoch 13:  87%|████████▋ | 320/369 [01:57<00:17,  2.73it/s, loss=0.00225, v_num=1666580, train_F1w=1.000, val_loss=0.0172, val_f1w=0.976]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 13:  87%|████████▋ | 322/369 [01:57<00:17,  2.74it/s, loss=0.00225, v_num=1666580, train_F1w=1.000, val_loss=0.0172, val_f1w=0.976]\n",
      "Epoch 13:  88%|████████▊ | 324/369 [01:57<00:16,  2.76it/s, loss=0.00225, v_num=1666580, train_F1w=1.000, val_loss=0.0172, val_f1w=0.976]\n",
      "Epoch 13:  88%|████████▊ | 326/369 [01:57<00:15,  2.77it/s, loss=0.00225, v_num=1666580, train_F1w=1.000, val_loss=0.0172, val_f1w=0.976]\n",
      "Epoch 13:  89%|████████▉ | 328/369 [01:57<00:14,  2.78it/s, loss=0.00225, v_num=1666580, train_F1w=1.000, val_loss=0.0172, val_f1w=0.976]\n",
      "Epoch 13:  89%|████████▉ | 330/369 [01:58<00:13,  2.80it/s, loss=0.00225, v_num=1666580, train_F1w=1.000, val_loss=0.0172, val_f1w=0.976]\n",
      "Epoch 13:  90%|████████▉ | 332/369 [01:58<00:13,  2.81it/s, loss=0.00225, v_num=1666580, train_F1w=1.000, val_loss=0.0172, val_f1w=0.976]\n",
      "Epoch 13:  91%|█████████ | 334/369 [01:58<00:12,  2.82it/s, loss=0.00225, v_num=1666580, train_F1w=1.000, val_loss=0.0172, val_f1w=0.976]\n",
      "Epoch 13:  91%|█████████ | 336/369 [01:58<00:11,  2.84it/s, loss=0.00225, v_num=1666580, train_F1w=1.000, val_loss=0.0172, val_f1w=0.976]\n",
      "Epoch 13:  92%|█████████▏| 338/369 [01:58<00:10,  2.85it/s, loss=0.00225, v_num=1666580, train_F1w=1.000, val_loss=0.0172, val_f1w=0.976]\n",
      "Epoch 13:  92%|█████████▏| 340/369 [01:58<00:10,  2.86it/s, loss=0.00225, v_num=1666580, train_F1w=1.000, val_loss=0.0172, val_f1w=0.976]\n",
      "Epoch 13:  93%|█████████▎| 342/369 [01:58<00:09,  2.87it/s, loss=0.00225, v_num=1666580, train_F1w=1.000, val_loss=0.0172, val_f1w=0.976]\n",
      "Epoch 13:  93%|█████████▎| 344/369 [01:59<00:08,  2.89it/s, loss=0.00225, v_num=1666580, train_F1w=1.000, val_loss=0.0172, val_f1w=0.976]\n",
      "Epoch 13:  94%|█████████▍| 346/369 [01:59<00:07,  2.90it/s, loss=0.00225, v_num=1666580, train_F1w=1.000, val_loss=0.0172, val_f1w=0.976]\n",
      "Epoch 13:  94%|█████████▍| 348/369 [01:59<00:07,  2.91it/s, loss=0.00225, v_num=1666580, train_F1w=1.000, val_loss=0.0172, val_f1w=0.976]\n",
      "Epoch 13:  95%|█████████▍| 350/369 [01:59<00:06,  2.93it/s, loss=0.00225, v_num=1666580, train_F1w=1.000, val_loss=0.0172, val_f1w=0.976]\n",
      "Epoch 13:  95%|█████████▌| 352/369 [01:59<00:05,  2.94it/s, loss=0.00225, v_num=1666580, train_F1w=1.000, val_loss=0.0172, val_f1w=0.976]\n",
      "Epoch 13:  96%|█████████▌| 354/369 [01:59<00:05,  2.95it/s, loss=0.00225, v_num=1666580, train_F1w=1.000, val_loss=0.0172, val_f1w=0.976]\n",
      "Epoch 13:  96%|█████████▋| 356/369 [02:00<00:04,  2.97it/s, loss=0.00225, v_num=1666580, train_F1w=1.000, val_loss=0.0172, val_f1w=0.976]\n",
      "Epoch 13:  97%|█████████▋| 358/369 [02:00<00:03,  2.98it/s, loss=0.00225, v_num=1666580, train_F1w=1.000, val_loss=0.0172, val_f1w=0.976]\n",
      "Epoch 13:  98%|█████████▊| 360/369 [02:00<00:03,  2.99it/s, loss=0.00225, v_num=1666580, train_F1w=1.000, val_loss=0.0172, val_f1w=0.976]\n",
      "Epoch 13:  98%|█████████▊| 362/369 [02:00<00:02,  3.01it/s, loss=0.00225, v_num=1666580, train_F1w=1.000, val_loss=0.0172, val_f1w=0.976]\n",
      "Epoch 13:  99%|█████████▊| 364/369 [02:00<00:01,  3.02it/s, loss=0.00225, v_num=1666580, train_F1w=1.000, val_loss=0.0172, val_f1w=0.976]\n",
      "Epoch 13:  99%|█████████▉| 366/369 [02:00<00:00,  3.03it/s, loss=0.00225, v_num=1666580, train_F1w=1.000, val_loss=0.0172, val_f1w=0.976]\n",
      "Epoch 13: 100%|█████████▉| 368/369 [02:00<00:00,  3.04it/s, loss=0.00225, v_num=1666580, train_F1w=1.000, val_loss=0.0172, val_f1w=0.976]\n",
      "Epoch 13: 100%|██████████| 369/369 [02:00<00:00,  3.05it/s, loss=0.00225, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.976]\n",
      "Epoch 13: 100%|██████████| 369/369 [02:01<00:00,  3.05it/s, loss=0.00225, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.976]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, global step 4465: avg_val_f1w was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14:  86%|████████▌ | 318/369 [01:57<00:18,  2.70it/s, loss=0.000461, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.976]Epoch    15: adjusting learning rate of group 0 to 5.0050e-05.\n",
      "Epoch 14:  87%|████████▋ | 320/369 [01:57<00:18,  2.71it/s, loss=0.000542, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.976]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 14:  87%|████████▋ | 322/369 [01:58<00:17,  2.73it/s, loss=0.000542, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.976]\n",
      "Epoch 14:  88%|████████▊ | 324/369 [01:58<00:16,  2.74it/s, loss=0.000542, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.976]\n",
      "Epoch 14:  88%|████████▊ | 326/369 [01:58<00:15,  2.75it/s, loss=0.000542, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.976]\n",
      "Epoch 14:  89%|████████▉ | 328/369 [01:58<00:14,  2.77it/s, loss=0.000542, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.976]\n",
      "Epoch 14:  89%|████████▉ | 330/369 [01:58<00:14,  2.78it/s, loss=0.000542, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.976]\n",
      "Epoch 14:  90%|████████▉ | 332/369 [01:58<00:13,  2.79it/s, loss=0.000542, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.976]\n",
      "Epoch 14:  91%|█████████ | 334/369 [01:59<00:12,  2.81it/s, loss=0.000542, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.976]\n",
      "Epoch 14:  91%|█████████ | 336/369 [01:59<00:11,  2.82it/s, loss=0.000542, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.976]\n",
      "Epoch 14:  92%|█████████▏| 338/369 [01:59<00:10,  2.83it/s, loss=0.000542, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.976]\n",
      "Epoch 14:  92%|█████████▏| 340/369 [01:59<00:10,  2.85it/s, loss=0.000542, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.976]\n",
      "Epoch 14:  93%|█████████▎| 342/369 [01:59<00:09,  2.86it/s, loss=0.000542, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.976]\n",
      "Epoch 14:  93%|█████████▎| 344/369 [01:59<00:08,  2.87it/s, loss=0.000542, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.976]\n",
      "Epoch 14:  94%|█████████▍| 346/369 [01:59<00:07,  2.88it/s, loss=0.000542, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.976]\n",
      "Epoch 14:  94%|█████████▍| 348/369 [02:00<00:07,  2.90it/s, loss=0.000542, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.976]\n",
      "Epoch 14:  95%|█████████▍| 350/369 [02:00<00:06,  2.91it/s, loss=0.000542, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.976]\n",
      "Epoch 14:  95%|█████████▌| 352/369 [02:00<00:05,  2.92it/s, loss=0.000542, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.976]\n",
      "Epoch 14:  96%|█████████▌| 354/369 [02:00<00:05,  2.93it/s, loss=0.000542, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.976]\n",
      "Epoch 14:  96%|█████████▋| 356/369 [02:00<00:04,  2.95it/s, loss=0.000542, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.976]\n",
      "Epoch 14:  97%|█████████▋| 358/369 [02:00<00:03,  2.96it/s, loss=0.000542, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.976]\n",
      "Epoch 14:  98%|█████████▊| 360/369 [02:01<00:03,  2.97it/s, loss=0.000542, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.976]\n",
      "Epoch 14:  98%|█████████▊| 362/369 [02:01<00:02,  2.99it/s, loss=0.000542, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.976]\n",
      "Epoch 14:  99%|█████████▊| 364/369 [02:01<00:01,  3.00it/s, loss=0.000542, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.976]\n",
      "Epoch 14:  99%|█████████▉| 366/369 [02:01<00:00,  3.01it/s, loss=0.000542, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.976]\n",
      "Epoch 14: 100%|█████████▉| 368/369 [02:01<00:00,  3.02it/s, loss=0.000542, v_num=1666580, train_F1w=1.000, val_loss=0.0137, val_f1w=0.976]\n",
      "Epoch 14: 100%|██████████| 369/369 [02:01<00:00,  3.03it/s, loss=0.000542, v_num=1666580, train_F1w=1.000, val_loss=0.0145, val_f1w=0.987]\n",
      "Epoch 14: 100%|██████████| 369/369 [02:01<00:00,  3.03it/s, loss=0.000542, v_num=1666580, train_F1w=1.000, val_loss=0.0145, val_f1w=0.987]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, global step 4784: avg_val_f1w was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 369/369 [02:01<00:00,  3.03it/s, loss=0.000542, v_num=1666580, train_F1w=1.000, val_loss=0.0145, val_f1w=0.987]\n"
     ]
    }
   ],
   "source": [
    "checkpoint_callback = ModelCheckpoint(dirpath='/mnt/gpid08/datasets/remote_sensing/tmp_from_gpid07/honey/results/', \n",
    "                                      filename='Base-Focal-Sampler-{epoch}-{val_loss:.2f}-{avg_val_f1w:.2f}',\n",
    "                                      monitor='avg_val_f1w',\n",
    "                                      verbose=True,\n",
    "                                      save_last=None,\n",
    "                                      save_top_k=1,\n",
    "                                      save_weights_only=False,\n",
    "                                      mode='max',\n",
    "                                      auto_insert_metric_name=True,\n",
    "                                      )\n",
    "\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor='EarlyStop_Log',\\\n",
    "                                    min_delta=0.00,\\\n",
    "                                    patience=5,\\\n",
    "                                    verbose=False,\\\n",
    "                                    mode='min')\n",
    "    \n",
    "\n",
    "## Sanity-check\n",
    "trainer = pl.Trainer(gpus=gpu,\n",
    "                     callbacks=[checkpoint_callback, early_stop_callback],\n",
    "                    #  deterministic=True,\n",
    "                     enable_progress_bar=True,\n",
    "                    #  progress_bar_\n",
    "                    #  limit_train_batches=2,\n",
    "                    #  limit_val_batches=2,\n",
    "                     max_epochs=20)\n",
    "\n",
    "# Then you call the fit method passing the PL-module and PL-Datamodule, to start training.\n",
    "trainer.fit(baseline_model_focal_sampler, dm)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0c956b3e-d513-4ffe-b587-aa0e5c22bc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "896\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "                 Pinus       1.00      0.04      0.07        28\n",
      "               Erica.m       0.77      0.98      0.86       184\n",
      "             Cistus sp       0.65      0.77      0.70        69\n",
      "             Lavandula       0.53      0.85      0.66        74\n",
      "             Citrus sp       0.41      0.49      0.44        53\n",
      "     Helianthus annuus       0.99      0.90      0.94        91\n",
      "        Eucalyptus sp.       1.00      0.15      0.26        95\n",
      "Rosmarinus officinalis       0.00      0.00      0.00        52\n",
      "              Brassica       0.45      0.89      0.60       140\n",
      "                Cardus       0.50      0.05      0.10        19\n",
      "                 Tilia       1.00      0.19      0.33        67\n",
      "             Taraxacum       1.00      0.20      0.33        25\n",
      "\n",
      "              accuracy                           0.63       897\n",
      "             macro avg       0.69      0.46      0.44       897\n",
      "          weighted avg       0.70      0.63      0.57       897\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Base-Focal-Sampler-epoch=5-val_loss=0.01-avg_val_f1w=0.99.ckpt\" #\"Base-Focal-Sampler-epoch=7-val_loss=0.01-avg_val_f1w=0.99.ckpt\"\n",
    "baseline_model  = LitModel_Focal(model=resnet_model, class_weights=class_weights)\n",
    "# Epoch 7, global step 2551: avg_val_f1w reached 0.98733 (best 0.98733), saving model to \"/mnt/gpid08/datasets/remote_sensing/tmp_from_gpid07/honey/results/Base-Focal-Sampler-epoch=7-val_loss=0.01-avg_val_f1w=0.99.ckpt\" as top 1\n",
    "\n",
    "# /mnt/gpid08/datasets/remote_sensing/tmp_from_gpid07/honey/results/Base-epoch=7-val_loss=0.06-avg_val_f1w=0.98.ckpt\n",
    "# baseline_model.model.load_from_checkpoint('/content/drive/MyDrive/dataset_honey/results/'+ model_name)\n",
    "\n",
    "checkpoint =  torch.load('/mnt/gpid08/datasets/remote_sensing/tmp_from_gpid07/honey/results/'+ model_name)\n",
    "\n",
    "baseline_model.load_state_dict(checkpoint['state_dict'])\n",
    "# baseline_model.load_from_checkpoint(\"Base-epoch=1-val_loss=0.05-avg_val_f1w=0.98.ckpt\")\n",
    "\n",
    "targets, preds = [],[]\n",
    "baseline_model.to('cuda')\n",
    "for ii, data in enumerate(dm.test_dataloader()):\n",
    "    with torch.no_grad():\n",
    "        targets.append(data['target'].numpy()) #torch.argmax(data['target'],dim=1).numpy()\n",
    "        # y = y.reshape((-1,1))\n",
    "        # print(y.shape)\n",
    "        ## inference\n",
    "        o = baseline_model.predict(data['image'].to('cuda')).cpu().numpy()\n",
    "        # print(o.shape)\n",
    "        preds.append(o)\n",
    "        # print(y)\n",
    "        # print(o)\n",
    "        # if ii==2:\n",
    "        #   break\n",
    "print(ii)\n",
    "preds2 = np.vstack([x for x in preds]).reshape(-1,1)\n",
    "targets2 = np.vstack([x for x in targets]).reshape(-1,1)\n",
    "target_names = ['Pinus','Erica.m', 'Cistus sp', 'Lavandula', 'Citrus sp', 'Helianthus annuus',\n",
    "          'Eucalyptus sp.', 'Rosmarinus officinalis', 'Brassica', 'Cardus', 'Tilia', 'Taraxacum']\n",
    "\n",
    "print(classification_report(targets2, preds2, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dbe31a-a975-45a9-a94b-bbfa6ea2162e",
   "metadata": {},
   "source": [
    "## Stratified k-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9d7eb3-ec82-44ec-8cb5-6fb64b20bd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2989840a-8da9-46b4-b25d-6624f05b06ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d3ffaa-b2b7-49f0-bcdb-c52452a4b795",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf46d40-1887-4077-a983-787da2f6283c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df.sample(frac=1).reset_index(drop=True)\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9b4bc3-829f-4dd9-8615-1b74041c0792",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['kfold']=-1\n",
    "for fold_, (t_, v_) in enumerate(kf.split(X=df_new['name'], y=df_new['labels'])):\n",
    "    # print(t_, v_)\n",
    "    df_new.loc[v_, 'kfold'] = fold_\n",
    "    # break\n",
    "    # df_new.loc[v_, 'kfold'] = fold_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a334b29e-4fda-408e-b7f4-701bc24a2e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a844d3-29a8-4341-99b1-db7054d41dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('list_files_kfolds.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14810b7-5486-4b27-9391-fe4940ff1f8d",
   "metadata": {},
   "source": [
    "# Iniciando los k-folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eabe743-1c6d-4955-b2fe-844eb998da56",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=1\n",
    "df_train_kfold = df_new[df_new.kfold != k].reset_index(drop=True)\n",
    "df_val_kfold   = df_new[df_new.kfold == k].reset_index(drop=True)\n",
    "# df_train_kfold = df_new[df_new.kfold != fold].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84d3ded-7519-40fd-937e-80bf8bfacce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_kfold.groupby('labels').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67c8df1-30ca-434f-810d-8f7b559ae445",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val_kfold.groupby('labels').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c85b88-a03a-4307-9ce1-adef0742264b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
